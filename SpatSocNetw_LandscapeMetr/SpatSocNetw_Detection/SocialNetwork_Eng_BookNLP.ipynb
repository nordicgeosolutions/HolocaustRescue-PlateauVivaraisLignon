{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089d24ac-f24b-4042-ba31-095569954168",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from booknlp.booknlp import BookNLP\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268a6a7e-a6f1-4c08-baab-e53bde6b88ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "input_dir = \"Your_Input_DIR\"  # Directory containing .txt files\n",
    "output_base_dir = \"booknlp_output\"  # Base directory for BookNLP outputs\n",
    "analysis_output_dir = \"analysis_data_output\"  # Output directory for analysis files\n",
    "location_coords_file = \"Your_location_file.csv\" # CSV with target locations and their coordinates and aliases\n",
    "window_size = 1  # Number of sentences before and after the sentence with the location for the context window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45ba567-947d-4330-be3c-dc40f6a21cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Initialize BookNLP ---\n",
    "model_params = {\n",
    "    \"pipeline\": \"entity,quote,supersense,event,coref\",\n",
    "    \"model\": \"big\"\n",
    "}\n",
    "booknlp = BookNLP(\"en\", model_params)\n",
    "\n",
    "# Create output directories if they don't exist\n",
    "os.makedirs(output_base_dir, exist_ok=True)\n",
    "os.makedirs(analysis_output_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca03a40-2b85-4ab2-a609-029bfcd23a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase, remove leading/trailing whitespace, punctuation, and normalize accents\n",
    "def normalize_text(text):\n",
    "    if pd.isna(text):\n",
    "        return ''\n",
    "    cleaned = str(text).strip().strip(string.punctuation).lower()\n",
    "    return unidecode(cleaned)\n",
    "\n",
    "# Load locations with coordinates and create a lookup for aliases\n",
    "location_lookup = {}  # Maps alias/canonical name (normalized) to canonical location data\n",
    "canonical_locs = set()\n",
    "locations_df = pd.read_csv(location_coords_file, encoding='utf-8')\n",
    "\n",
    "for _, row in locations_df.iterrows():\n",
    "    can_raw = row['Location']\n",
    "    canonical = normalize_text(can_raw)  # normalize canonical!\n",
    "    x, y, z = row['X_Coord'], row['Y_Coord'], row['Z_Coord']\n",
    "    canonical_locs.add(canonical)\n",
    "    # Store canonical in lookup\n",
    "    location_lookup[canonical] = {\"canonical_name\": can_raw, \"x\": x, \"y\": y, \"z\": z}\n",
    "    # Add all aliases, normalized\n",
    "    aliases = str(row['Aliases']) if not pd.isna(row['Aliases']) else \"\"\n",
    "    for alias in [a.strip() for a in aliases.split(';') if a.strip()]:\n",
    "        alias_norm = normalize_text(alias)\n",
    "        location_lookup[alias_norm] = {\"canonical_name\": can_raw, \"x\": x, \"y\": y, \"z\": z}\n",
    "print(f\"Loaded {len(canonical_locs)} geocoded canonical locations and all normalized aliases.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9165cb-9ebd-4f34-bff3-a4ca82685ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define \"Helper\" Functions\n",
    "\n",
    "# ------------------- Supersense Helper -----------------------\n",
    "def make_supersense_lookup(tokens, supersense_df):\n",
    "    tok2ss = {}\n",
    "    for _, row in supersense_df.iterrows():\n",
    "        for t in range(int(row['start_token']), int(row['end_token']) + 1):\n",
    "            tok2ss[t] = row['supersense_category']\n",
    "    return tok2ss\n",
    "\n",
    "# ------------------- Label/Metadata Helpers ------------------\n",
    "def get_main_label_from_entities(entities, coref):\n",
    "    # entities['COREF'] is int\n",
    "    c = int(coref)\n",
    "    mentions = entities[(entities['cat'] == \"PER\") & (entities['COREF'] == c)]\n",
    "    texts = [str(txt) for txt in mentions['text'] if pd.notna(txt)]\n",
    "    prons = {\"i\", \"me\", \"my\", \"he\", \"him\", \"his\", \"she\", \"her\", \"they\", \"them\", \"their\", \"you\", \"your\", \"we\", \"us\", \"our\"}\n",
    "    for t in texts:\n",
    "        if t.strip().lower() not in prons:\n",
    "            return t\n",
    "    return texts[0] if texts else f\"CHAR_{coref}\"\n",
    "\n",
    "def get_character_metadata_from_bookjson(book_json, coref):\n",
    "    if \"characters\" in book_json:\n",
    "        for character in book_json[\"characters\"]:\n",
    "            if str(character[\"id\"]) == str(coref):\n",
    "                count = character.get(\"count\", 0)\n",
    "                referential_gender = \"unknown\"\n",
    "                if \"g\" in character and character[\"g\"] and character[\"g\"].get(\"argmax\"):\n",
    "                    referential_gender = character[\"g\"][\"argmax\"]\n",
    "                label = \"\"\n",
    "                mentions = character.get(\"mentions\", {})\n",
    "                propers = mentions.get(\"proper\", [])\n",
    "                if propers and \"n\" in propers[0]:\n",
    "                    label = propers[0][\"n\"]\n",
    "                modList = character.get(\"mod\", [])\n",
    "                mod_counter = Counter()\n",
    "                for m in modList:\n",
    "                    if \"d\" in m:\n",
    "                        mod_counter[m[\"d\"]] += m.get(\"c\", 1)\n",
    "                if mod_counter:\n",
    "                    most_common_mod, mod_count = mod_counter.most_common(1)[0]\n",
    "                else:\n",
    "                    most_common_mod, mod_count = \"\", 0\n",
    "                return {\n",
    "                    \"label\": label,\n",
    "                    \"gender\": referential_gender,\n",
    "                    \"mention_count\": count,\n",
    "                    \"most_common_mod\": most_common_mod,\n",
    "                    \"mod_count\": mod_count\n",
    "                }\n",
    "    return {\n",
    "        \"label\": \"\",\n",
    "        \"gender\": \"\",\n",
    "        \"mention_count\": 0,\n",
    "        \"most_common_mod\": \"\",\n",
    "        \"mod_count\": 0\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e58e03-2c0d-478d-bbff-65e0e4cdf3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------- MAIN LOOP: PER BOOK -----------------\n",
    "for filename in tqdm(os.listdir(input_dir), desc=\"Processing Books (.txt)\"):\n",
    "    if not filename.endswith(\".txt\"):\n",
    "        continue\n",
    "    book_id = os.path.splitext(filename)[0]\n",
    "    input_file = os.path.join(input_dir, filename)\n",
    "    book_output_dir = os.path.join(output_base_dir, book_id)\n",
    "    os.makedirs(book_output_dir, exist_ok=True)\n",
    "    # ----- Run BookNLP -----\n",
    "    try:\n",
    "        booknlp.process(input_file, book_output_dir, book_id)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR running BookNLP on {book_id}: {e}\")\n",
    "        continue\n",
    "    # ----- Load BookNLP outputs -----\n",
    "    try:\n",
    "        tokens = pd.read_csv(os.path.join(book_output_dir, f\"{book_id}.tokens\"), sep=\"\\t\")\n",
    "        entities = pd.read_csv(os.path.join(book_output_dir, f\"{book_id}.entities\"), sep=\"\\t\")\n",
    "        supersenses = pd.read_csv(os.path.join(book_output_dir, f\"{book_id}.supersense\"), sep=\"\\t\")\n",
    "        tok2ss = make_supersense_lookup(tokens, supersenses)\n",
    "        quotes_path = os.path.join(book_output_dir, f\"{book_id}.quotes\")\n",
    "        quotes = pd.read_csv(quotes_path, sep=\"\\t\") if os.path.isfile(quotes_path) else None\n",
    "        book_json_path = os.path.join(book_output_dir, f\"{book_id}.book\")\n",
    "        book_json = {}\n",
    "        if os.path.isfile(book_json_path):\n",
    "            with open(book_json_path, encoding='utf-8') as f:\n",
    "                book_json = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR loading BookNLP outputs for {book_id}: {e}\")\n",
    "        continue\n",
    "    # --- Node: COREF setup ---\n",
    "    per_entities = entities[entities['cat'] == \"PER\"]\n",
    "    all_corefs = per_entities['COREF'].unique()\n",
    "    coref2mention_tokens = defaultdict(list)\n",
    "    for _, row in per_entities.iterrows():\n",
    "        for t in range(int(row['start_token']), int(row['end_token']) + 1):\n",
    "            coref2mention_tokens[str(row['COREF'])].append(t)\n",
    "    token2sent = dict(zip(tokens['token_ID_within_document'], tokens['sentence_ID']))\n",
    "\n",
    "    # ========== Edge Extraction Loop ==========\n",
    "    edges = []\n",
    "    used_windows = set()\n",
    "    for idx, tok in tokens.iterrows():\n",
    "        loc_surface = normalize_text(tok['word'])\n",
    "        s_id = tok['sentence_ID']\n",
    "        if loc_surface not in location_lookup:\n",
    "            continue\n",
    "        locus = location_lookup[loc_surface]\n",
    "        canonical_loc = locus['canonical_name']\n",
    "        x, y, z = locus[\"x\"], locus[\"y\"], locus[\"z\"]\n",
    "        win_key = (s_id, canonical_loc)\n",
    "        if win_key in used_windows:\n",
    "            continue\n",
    "        used_windows.add(win_key)\n",
    "        win_start = max(0, s_id - window_size)\n",
    "        win_end = s_id + window_size\n",
    "        window_sent_ids = list(range(win_start, win_end + 1))\n",
    "\n",
    "        window_token_ids = set(tokens[tokens['sentence_ID'].isin(window_sent_ids)]['token_ID_within_document'])\n",
    "        # --- Corefs present in this window ---\n",
    "        corefs_in_window = set()\n",
    "        for coref, mention_tokens in coref2mention_tokens.items():\n",
    "            if any(t in window_token_ids for t in mention_tokens):\n",
    "                corefs_in_window.add(coref)\n",
    "        if not corefs_in_window or len(corefs_in_window) < 2:\n",
    "            continue\n",
    "\n",
    "        # --------- Scene edges ---------------\n",
    "        coref_list = sorted(corefs_in_window)\n",
    "        for i in range(len(coref_list)):\n",
    "            for j in range(i + 1, len(coref_list)):\n",
    "                edges.append({\n",
    "                    \"Source\": coref_list[i],\n",
    "                    \"Target\": coref_list[j],\n",
    "                    \"Weight\": 1,\n",
    "                    \"EdgeType\": \"scene_copresence\",\n",
    "                    \"Location\": canonical_loc,\n",
    "                    \"X_Coord\": x,\n",
    "                    \"Y_Coord\": y,\n",
    "                    \"Z_Coord\": z,\n",
    "                    \"BookID\": book_id,\n",
    "                    \"LocationWindow_Sentence\": s_id,\n",
    "                    \"ContextWinStart\": win_start,\n",
    "                    \"ContextWinEnd\": win_end,\n",
    "                    \"Events\": \"\",\n",
    "                    \"Supersense\": \"\",\n",
    "                    \"QuoteText\": \"\"\n",
    "                })\n",
    "\n",
    "        # --------- Agent/Patient (verb) edges ---------------\n",
    "\n",
    "        # For Debugging:\n",
    "        # print(\"==== BEGIN Agent/Patient (verb) edges ====\")\n",
    "\n",
    "        # For debugging: List and count POS_tag values in window\n",
    "        # window_tokens = tokens[tokens['sentence_ID'].isin(window_sent_ids)]\n",
    "        # print(\"POS_tag unique values in window tokens:\", window_tokens['POS_tag'].unique())\n",
    "        # print(\"Total tokens in window:\", len(window_tokens))\n",
    "\n",
    "        # Candidate verbs diagnostic\n",
    "        # total_verb_candidates = 0\n",
    "        # for sent_id in window_sent_ids:\n",
    "        #     stoks = tokens[tokens['sentence_ID'] == sent_id]\n",
    "        #     for _, trow in stoks.iterrows():\n",
    "        #         if str(trow['POS_tag']) == 'VERB':\n",
    "        #             total_verb_candidates += 1\n",
    "        # print(f\"# candidate verbs found in this window: {total_verb_candidates}\")\n",
    "\n",
    "        # Prepare entity mapping (cat == \"PER\")\n",
    "        person_tokens = {}\n",
    "        for coref in corefs_in_window:\n",
    "            for t in coref2mention_tokens[coref]:\n",
    "                person_tokens[t] = coref\n",
    "\n",
    "        for sent_id in window_sent_ids:\n",
    "            stoks = tokens[tokens['sentence_ID'] == sent_id]\n",
    "            for _, trow in stoks.iterrows():\n",
    "                if str(trow['POS_tag']) != 'VERB':\n",
    "                    continue\n",
    "                verb_token_id = trow['token_ID_within_document']\n",
    "                verb_lemma = trow['lemma']\n",
    "                verb_word  = trow['word']\n",
    "                verb_ss    = tok2ss.get(verb_token_id, \"NA\")\n",
    "\n",
    "                # For Debugging\n",
    "                # print(f\"Verb candidate: {verb_word} (Lemma: {verb_lemma}) at token {verb_token_id} in sentence {sent_id}\")\n",
    "\n",
    "                subj_rows = stoks[\n",
    "                    (stoks['syntactic_head_ID'] == verb_token_id) &\n",
    "                    (stoks['dependency_relation'].isin(['nsubj', 'nsubjpass']))\n",
    "                ]\n",
    "                obj_rows = stoks[\n",
    "                    (stoks['syntactic_head_ID'] == verb_token_id) &\n",
    "                    (stoks['dependency_relation'].isin(['dobj', 'obj', 'iobj']))\n",
    "                ]\n",
    "                # For Debugging\n",
    "                # print(f\"  Found {len(subj_rows)} subjects and {len(obj_rows)} objects for this verb.\")\n",
    "\n",
    "                if subj_rows.empty or obj_rows.empty:\n",
    "                    continue\n",
    "\n",
    "                for _, subj_row in subj_rows.iterrows():\n",
    "                    subj_token_id = subj_row['token_ID_within_document']\n",
    "                    subj_coref = None\n",
    "                    for _, ent_row in per_entities.iterrows():\n",
    "                        if int(ent_row['start_token']) <= subj_token_id <= int(ent_row['end_token']):\n",
    "                            subj_coref = str(ent_row['COREF'])\n",
    "                            # For Debugging\n",
    "                            # print(f\"    Subject token {subj_token_id} maps to COREF {subj_coref} ('{ent_row['text']}')\")\n",
    "                            break\n",
    "                    if not subj_coref:\n",
    "                        # For Debugging\n",
    "                        # print(f\"    WARNING: Subject token {subj_token_id} could not be mapped to a 'PER' entity\")\n",
    "                        continue\n",
    "\n",
    "                    for _, obj_row in obj_rows.iterrows():\n",
    "                        obj_token_id = obj_row['token_ID_within_document']\n",
    "                        obj_coref = None\n",
    "                        for _, ent_row in per_entities.iterrows():\n",
    "                            if int(ent_row['start_token']) <= obj_token_id <= int(ent_row['end_token']):\n",
    "                                obj_coref = str(ent_row['COREF'])\n",
    "                                # For Debugging\n",
    "                                # print(f\"    Object token {obj_token_id} maps to COREF {obj_coref} ('{ent_row['text']}')\")\n",
    "                                break\n",
    "                        if not obj_coref:\n",
    "                            # For Debugging\n",
    "                            # print(f\"    WARNING: Object token {obj_token_id} could not be mapped to a 'PER' entity\")\n",
    "                            continue\n",
    "                        if subj_coref != obj_coref:\n",
    "                            # For Debugging                            \n",
    "                            # print(f\"    ---> Adding ACTION edge: Source: {subj_coref}, Target: {obj_coref}, Verb: {verb_lemma}, Supersense: {verb_ss}\")\n",
    "                            edges.append({\n",
    "                                \"Source\": subj_coref,\n",
    "                                \"Target\": obj_coref,\n",
    "                                \"Weight\": 1,\n",
    "                                \"EdgeType\": \"action\",\n",
    "                                \"Location\": canonical_loc,\n",
    "                                \"X_Coord\": x,\n",
    "                                \"Y_Coord\": y,\n",
    "                                \"Z_Coord\": z,\n",
    "                                \"BookID\": book_id,\n",
    "                                \"LocationWindow_Sentence\": s_id,\n",
    "                                \"ContextWinStart\": win_start,\n",
    "                                \"ContextWinEnd\": win_end,\n",
    "                                \"Events\": verb_lemma,\n",
    "                                \"Supersense\": verb_ss,\n",
    "                                \"QuoteText\": \"\"\n",
    "                            })\n",
    "\n",
    "        # For Debugging\n",
    "        # print(\"==== END Agent/Patient (verb) edges ====\")\n",
    "        \n",
    "        # --------- Dialogue edges (quotes) ---------------\n",
    "        if quotes is not None:\n",
    "            # All token_ID_within_document in window:\n",
    "            window_token_ids = set(tokens[tokens['sentence_ID'].isin(window_sent_ids)]['token_ID_within_document'])\n",
    "            for _, qrow in quotes.iterrows():\n",
    "                # quote_start/quote_end are token indices\n",
    "                quote_tokens = set(range(int(qrow['quote_start']), int(qrow['quote_end']) + 1))\n",
    "                if not window_token_ids.intersection(quote_tokens):\n",
    "                    continue\n",
    "                speaker = str(int(qrow['char_id'])) if not pd.isna(qrow['char_id']) else None\n",
    "                if not speaker or speaker not in corefs_in_window:\n",
    "                    continue\n",
    "                quote_text = qrow['quote'] if 'quote' in qrow and not pd.isna(qrow['quote']) else \"\"\n",
    "                for other_coref in corefs_in_window:\n",
    "                    if other_coref != speaker:\n",
    "                        edges.append({\n",
    "                            \"Source\": speaker,\n",
    "                            \"Target\": other_coref,\n",
    "                            \"Weight\": 1,\n",
    "                            \"EdgeType\": \"dialogue\",\n",
    "                            \"Location\": canonical_loc,\n",
    "                            \"X_Coord\": x,\n",
    "                            \"Y_Coord\": y,\n",
    "                            \"Z_Coord\": z,\n",
    "                            \"BookID\": book_id,\n",
    "                            \"LocationWindow_Sentence\": s_id,\n",
    "                            \"ContextWinStart\": win_start,\n",
    "                            \"ContextWinEnd\": win_end,\n",
    "                            \"Events\": \"\",\n",
    "                            \"Supersense\": \"\",\n",
    "                            \"QuoteText\": quote_text\n",
    "                        })\n",
    "\n",
    "    # ================= NODE TABLE ========================\n",
    "    character_corefs_in_edges = set()\n",
    "    for edge in edges:\n",
    "        character_corefs_in_edges.add(str(edge['Source']))\n",
    "        character_corefs_in_edges.add(str(edge['Target']))\n",
    "    nodes = []\n",
    "    for coref in character_corefs_in_edges:\n",
    "        node_attr = get_character_metadata_from_bookjson(book_json, coref) if book_json else {}\n",
    "        label = node_attr.get(\"label\") or get_main_label_from_entities(entities, coref)\n",
    "        nodes.append({\n",
    "            \"Id\": coref,\n",
    "            \"Label\": label,\n",
    "            \"Gender\": node_attr.get(\"gender\", \"\"),\n",
    "            \"MentionCount\": node_attr.get(\"mention_count\", 0),\n",
    "            \"Type\": \"Character\",\n",
    "            \"BookID\": book_id\n",
    "            # \"MostCommonModifier\": node_attr.get(\"most_common_mod\", \"\"),\n",
    "            # \"ModifierCount\": node_attr.get(\"mod_count\", 0)\n",
    "        })\n",
    "\n",
    "    # -------------- Add location nodes --------------\n",
    "    added_locations = set()\n",
    "    for _, tok in tokens.iterrows():\n",
    "        loc_word = normalize_text(tok['word'])\n",
    "        if loc_word in location_lookup:\n",
    "            canonical_loc = location_lookup[loc_word]['canonical_name']\n",
    "            if canonical_loc in added_locations:\n",
    "                continue\n",
    "            added_locations.add(canonical_loc)\n",
    "            nodes.append({\n",
    "                \"Id\": f\"LOC_{canonical_loc.replace(' ', '_')}\",\n",
    "                \"Label\": canonical_loc,\n",
    "                \"Gender\": \"\",\n",
    "                \"MentionCount\": \"\",\n",
    "                \"Type\": \"Location\",\n",
    "                \"BookID\": book_id,\n",
    "                # \"MostCommonModifier\": \"\",\n",
    "                # \"ModifierCount\": \"\",\n",
    "                \"X_Coord\": location_lookup[loc_word]['x'],\n",
    "                \"Y_Coord\": location_lookup[loc_word]['y'],\n",
    "                \"Z_Coord\": location_lookup[loc_word]['z']\n",
    "            })\n",
    "\n",
    "    # =============== OUTPUT ===========================\n",
    "    edges_df = pd.DataFrame(edges)\n",
    "    nodes_df = pd.DataFrame(nodes).drop_duplicates(\"Id\")\n",
    "    edges_df.to_csv(os.path.join(analysis_output_dir, f\"{book_id}_edges.csv\"), index=False)\n",
    "    nodes_df.to_csv(os.path.join(analysis_output_dir, f\"{book_id}_nodes.csv\"), index=False)\n",
    "    print(f\"Done: {book_id}: {len(edges_df)} edges, {len(nodes_df)} nodes.\")\n",
    "\n",
    "print(\"\\n=== ALL BOOKS FINISHED ===\")\n",
    "print(f\"Edges/Nodes are in: {analysis_output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81bc961-b74d-41a8-a032-6f01ecd9f894",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
