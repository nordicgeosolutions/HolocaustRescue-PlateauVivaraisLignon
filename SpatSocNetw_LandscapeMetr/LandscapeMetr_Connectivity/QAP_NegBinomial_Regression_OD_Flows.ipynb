{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113cda80-e282-40c3-8df9-53f859778a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "QAP Negative Binomial GLM for OD flows in Python (ArcGIS Pro) with:\n",
    "- Optional exposure offset (dyadic field; used as log(offset) in the model)\n",
    "- Directed or Undirected handling (Undirected collapses i<->j pairs)\n",
    "\n",
    "Script tool parameters (ORDER MATTERS):\n",
    "  0 in_table         (GDB table path)\n",
    "  1 source_field     (e.g., \"SourceID\")\n",
    "  2 target_field     (e.g., \"TargetID\")\n",
    "  3 dep_field        (count-dependent variable, e.g., \"Weight\")\n",
    "  4 predictor_fields (multi-value or semicolon-separated list)\n",
    "  5 exposure_field   (optional; blank to omit; e.g., \"Exposure\")\n",
    "  6 reps             (number of QAP permutations, e.g., 5000)\n",
    "  7 directed         (\"TRUE\"/\"FALSE\")\n",
    "  8 out_coef_csv     (output coefficients CSV path)\n",
    "  9 out_pred_csv     (output predictions CSV path)\n",
    "\n",
    "Outputs:\n",
    "- Coef CSV: Predictor, Estimate, PermP\n",
    "  (Estimates are NB-GLM coefficients on log scale; exp(Estimate)=IRR)\n",
    "- Predictions CSV: SourceID, TargetID, Observed, Predicted, Residual\n",
    "\n",
    "Requires: statsmodels (install in ArcGIS Pro env, e.g., conda install -n arcgispro-py3 statsmodels)\n",
    "\"\"\"\n",
    "\n",
    "import arcpy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Try to import statsmodels; fail with a clear message if missing.\n",
    "try:\n",
    "    import statsmodels.api as sm\n",
    "except Exception:\n",
    "    arcpy.AddError(\n",
    "        \"This tool requires the 'statsmodels' Python package.\\n\"\n",
    "        \"Install with: conda install -n arcgispro-py3 statsmodels\"\n",
    "    )\n",
    "    raise\n",
    "\n",
    "\n",
    "def table_to_df(table, fields):\n",
    "    arr = arcpy.da.TableToNumPyArray(table, fields, skip_nulls=False)\n",
    "    if hasattr(arr, \"mask\"):\n",
    "        df = pd.DataFrame(arr.filled(np.nan))\n",
    "    else:\n",
    "        df = pd.DataFrame(arr)\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_matrix(nodes, src_vec, tgt_vec, val_vec):\n",
    "    n = len(nodes)\n",
    "    idx_map = {int(nid): i for i, nid in enumerate(nodes)}\n",
    "    I = np.array([idx_map[int(s)] for s in src_vec], dtype=int)\n",
    "    J = np.array([idx_map[int(t)] for t in tgt_vec], dtype=int)\n",
    "    M = np.full((n, n), np.nan, dtype=float)\n",
    "    M[I, J] = val_vec.astype(float)\n",
    "    np.fill_diagonal(M, np.nan)  # no self-ties\n",
    "    return M, I, J\n",
    "\n",
    "\n",
    "def sym_mean(A):\n",
    "    \"\"\"Symmetrize by nan-mean across ij and ji (keeps NaN if both are NaN).\"\"\"\n",
    "    AT = A.T\n",
    "    both_nan = np.isnan(A) & np.isnan(AT)\n",
    "    S = np.nanmean(np.dstack([A, AT]), axis=2)\n",
    "    S[both_nan] = np.nan\n",
    "    np.fill_diagonal(S, np.nan)\n",
    "    return S\n",
    "\n",
    "\n",
    "def undirected_upper_from_mats(Y, Xlist, E=None, sum_counts=True):\n",
    "    \"\"\"\n",
    "    Build undirected vectors (upper triangle i<j):\n",
    "      - Counts: sum across directions if sum_counts=True, else mean across directions\n",
    "      - Predictors and Exposure: nan-mean across directions (symmetric)\n",
    "    Returns y_vec, X_mat, E_vec, idx_i, idx_j\n",
    "    \"\"\"\n",
    "    n = Y.shape[0]\n",
    "    iu, ju = np.triu_indices(n, k=1)\n",
    "\n",
    "    # Counts: sum across directions (treat missing as 0, but keep NaN if both missing)\n",
    "    Y0 = np.nan_to_num(Y, nan=0.0)\n",
    "    YT0 = np.nan_to_num(Y.T, nan=0.0)\n",
    "    Ysum = Y0 + YT0\n",
    "    both_nan = np.isnan(Y) & np.isnan(Y.T)\n",
    "    y_vec = Ysum[iu, ju]\n",
    "    y_vec = np.where(both_nan[iu, ju], np.nan, y_vec)\n",
    "    if not sum_counts:\n",
    "        # Replace sum with nan-mean across directions\n",
    "        y_mean = np.nanmean(np.dstack([Y, Y.T]), axis=2)\n",
    "        y_vec = y_mean[iu, ju]\n",
    "\n",
    "    # Predictors: nan-mean across directions\n",
    "    X_cols = []\n",
    "    for Xk in Xlist:\n",
    "        Xs = np.nanmean(np.dstack([Xk, Xk.T]), axis=2)\n",
    "        X_cols.append(Xs[iu, ju])\n",
    "    X_mat = np.column_stack(X_cols) if X_cols else np.empty((len(y_vec), 0))\n",
    "\n",
    "    # Exposure: nan-mean across directions (avoids doubling when symmetric)\n",
    "    if E is not None:\n",
    "        Es = np.nanmean(np.dstack([E, E.T]), axis=2)\n",
    "        E_vec = Es[iu, ju]\n",
    "    else:\n",
    "        E_vec = None\n",
    "\n",
    "    return y_vec, X_mat, E_vec, iu, ju\n",
    "\n",
    "\n",
    "def _alpha_moments(y, mu, eps=1e-12):\n",
    "    \"\"\"\n",
    "    Method-of-moments NB2 alpha estimate:\n",
    "      Var(y) = mu + alpha * mu^2  =>  alpha = sum((y-mu)^2 - y) / sum(mu^2)\n",
    "    Clipped to be >= eps.\n",
    "    \"\"\"\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    mu = np.asarray(mu, dtype=float)\n",
    "    num = float(np.sum((y - mu) ** 2 - y))\n",
    "    den = float(np.sum(mu ** 2))\n",
    "    if den <= 0:\n",
    "        return eps\n",
    "    alpha = num / den\n",
    "    return float(max(alpha, eps))\n",
    "\n",
    "\n",
    "def fit_nb_glm(y, X, offset=None, max_alpha_iter=10, alpha_tol=1e-3):\n",
    "    \"\"\"\n",
    "    Fit a Negative Binomial GLM with log link using statsmodels.\n",
    "    Alpha is estimated iteratively by method-of-moments.\n",
    "\n",
    "    y: 1D array of non-negative counts.\n",
    "    X: 2D array of predictors (without intercept).\n",
    "    offset: 1D array on linear predictor scale (i.e., log(exposure)); or None.\n",
    "\n",
    "    Returns:\n",
    "      beta (including intercept),\n",
    "      mu (fitted means),\n",
    "      residuals (y - mu),\n",
    "      AIC, log-likelihood, alpha\n",
    "    \"\"\"\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    X1 = np.column_stack([np.ones(len(y), dtype=float), X])  # add intercept\n",
    "    off = None if offset is None else np.asarray(offset, dtype=float)\n",
    "\n",
    "    # Start with Poisson to initialize mu and alpha\n",
    "    pois = sm.GLM(y, X1, family=sm.families.Poisson(), offset=off).fit(maxiter=200, tol=1e-8)\n",
    "    mu = pois.fittedvalues.astype(float)\n",
    "    alpha = _alpha_moments(y, mu)\n",
    "\n",
    "    # Iterate NB with updated alpha\n",
    "    res_nb = None\n",
    "    for _ in range(max_alpha_iter):\n",
    "        fam = sm.families.NegativeBinomial(alpha=alpha)\n",
    "        res_nb = sm.GLM(y, X1, family=fam, offset=off).fit(maxiter=200, tol=1e-8)\n",
    "        mu_new = res_nb.fittedvalues.astype(float)\n",
    "        alpha_new = _alpha_moments(y, mu_new)\n",
    "        if abs(alpha_new - alpha) / (alpha + 1e-12) < alpha_tol:\n",
    "            mu = mu_new\n",
    "            alpha = alpha_new\n",
    "            break\n",
    "        mu = mu_new\n",
    "        alpha = alpha_new\n",
    "\n",
    "    if res_nb is None:\n",
    "        # Fallback to Poisson\n",
    "        beta = pois.params.astype(float)\n",
    "        mu = pois.fittedvalues.astype(float)\n",
    "        resid_resp = (y - mu).astype(float)\n",
    "        aic = float(pois.aic)\n",
    "        llf = float(pois.llf)\n",
    "        alpha = 0.0\n",
    "        return beta, mu, resid_resp, aic, llf, alpha\n",
    "\n",
    "    beta = res_nb.params.astype(float)\n",
    "    mu = res_nb.fittedvalues.astype(float)\n",
    "    resid_resp = (y - mu).astype(float)\n",
    "    aic = float(res_nb.aic)\n",
    "    llf = float(res_nb.llf)\n",
    "    return beta, mu, resid_resp, aic, llf, float(alpha)\n",
    "\n",
    "\n",
    "def qap_glm_nb(df, src_field, tgt_field, dep_field, pred_fields,\n",
    "               reps=5000, directed=True, exposure_field=None, seed=123):\n",
    "    # Clean types and ensure non-negative counts\n",
    "    df = df.dropna(subset=[src_field, tgt_field, dep_field] + pred_fields).copy()\n",
    "    df[src_field] = df[src_field].astype(int)\n",
    "    df[tgt_field] = df[tgt_field].astype(int)\n",
    "    for f in pred_fields:\n",
    "        df[f] = df[f].astype(float)\n",
    "    df[dep_field] = np.maximum(0.0, df[dep_field].astype(float))\n",
    "\n",
    "    # If exposure provided, clean it (must be > 0); small floor avoids log(0)\n",
    "    if exposure_field and exposure_field.strip():\n",
    "        df[exposure_field] = df[exposure_field].astype(float)\n",
    "        df[exposure_field] = np.where(df[exposure_field] <= 0, 1e-12, df[exposure_field])\n",
    "        use_offset = True\n",
    "    else:\n",
    "        use_offset = False\n",
    "\n",
    "    # Node set\n",
    "    nodes = sorted(set(df[src_field]).union(set(df[tgt_field])))\n",
    "    nodes = [int(n) for n in nodes]\n",
    "\n",
    "    # Build matrices\n",
    "    Y, I, J = build_matrix(\n",
    "        nodes, df[src_field].values, df[tgt_field].values, df[dep_field].values\n",
    "    )\n",
    "    Xmats = []\n",
    "    for f in pred_fields:\n",
    "        Xk, _, _ = build_matrix(\n",
    "            nodes, df[src_field].values, df[tgt_field].values, df[f].values\n",
    "        )\n",
    "        Xmats.append(Xk)\n",
    "    Emat = None\n",
    "    if use_offset:\n",
    "        Emat, _, _ = build_matrix(\n",
    "            nodes, df[src_field].values, df[tgt_field].values, df[exposure_field].values\n",
    "        )\n",
    "\n",
    "    # Observed vectors\n",
    "    if directed:\n",
    "        # Use exactly the rows in the table\n",
    "        y_obs = Y[I, J]\n",
    "        X_obs = np.column_stack([Xk[I, J] for Xk in Xmats]) if Xmats else np.empty((len(y_obs), 0))\n",
    "        mask = (~np.isnan(y_obs)) & (np.all(~np.isnan(X_obs), axis=1) if X_obs.size else True)\n",
    "        y_obs = y_obs[mask].astype(float)\n",
    "        X_obs = X_obs[mask, :].astype(float) if X_obs.size else np.empty((mask.sum(), 0))\n",
    "        src_obs = df[src_field].values[mask].astype(int)\n",
    "        tgt_obs = df[tgt_field].values[mask].astype(int)\n",
    "        if use_offset:\n",
    "            E_obs = Emat[I, J][mask].astype(float)\n",
    "            E_obs = np.where(E_obs <= 0, 1e-12, E_obs)\n",
    "            off_obs = np.log(E_obs)\n",
    "        else:\n",
    "            off_obs = None\n",
    "    else:\n",
    "        # Undirected: collapse to unique unordered pairs (i<j)\n",
    "        # Counts are summed across directions; predictors and exposure averaged\n",
    "        y_vec, X_mat, E_vec, iu, ju = undirected_upper_from_mats(\n",
    "            Y, Xmats, E=Emat, sum_counts=True\n",
    "        )\n",
    "        valid = ~np.isnan(y_vec)\n",
    "        if X_mat.size:\n",
    "            valid &= np.all(~np.isnan(X_mat), axis=1)\n",
    "        y_obs = y_vec[valid].astype(float)\n",
    "        X_obs = X_mat[valid, :].astype(float) if X_mat.size else np.empty((valid.sum(), 0))\n",
    "        src_obs = np.array([nodes[i] for i in iu[valid]], dtype=int)\n",
    "        tgt_obs = np.array([nodes[j] for j in ju[valid]], dtype=int)\n",
    "        if use_offset:\n",
    "            E_obs = E_vec[valid].astype(float)\n",
    "            E_obs = np.where(E_obs <= 0, 1e-12, E_obs)\n",
    "            off_obs = np.log(E_obs)\n",
    "        else:\n",
    "            off_obs = None\n",
    "\n",
    "    if y_obs.size == 0:\n",
    "        raise ValueError(\"No valid rows to model after cleaning.\")\n",
    "    if y_obs.size <= X_obs.shape[1]:\n",
    "        raise ValueError(\"Not enough observations to estimate all parameters.\")\n",
    "\n",
    "    # Fit observed NB-GLM with optional offset\n",
    "    beta_obs, mu_obs, resid_obs, aic_obs, llf_obs, alpha_obs = fit_nb_glm(\n",
    "        y_obs, X_obs, offset=off_obs\n",
    "    )\n",
    "    k = beta_obs.size\n",
    "    n = y_obs.size\n",
    "    aicc_obs = aic_obs + (2.0 * k * (k + 1)) / (n - k - 1) if (n - k - 1) > 0 else float(\"nan\")\n",
    "\n",
    "    # QAP permutations\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n_nodes = len(nodes)\n",
    "    B = np.full((reps, k), np.nan, dtype=float)\n",
    "\n",
    "    for r in range(reps):\n",
    "        perm = rng.permutation(n_nodes)\n",
    "        Yp = Y[perm][:, perm]\n",
    "        Xps = [Xk[perm][:, perm] for Xk in Xmats]\n",
    "        Ep = Emat[perm][:, perm] if use_offset else None\n",
    "\n",
    "        try:\n",
    "            if directed:\n",
    "                yp = Yp[I, J][mask]\n",
    "                if Xmats:\n",
    "                    Xp = np.column_stack([Xpk[I, J][mask] for Xpk in Xps])\n",
    "                else:\n",
    "                    Xp = np.empty((yp.size, 0))\n",
    "                if use_offset:\n",
    "                    Epv = Ep[I, J][mask]\n",
    "                    Epv = np.where(np.asarray(Epv) <= 0, 1e-12, Epv)\n",
    "                    offv = np.log(Epv)\n",
    "                else:\n",
    "                    offv = None\n",
    "                valid = (~np.isnan(yp)) & (np.all(~np.isnan(Xp), axis=1) if Xp.size else True)\n",
    "                yp = yp[valid]\n",
    "                Xp = Xp[valid, :] if Xp.size else Xp\n",
    "                offv = offv[valid] if offv is not None else None\n",
    "            else:\n",
    "                yv, Xv, Ev, iu_p, ju_p = undirected_upper_from_mats(\n",
    "                    Yp, Xps, E=Ep, sum_counts=True\n",
    "                )\n",
    "                valid = ~np.isnan(yv)\n",
    "                if Xv.size:\n",
    "                    valid &= np.all(~np.isnan(Xv), axis=1)\n",
    "                yp = yv[valid]\n",
    "                Xp = Xv[valid, :] if Xv.size else Xv\n",
    "                if use_offset:\n",
    "                    Ev = np.where(np.asarray(Ev) <= 0, 1e-12, Ev)\n",
    "                    offv = np.log(Ev[valid])\n",
    "                else:\n",
    "                    offv = None\n",
    "\n",
    "            if yp.size <= k - 1:\n",
    "                continue\n",
    "\n",
    "            bp, _, _, _, _, _ = fit_nb_glm(yp.astype(float), Xp.astype(float), offset=offv)\n",
    "            if bp.size == k:\n",
    "                B[r, :] = bp\n",
    "        except Exception:\n",
    "            # Non-convergence or numerical issue; skip this permutation\n",
    "            continue\n",
    "\n",
    "    # Two-sided permutation p-values\n",
    "    pvals = []\n",
    "    for j in range(k):\n",
    "        bj = beta_obs[j]\n",
    "        perm_vals = B[:, j]\n",
    "        perm_vals = perm_vals[~np.isnan(perm_vals)]\n",
    "        if perm_vals.size == 0:\n",
    "            p = np.nan\n",
    "        else:\n",
    "            p = (np.sum(np.abs(perm_vals) >= np.abs(bj)) + 1.0) / (perm_vals.size + 1.0)\n",
    "        pvals.append(p)\n",
    "\n",
    "    return (\n",
    "        beta_obs,\n",
    "        pvals,\n",
    "        mu_obs,\n",
    "        resid_obs,\n",
    "        src_obs,\n",
    "        tgt_obs,\n",
    "        aic_obs,\n",
    "        aicc_obs,\n",
    "        llf_obs,\n",
    "        alpha_obs,\n",
    "    )\n",
    "\n",
    "\n",
    "def main():\n",
    "    in_table = arcpy.GetParameterAsText(0)\n",
    "    src_field = arcpy.GetParameterAsText(1)\n",
    "    tgt_field = arcpy.GetParameterAsText(2)\n",
    "    dep_field = arcpy.GetParameterAsText(3)\n",
    "    preds_param = arcpy.GetParameterAsText(4)\n",
    "    exposure_field = arcpy.GetParameterAsText(5)  # may be empty\n",
    "    reps = int(arcpy.GetParameterAsText(6))\n",
    "    directed_str = arcpy.GetParameterAsText(7)\n",
    "    out_coef_csv = arcpy.GetParameterAsText(8)\n",
    "    out_pred_csv = arcpy.GetParameterAsText(9)\n",
    "\n",
    "    directed = True\n",
    "    if isinstance(directed_str, str):\n",
    "        directed = directed_str.strip().upper() in (\"TRUE\", \"T\", \"YES\", \"Y\", \"1\")\n",
    "\n",
    "    # Predictor fields may arrive as list or semicolon string\n",
    "    if isinstance(preds_param, list):\n",
    "        pred_fields = preds_param\n",
    "    else:\n",
    "        pred_fields = [p for p in preds_param.split(\";\") if p]\n",
    "\n",
    "    fields = [src_field, tgt_field, dep_field] + pred_fields\n",
    "    if exposure_field and exposure_field.strip():\n",
    "        fields.append(exposure_field)\n",
    "\n",
    "    df = table_to_df(in_table, fields)\n",
    "\n",
    "    (\n",
    "        beta,\n",
    "        pvals,\n",
    "        mu,\n",
    "        resid,\n",
    "        src_obs,\n",
    "        tgt_obs,\n",
    "        aic,\n",
    "        aicc,\n",
    "        llf,\n",
    "        alpha,\n",
    "    ) = qap_glm_nb(\n",
    "        df,\n",
    "        src_field,\n",
    "        tgt_field,\n",
    "        dep_field,\n",
    "        pred_fields,\n",
    "        reps=reps,\n",
    "        directed=directed,\n",
    "        exposure_field=(exposure_field if exposure_field and exposure_field.strip() else None),\n",
    "    )\n",
    "\n",
    "    # Coefficients CSV\n",
    "    coef_names = [\"Intercept\"] + pred_fields\n",
    "    coef_df = pd.DataFrame({\"Predictor\": coef_names, \"Estimate\": beta, \"PermP\": pvals})\n",
    "    os.makedirs(os.path.dirname(out_coef_csv), exist_ok=True)\n",
    "    coef_df.to_csv(out_coef_csv, index=False)\n",
    "\n",
    "    # Predictions CSV\n",
    "    pred_df = pd.DataFrame(\n",
    "        {\n",
    "            \"SourceID\": src_obs.astype(int),\n",
    "            \"TargetID\": tgt_obs.astype(int),\n",
    "            \"Observed\": (mu + resid).astype(float),\n",
    "            \"Predicted\": mu.astype(float),\n",
    "            \"Residual\": resid.astype(float),\n",
    "        }\n",
    "    )\n",
    "    os.makedirs(os.path.dirname(out_pred_csv), exist_ok=True)\n",
    "    pred_df.to_csv(out_pred_csv, index=False)\n",
    "\n",
    "    arcpy.AddMessage(\"QAP Negative Binomial GLM completed.\")\n",
    "    arcpy.AddMessage(f\"AIC: {aic:.6f}\")\n",
    "    arcpy.AddMessage(f\"AICc: {aicc:.6f}\")\n",
    "    arcpy.AddMessage(f\"Log-likelihood: {llf:.6f}\")\n",
    "    arcpy.AddMessage(f\"Estimated overdispersion (alpha): {alpha:.6g}\")\n",
    "    arcpy.AddMessage(f\"Directed: {directed}\")\n",
    "    if exposure_field and exposure_field.strip():\n",
    "        arcpy.AddMessage(f\"Offset field used (log scale): {exposure_field}\")\n",
    "    else:\n",
    "        arcpy.AddMessage(\"No offset field used.\")\n",
    "    arcpy.AddMessage(f\"Coefficients CSV: {out_coef_csv}\")\n",
    "    arcpy.AddMessage(f\"Predictions CSV: {out_pred_csv}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
