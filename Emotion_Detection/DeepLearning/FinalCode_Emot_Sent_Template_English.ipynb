{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "155992c0-18fb-4da2-860c-2636afb36863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load basic libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from pandas import json_normalize\n",
    "import numpy as np\n",
    "\n",
    "# Set model to use GPU\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452a4272-3b49-471f-957b-2ef9d3e83ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALL Keywords extracted from:  Rules-Based Entity Ruler Custom-built Geoparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f7b2ff4-985e-4d3b-9c53-4746d842f63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Plateau_Locations        LAT       LON     ALT\n",
      "0      Chambon-sur-Lignon  45.060810  4.302941   961.0\n",
      "1                     Fay  44.985435  4.226421  1181.0\n",
      "2          Fay-sur-Lignon  44.985435  4.226421  1181.0\n",
      "3              La Fayolle  45.053501  4.285560  1002.0\n",
      "4              Le Chambon  45.060810  4.302941   961.0\n",
      "5   Le Chambon sur Lignon  45.060810  4.302941   961.0\n",
      "6   Le Chambon-sur-Lignon  45.060810  4.302941   961.0\n",
      "7                Le Mazet  45.047192  4.244734  1027.0\n",
      "8               Les Tavas  45.051365  4.338720  1005.0\n",
      "9                   Mazet  45.047192  4.244734  1027.0\n",
      "10                Plateau  45.060810  4.302941   961.0\n",
      "11           Saint-Agreve  45.010014  4.395049  1066.0\n",
      "12           Saint-Jeures  45.095669  4.205091  1043.0\n",
      "13                  Tence  45.116135  4.289156   849.0\n",
      "['Chambon-sur-Lignon', 'Fay', 'Fay-sur-Lignon', 'La Fayolle', 'Le Chambon', 'Le Chambon sur Lignon', 'Le Chambon-sur-Lignon', 'Le Mazet', 'Les Tavas', 'Mazet', 'Plateau', 'Saint-Agreve', 'Saint-Jeures', 'Tence']\n"
     ]
    }
   ],
   "source": [
    "## Reading in the csv file, from ENGLISH language Rules-Based NLP results, of the extracted names and coordinates of\n",
    "## the towns, villages, and named features of the Plateau, creating a pandas dataframe with these locations and their\n",
    "## coordinates, then creating a list of just the location names, which then becomes one of the lists of keywords.\n",
    "\n",
    "df_PlatLoc = pd.read_csv(\"YOUR_DATA_keywords\")\n",
    "df_PlatLoc = df_PlatLoc.dropna()\n",
    "df_PlatLoc = df_PlatLoc[[\"Plateau_Locations\", \"LAT\", \"LON\", \"ALT\"]]\n",
    "df_PlatLoc[\"LAT\"] = pd.to_numeric(df_PlatLoc[\"LAT\"], downcast=\"float\")\n",
    "df_PlatLoc[\"LON\"] = pd.to_numeric(df_PlatLoc[\"LON\"], downcast=\"float\")\n",
    "df_PlatLoc[\"ALT\"] = pd.to_numeric(df_PlatLoc[\"ALT\"], downcast=\"float\")\n",
    "df_PlatLoc.columns = [\"Plateau_Locations\", \"LAT\", \"LON\", \"ALT\"]\n",
    "print(df_PlatLoc)\n",
    "PlatLoc = df_PlatLoc[\"Plateau_Locations\"].to_list()\n",
    "print(PlatLoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa5e8ec8-c17d-4c7f-9e21-69cd640c96af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   other_French_Locations        LAT       LON    ALT\n",
      "0               Annemasse  46.194389  6.237750  441.0\n",
      "1            Camp de Gurs  43.273762 -0.740310  186.0\n",
      "2      Camp Saint-Cyprien  42.636631  3.034213    2.0\n",
      "3                  Drancy  48.925831  2.445280   47.0\n",
      "4                    Gurs  43.273762 -0.740310  186.0\n",
      "5                  Le Puy  45.043659  3.885230  640.0\n",
      "6         Le Puy-en-Velay  45.043659  3.885230  640.0\n",
      "7                    Lyon  45.748459  4.846710  173.0\n",
      "8               Marseille  43.296951  5.381070   28.0\n",
      "9              Marseilles  43.296951  5.381070   28.0\n",
      "10                  Paris  48.853409  2.348800   42.0\n",
      "11                    Puy  45.043659  3.885230  640.0\n",
      "12             Rivesaltes  42.807911  2.890970   30.0\n",
      "13          Saint Étienne  45.433891  4.390000  529.0\n",
      "14          Saint-Cyprien  42.636631  3.034213    2.0\n",
      "15          Saint-Etienne  45.433891  4.390000  529.0\n",
      "16          Saint-Étienne  45.433891  4.390000  529.0\n",
      "17            St. Etienne  45.433891  4.390000  529.0\n",
      "18                Valence  44.925598  4.909560  126.0\n",
      "['Annemasse', 'Camp de Gurs', 'Camp Saint-Cyprien', 'Drancy', 'Gurs', 'Le Puy', 'Le Puy-en-Velay', 'Lyon', 'Marseille', 'Marseilles', 'Paris', 'Puy', 'Rivesaltes', 'Saint Étienne', 'Saint-Cyprien', 'Saint-Etienne', 'Saint-Étienne', 'St. Etienne', 'Valence']\n"
     ]
    }
   ],
   "source": [
    "## Reading in the csv file, from ENGLISH language Rules-Based NLP results, of the extracted names and coordinates of\n",
    "## the other locations in France (not on Plateau), creating a pandas dataframe with these locations and their\n",
    "## coordinates, then creating a list of just the location names, which then becomes one of the lists of keywords.\n",
    "\n",
    "df_FranceLoc = pd.read_csv(\"YOUR_DATA_keywords\")\n",
    "df_FranceLoc = df_FranceLoc.dropna()\n",
    "df_FranceLoc = df_FranceLoc[[\"other_French_Locations\", \"LAT\", \"LON\", \"ALT\"]]\n",
    "df_FranceLoc[\"LAT\"] = pd.to_numeric(df_FranceLoc[\"LAT\"], downcast=\"float\")\n",
    "df_FranceLoc[\"LON\"] = pd.to_numeric(df_FranceLoc[\"LON\"], downcast=\"float\")\n",
    "df_FranceLoc[\"ALT\"] = pd.to_numeric(df_FranceLoc[\"ALT\"], downcast=\"float\")\n",
    "df_FranceLoc.columns = [\"other_French_Locations\", \"LAT\", \"LON\", \"ALT\"]\n",
    "print(df_FranceLoc)\n",
    "FranceLoc = df_FranceLoc[\"other_French_Locations\"].to_list()\n",
    "print(FranceLoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "246ed4f0-cffe-48cb-8107-f781015bb553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Childrens_Homes        LAT       LON     ALT\n",
      "0          Côteau Fleuri  45.052872  4.340592  1028.0\n",
      "1               crickets  45.071281  4.326289  1067.0\n",
      "2                Faïdoli  45.055611  4.289219   976.0\n",
      "3               Grillons  45.071281  4.326289  1067.0\n",
      "4                 Guespy  45.062374  4.312919  1006.0\n",
      "5     House of the Rocks  45.052620  4.321041   959.0\n",
      "6              La Guespy  45.062374  4.312919  1006.0\n",
      "7                L'Abric  45.060558  4.311144   997.0\n",
      "8          L'Abric House  45.060558  4.311144   997.0\n",
      "9             Le Faïdoli  45.055611  4.289219   976.0\n",
      "10           Les Grillon  45.071281  4.326289  1067.0\n",
      "11          Les Grillons  45.071281  4.326289  1067.0\n",
      "12            Les Roches  45.052620  4.321041   959.0\n",
      "13  Maison des de Roches  45.052620  4.321041   959.0\n",
      "14     Maison des Roches  45.052620  4.321041   959.0\n",
      "15    Pension De Famille  45.071281  4.326289  1067.0\n",
      "16               Rohnert  45.052620  4.321041   959.0\n",
      "17          the crickets  45.071281  4.326289  1067.0\n",
      "['Côteau Fleuri', 'crickets', 'Faïdoli', 'Grillons', 'Guespy', 'House of the Rocks', 'La Guespy', \"L'Abric\", \"L'Abric House\", 'Le Faïdoli', 'Les Grillon', 'Les Grillons', 'Les Roches', 'Maison des de Roches', 'Maison des Roches', 'Pension De Famille', 'Rohnert', 'the crickets']\n"
     ]
    }
   ],
   "source": [
    "## Reading in the csv file, from ENGLISH language Rules-Based NLP results, of the extracted names and coordinates of\n",
    "## the Childrens Homes on the Plateau, creating a pandas dataframe with these locations and their\n",
    "## coordinates, then creating a list of just the location names, which then becomes one of the lists of keywords.\n",
    "\n",
    "df_homes = pd.read_csv(\"YOUR_DATA_keywords\")\n",
    "df_homes = df_homes.dropna()\n",
    "df_homes = df_homes[[\"Childrens_Homes\", \"LAT\", \"LON\", \"ALT\"]]\n",
    "df_homes[\"LAT\"] = pd.to_numeric(df_homes[\"LAT\"], downcast=\"float\")\n",
    "df_homes[\"LON\"] = pd.to_numeric(df_homes[\"LON\"], downcast=\"float\")\n",
    "df_homes[\"ALT\"] = pd.to_numeric(df_homes[\"ALT\"], downcast=\"float\")\n",
    "df_homes.columns = [\"Childrens_Homes\", \"LAT\", \"LON\", \"ALT\"]\n",
    "print(df_homes)\n",
    "homes = df_homes[\"Childrens_Homes\"].to_list()\n",
    "print(homes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7f2ea93-452d-459f-9802-be54cb60af61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Farms        LAT       LON    ALT\n",
      "0  La Bergerie  45.131886  4.284475  822.0\n",
      "['La Bergerie']\n"
     ]
    }
   ],
   "source": [
    "## Reading in the csv file, from ENGLISH language Rules-Based NLP results, of the extracted names and coordinates of\n",
    "## the Farms on the Plateau, creating a pandas dataframe with these locations and their\n",
    "## coordinates, then creating a list of just the location names, which then becomes one of the lists of keywords.\n",
    "\n",
    "df_farms = pd.read_csv(\"YOUR_DATA_keywords\")\n",
    "df_farms = df_farms.dropna()\n",
    "df_farms = df_farms[[\"Farms\", \"LAT\", \"LON\", \"ALT\"]]\n",
    "df_farms[\"LAT\"] = pd.to_numeric(df_farms[\"LAT\"], downcast=\"float\")\n",
    "df_farms[\"LON\"] = pd.to_numeric(df_farms[\"LON\"], downcast=\"float\")\n",
    "df_farms[\"ALT\"] = pd.to_numeric(df_farms[\"ALT\"], downcast=\"float\")\n",
    "df_farms.columns = [\"Farms\", \"LAT\", \"LON\", \"ALT\"]\n",
    "print(df_farms)\n",
    "farms = df_farms[\"Farms\"].to_list()\n",
    "print(farms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7058841e-73fc-49a5-823d-987644ede291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Schools        LAT       LON     ALT\n",
      "0          Cévenol  45.060959  4.303929   961.0\n",
      "1  College Cevenol  45.060959  4.303929   961.0\n",
      "2  College Cévenol  45.060959  4.303929   961.0\n",
      "3  Collège Cévenol  45.060959  4.303929   961.0\n",
      "4      Ferme École  45.055912  4.325135  1005.0\n",
      "5   Studio Cévenol  45.058929  4.301951   942.0\n",
      "['Cévenol', 'College Cevenol', 'College Cévenol', 'Collège Cévenol', 'Ferme École', 'Studio Cévenol']\n"
     ]
    }
   ],
   "source": [
    "## Reading in the csv file, from ENGLISH language Rules-Based NLP results, of the extracted names and coordinates of\n",
    "## the Schools on the Plateau, creating a pandas dataframe with these locations and their\n",
    "## coordinates, then creating a list of just the location names, which then becomes one of the lists of keywords.\n",
    "\n",
    "df_schools = pd.read_csv(\"YOUR_DATA_keywords\")\n",
    "df_schools = df_schools.dropna()\n",
    "df_schools = df_schools[[\"Schools\", \"LAT\", \"LON\", \"ALT\"]]\n",
    "df_schools[\"LAT\"] = pd.to_numeric(df_schools[\"LAT\"], downcast=\"float\")\n",
    "df_schools[\"LON\"] = pd.to_numeric(df_schools[\"LON\"], downcast=\"float\")\n",
    "df_schools[\"ALT\"] = pd.to_numeric(df_schools[\"ALT\"], downcast=\"float\")\n",
    "df_schools.columns = [\"Schools\", \"LAT\", \"LON\", \"ALT\"]\n",
    "print(df_schools)\n",
    "schools = df_schools[\"Schools\"].to_list()\n",
    "print(schools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b03aa9fe-9f57-4bf7-8e59-0b87657fc9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Features\n",
      "0      village\n",
      "1        house\n",
      "2       school\n",
      "3         home\n",
      "4         farm\n",
      "..         ...\n",
      "74        bank\n",
      "75        mill\n",
      "76  university\n",
      "77       wagon\n",
      "78       fence\n",
      "\n",
      "[79 rows x 1 columns]\n",
      "['village', 'house', 'school', 'home', 'farm', 'train', 'camp', 'church', 'woods', 'bus', 'homes', 'farms', 'mountains', 'border', 'station', 'garden', 'camps', 'houses', 'mountain', 'plateau', 'apartment', 'pass', 'concentration camps', 'railroad', 'car', 'road', 'college', 'barn', 'tree', 'valley', 'fields', 'store', 'field', 'forest', 'hotel', 'borders', 'trains', 'wall', 'hill', 'concentration camp', 'bridge', 'office', 'forests', 'trucks', 'universities', 'roads', 'schools', 'museum', 'railway station', 'churches', 'walls', 'truck', 'corridor', 'police station', 'airport', 'orchards', 'wagons', 'hospital', 'desert', 'peak', 'factory', 'monasteries', 'estate', 'stables', 'barracks', 'chateau', 'monastery', 'facility', 'museums', 'gardens', 'gate', 'trees', 'prison', 'banks', 'bank', 'mill', 'university', 'wagon', 'fence']\n"
     ]
    }
   ],
   "source": [
    "## Reading in the csv file, from ENGLISH language Rules-Based NLP results, of the extracted non-named\n",
    "## features mentioned in the testimonies, creating a pandas dataframe with features,\n",
    "## then creating a list of the features, which then becomes one of the lists of keywords.\n",
    "\n",
    "df_features = pd.read_csv(\"YOUR_DATA_keywords\")\n",
    "df_features = df_features.dropna()\n",
    "df_features = df_features[[\"Features\"]]\n",
    "df_features.columns = [\"Features\"]\n",
    "print(df_features)\n",
    "features = df_features[\"Features\"].to_list()\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70e3a330-1b56-4278-b9b4-93126c072167",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the ENGLISH-language dataset for analysis.\n",
    "\n",
    "path = \"YOUR_DATA_Eng\"\n",
    "\n",
    "def read_txt_files(directory):\n",
    "    # Reads all .txt files in a directory and returns a combined string of their contents.\n",
    "\n",
    "    file_contents = ''\n",
    "    \n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            with open(filepath, \"r\", encoding=\"utf8\") as f:\n",
    "                file_contents = file_contents + (f.read())\n",
    "    return file_contents\n",
    "\n",
    "texts = read_txt_files(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff1bbe81-92dc-4a6f-b4fc-9694928f8e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'joy', 'score': 0.9771687984466553}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import ENGLISH emotion classification model, and \"warm it up\" on the simple sentence.\n",
    "from transformers import pipeline\n",
    "classifier = pipeline(\"text-classification\", model=\"j-hartmann/emotion-english-distilroberta-base\")\n",
    "classifier(\"I love this!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea92c505-9ba7-4be5-9100-5232f0ba06e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define function to perform emotion classification on text that is 150 characters on either side of keywords\n",
    "\n",
    "def detect_emotion_with_context(text, keywords, context_size=150):\n",
    "    \"\"\"\n",
    "    Detects emotions associated with keywords in text, providing surrounding context.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text.\n",
    "        keywords (list): A list of keywords to search for.\n",
    "        context_size (int): The number of characters to include as context \n",
    "                          around the keyword.  In this case, I used 150 (tried 50, 100, 125, 200 first, \n",
    "                          but poor results).\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are keywords found, and values are lists \n",
    "              of tuples. Each tuple contains:\n",
    "              - The classified emotion (from hartmann_distil_roberta).\n",
    "              - The surrounding context of the keyword.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for keyword in keywords:\n",
    "        keyword_results = []\n",
    "        # Case-insensitive search for the keyword with re.IGNORECASE\n",
    "        for match in re.finditer(r'\\b' + re.escape(keyword) + r'\\b', text, re.IGNORECASE):\n",
    "            start = max(0, match.start() - 150)\n",
    "            end = min(len(text), match.end() + 150)\n",
    "            context = text[start:end]\n",
    "            emotion = classifier(context)\n",
    "            \n",
    "            keyword_results.append((keyword, emotion, context))\n",
    "        if keyword_results:\n",
    "            results[keyword] = keyword_results\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c58e03e5-9612-447d-8a4a-dab6e7c6c1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## IMPORTANT!!  Need to do each set of keywords one at a time, all the way through exporting the results\n",
    "## to their respective csv file.  This is because the model is run once per set of keywords.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9b3bee-d268-49a1-a1b0-523539f1b7f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
