{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "380e7cc7-cb38-4a47-8e37-7b8b355478e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for CUDA and GPU, and if True, GPU will be used.\n",
    "\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e7f7f4a3-1fc2-4369-8a5f-332f586b582d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in the sample dataset, the smaller sub-corpus.\n",
    "\n",
    "import os\n",
    "path = \"YOUR_DATA_test\"\n",
    "\n",
    "def read_txt_files(directory):\n",
    "    # Reads all .txt files in a directory and returns a combined string of their contents.\n",
    "\n",
    "    file_contents = ''\n",
    "    \n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            with open(filepath, \"r\", encoding=\"utf8\") as f:\n",
    "                file_contents = file_contents + (f.read())\n",
    "    return file_contents\n",
    "\n",
    "texts = read_txt_files(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "46fd835e-6d0d-4678-ba55-67a494f4174e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text split into 5 chunks for extraction.\n",
      "Extracted 0 toponyms from chunk at char 18995.\n",
      "Extracted 0 toponyms from chunk at char 0.\n",
      "Extracted 58 toponyms from chunk at char 9305.\n",
      "Extracted 36 toponyms from chunk at char 37267.\n",
      "Extracted 53 toponyms from chunk at char 28243.\n",
      "\n",
      "Stage 1 complete: Extracted 147 total toponym instances.\n",
      "Analyzed: Washington (from .toponym_instances)\n",
      "Analyzed: France (from .toponym_instances)\n",
      "Analyzed: Romania (from .toponym_instances)\n",
      "Analyzed: Chambon (from .toponym_instances)\n",
      "Analyzed: France (from .toponym_instances)\n",
      "Analyzed: Haute-Loire (from .toponym_instances)\n",
      "Analyzed: Chambon-sur-Lignon (from .toponym_instances)\n",
      "Analyzed: Europe (from .toponym_instances)\n",
      "Analyzed: Germany (from .toponym_instances)\n",
      "Analyzed: Saint-Etienne (from .toponym_instances)\n",
      "Analyzed: Eastern Europe (from .toponym_instances)\n",
      "Analyzed: Maison des Roches (from .toponym_instances)\n",
      "Analyzed: Maison des Roches (from .toponym_instances)\n",
      "Analyzed: Belgium (from .toponym_instances)\n",
      "Analyzed: Spain (from .toponym_instances)\n",
      "Analyzed: Hungary (from .toponym_instances)\n",
      "Analyzed: Drancy (from .toponym_instances)\n",
      "Analyzed: Switzerland (from .toponym_instances)\n",
      "Analyzed: Switzerland (from .toponym_instances)\n",
      "Analyzed: Russia (from .toponym_instances)\n",
      "Analyzed: Chambon-sur-Lignon (from .toponym_instances)\n",
      "Analyzed: France (from .toponym_instances)\n",
      "Analyzed: Chambon (from .toponym_instances)\n",
      "Analyzed: France (from .toponym_instances)\n",
      "Analyzed: Le Chambon (from .toponym_instances)\n",
      "Analyzed: Vallorcine (from .toponym_instances)\n",
      "Analyzed: Chambon (from .toponym_instances)\n",
      "Analyzed: Chambon-sur-Lignon (from .toponym_instances)\n",
      "Analyzed: Poland (from .toponym_instances)\n",
      "Analyzed: Vallorcine (from .toponym_instances)\n",
      "Analyzed: Switzerland (from .toponym_instances)\n",
      "Analyzed: France (from .toponym_instances)\n",
      "Analyzed: Rivesaltes (from .toponym_instances)\n",
      "Analyzed: Chambon-sur-Lignon (from .toponym_instances)\n",
      "Analyzed: Le Chambon-sur-Lignon (from .toponym_instances)\n",
      "Analyzed: Chambon-sur-Lignon (from .toponym_instances)\n",
      "Analyzed: Rivesaltes (from .toponym_instances)\n",
      "Analyzed: mountains (from .toponym_instances)\n",
      "Analyzed: OSE (from .toponym_instances)\n",
      "Analyzed: Austria (from .toponym_instances)\n",
      "Analyzed: France (from .toponym_instances)\n",
      "Analyzed: Chambon (from .toponym_instances)\n",
      "Analyzed: France (from .toponym_instances)\n",
      "Analyzed: Le Chambon-sur-Lignon (from .toponym_instances)\n",
      "Analyzed: Gurs (from .toponym_instances)\n",
      "Analyzed: La Guespy (from .toponym_instances)\n",
      "Analyzed: France (from .toponym_instances)\n",
      "Analyzed: French (from .toponym_instances)\n",
      "Analyzed: Chambon-sur-Lignon (from .toponym_instances)\n",
      "Analyzed: France (from .toponym_instances)\n",
      "Analyzed: French (from .toponym_instances)\n",
      "Analyzed: France (from .toponym_instances)\n",
      "Analyzed: Drancy (from .toponym_instances)\n",
      "Analyzed: La Guespy (from .toponym_instances)\n",
      "Analyzed: Austrian (from .toponym_instances)\n",
      "Analyzed: Auschwitz (from .toponym_instances)\n",
      "Analyzed: Le Chambon (from .toponym_instances)\n",
      "Analyzed: Drancy (from .toponym_instances)\n",
      "Analyzed: Dr. Le Forestier (from .toponym_instances)\n",
      "Analyzed: French (from .toponym_instances)\n",
      "Analyzed: Salvation Army (from .toponym_instances)\n",
      "Analyzed: Camp de Gurs (from .toponym_instances)\n",
      "Analyzed: Le Puy (from .toponym_instances)\n",
      "Analyzed: Milice Laval (from .toponym_instances)\n",
      "Analyzed: French (from .toponym_instances)\n",
      "Analyzed: Drancy (from .toponym_instances)\n",
      "Analyzed: Pastor Trocme (from .toponym_instances)\n",
      "Analyzed: Le Chambon (from .toponym_instances)\n",
      "Analyzed: Gurs (from .toponym_instances)\n",
      "Analyzed: Grenoble (from .toponym_instances)\n",
      "Analyzed: Annecy (from .toponym_instances)\n",
      "Analyzed: Alsace-Lorraine (from .toponym_instances)\n",
      "Analyzed: Swiss Red Cross (from .toponym_instances)\n",
      "Analyzed: Switzerland (from .toponym_instances)\n",
      "Analyzed: Valence (from .toponym_instances)\n",
      "Analyzed: Salvation Army (from .toponym_instances)\n",
      "Analyzed: Camp de Gurs (from .toponym_instances)\n",
      "Analyzed: Le Chambon (from .toponym_instances)\n",
      "Analyzed: Le Chambon (from .toponym_instances)\n",
      "Analyzed: Spain (from .toponym_instances)\n",
      "Analyzed: Washington, DC (from .toponym_instances)\n",
      "Analyzed: Bayside, New York (from .toponym_instances)\n",
      "Analyzed: Le Chambon (from .toponym_instances)\n",
      "Analyzed: La Guespy (from .toponym_instances)\n",
      "Analyzed: France (from .toponym_instances)\n",
      "Analyzed: Switzerland (from .toponym_instances)\n",
      "Analyzed: Le Chambon (from .toponym_instances)\n",
      "Analyzed: Swiss Red Cross (from .toponym_instances)\n",
      "Analyzed: Le Chambon (from .toponym_instances)\n",
      "Analyzed: Switzerland (from .toponym_instances)\n",
      "Analyzed: Swiss Red Cross (from .toponym_instances)\n",
      "Analyzed: Valence (from .toponym_instances)\n",
      "Analyzed: Camp de Gurs (from .toponym_instances)\n",
      "Analyzed: Valence (from .toponym_instances)\n",
      "Analyzed: Annecy (from .toponym_instances)\n",
      "Analyzed: Annecy (from .toponym_instances)\n",
      "Analyzed: Switzerland (from .toponym_instances)\n",
      "Analyzed: Camp de Gurs (from .toponym_instances)\n",
      "Analyzed: France (from .toponym_instances)\n",
      "Analyzed: Le Chambon-sur-Lignon (from .toponym_instances)\n",
      "Analyzed: Le Chambon (from .toponym_instances)\n",
      "Analyzed: OSE (from .toponym_instances)\n",
      "Analyzed: Le Chambon-sur Lignon (from .toponym_instances)\n",
      "Analyzed: French (from .toponym_instances)\n",
      "Analyzed: Le Chambon (from .toponym_instances)\n",
      "Analyzed: OSE (from .toponym_instances)\n",
      "Analyzed: Swiss (from .toponym_instances)\n",
      "Analyzed: OSE (from .toponym_instances)\n",
      "Analyzed: OSE (from .toponym_instances)\n",
      "Analyzed: OSE (from .toponym_instances)\n",
      "Analyzed: Swiss Red Cross (from .toponym_instances)\n",
      "Analyzed: France (from .toponym_instances)\n",
      "Analyzed: Swiss Red Cross (from .toponym_instances)\n",
      "Analyzed: New York (from .toponym_instances)\n",
      "Analyzed: Vichy (from .toponym_instances)\n",
      "Analyzed: Vichy (from .toponym_instances)\n",
      "Analyzed: Massif Central (from .toponym_instances)\n",
      "Analyzed: Toulouse (from .toponym_instances)\n",
      "Analyzed: Vichy (from .toponym_instances)\n",
      "Analyzed: France (from .toponym_instances)\n",
      "Analyzed: Le Chambon (from .toponym_instances)\n",
      "Analyzed: Swiss Red Cross (from .toponym_instances)\n",
      "Analyzed: Le Chambon (from .toponym_instances)\n",
      "Analyzed: Vichy (from .toponym_instances)\n",
      "Analyzed: Grenoble (from .toponym_instances)\n",
      "Analyzed: England (from .toponym_instances)\n",
      "Analyzed: Laval (from .toponym_instances)\n",
      "Analyzed: Swiss Red Cross (from .toponym_instances)\n",
      "Analyzed: Camp de Gurs (from .toponym_instances)\n",
      "Analyzed: Gurs (from .toponym_instances)\n",
      "Analyzed: Algiers (from .toponym_instances)\n",
      "Analyzed: Gurs (from .toponym_instances)\n",
      "Analyzed: England (from .toponym_instances)\n",
      "Analyzed: Swiss Red Cross (from .toponym_instances)\n",
      "Analyzed: French (from .toponym_instances)\n",
      "Analyzed: France (from .toponym_instances)\n",
      "Analyzed: Le Chambon-sur-Lignon (from .toponym_instances)\n",
      "Analyzed: Drancy (from .toponym_instances)\n",
      "Analyzed: Laval (from .toponym_instances)\n",
      "Analyzed: Swiss (from .toponym_instances)\n",
      "Analyzed: Lyon (from .toponym_instances)\n",
      "Analyzed: Drancy (from .toponym_instances)\n",
      "Analyzed: Gurs (from .toponym_instances)\n",
      "Analyzed: Swiss Red Cross (from .toponym_instances)\n",
      "Analyzed: France (from .toponym_instances)\n",
      "Analyzed: University of Lyon (from .toponym_instances)\n",
      "Analyzed: Auschwitz (from .toponym_instances)\n",
      "\n",
      "Stage 2 complete: Produced 147 detailed toponym analyses.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Developing function to identify and resolve toponyms, and detect emotions in context \n",
    "on either side of each toponym.  Context length is based on trying different lengths,\n",
    "with the final context length chosen based on which gives the most likely detected emotion\n",
    "with the highest confidence score.\n",
    "\n",
    "\"\"\"\n",
    "# Access libraries\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import tiktoken\n",
    "\n",
    "\n",
    "# Set a global variable for my OpenAI API key so that the model can be accessed.\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"\n",
    "client = OpenAI()\n",
    "\n",
    "# Alternative data for testing:\n",
    "#texts = \"I traveled from Paris to Berlin and saw New York on the way.  It was fantastic.  I was so happy.\"\n",
    "\n",
    "# ========== Robust OpenAI Output Extraction ==========\n",
    "def extract_json_from_arguments(response):\n",
    "    \"\"\"\n",
    "    Robust extraction for OpenAI responses.\n",
    "    Handles both function call and text output scenarios.\n",
    "    Returns dict or list or [].\n",
    "    \"\"\"\n",
    "    # Case 1: Function call pattern\n",
    "    if hasattr(response, \"output\") and response.output:\n",
    "        first = response.output[0]\n",
    "        if hasattr(first, \"arguments\"): # should be a string\n",
    "            arguments_string = first.arguments\n",
    "            if isinstance(arguments_string, (str, bytes)):\n",
    "                try:\n",
    "                    return json.loads(arguments_string)\n",
    "                except Exception as e:\n",
    "                    print(f\"JSON parsing error: {e}\\nARGUMENTS STRING: {arguments_string}\")\n",
    "                    return []\n",
    "            else:\n",
    "                # If already parsed (rare)\n",
    "                return arguments_string\n",
    "        # If it's classic text response\n",
    "        if hasattr(first, \"content\") and first.content:\n",
    "            text_fragment = getattr(first.content[0], \"text\", None)\n",
    "            if text_fragment:\n",
    "                try:\n",
    "                    return json.loads(text_fragment)\n",
    "                except Exception as e:\n",
    "                    print(f\"JSON parsing error (text): {e}\\nTEXT: {text_fragment}\")\n",
    "                    return []\n",
    "    # Case 2: Tool-style .outputs (not present in your current responses)\n",
    "    if hasattr(response, \"outputs\") and response.outputs and hasattr(response.outputs[0], \"arguments\"):\n",
    "        arguments = response.outputs[0].arguments\n",
    "        if arguments is not None:\n",
    "            return arguments\n",
    "    print(\"No recognizable output format found in OpenAI response.\")\n",
    "    return []\n",
    "\n",
    "# 2. Token-based Chunking (if needed). For huge inputs: For texts under 1M tokens: use the entire text in one go. For larger: chunk by tokens.\n",
    "\n",
    "def chunk_text_by_tokens(text, max_tokens=3000, overlap_tokens=600):\n",
    "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    all_tokens = enc.encode(text)\n",
    "    chunks = []\n",
    "    i, text_len = 0, len(all_tokens)\n",
    "    while i < text_len:\n",
    "        start = i\n",
    "        end = min(i + max_tokens, text_len)\n",
    "        token_chunk = all_tokens[start:end]\n",
    "        chunk_text = enc.decode(token_chunk)\n",
    "        offset = text.find(chunk_text)\n",
    "        chunks.append((chunk_text, offset))\n",
    "        if end == text_len:\n",
    "            break\n",
    "        i += max_tokens - overlap_tokens\n",
    "    return chunks\n",
    "\n",
    "# 3. API Call with Retry for Thread Use\n",
    "\n",
    "def call_api_with_retry_chunk(chunk, offset, extraction_instructions, client, max_output_tokens=2048, retries=4):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = client.responses.create(\n",
    "                model=\"gpt-4.1-2025-04-14\",\n",
    "                instructions=extraction_instructions,\n",
    "                input=chunk,\n",
    "                text={\"format\": {\"type\": \"text\"}},\n",
    "                reasoning={},\n",
    "                tools=[\n",
    "                    {\n",
    "                        \"type\": \"function\",\n",
    "                        \"name\": \"recognize_toponyms\",\n",
    "                        \"description\": \"Given the user input text, identify all the toponyms in the text.\",\n",
    "                        \"parameters\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"required\": [\"input_text\", \"toponyms\"],\n",
    "                            \"properties\": {\n",
    "                                \"input_text\": {\"type\": \"string\", \"description\": \"The text string from which to recognize and identify toponyms.\"},\n",
    "                                \"toponyms\": {\n",
    "                                    \"type\": \"array\",\n",
    "                \t\t\t\t    \"description\": \"Array of recognized and identified toponyms.\",\n",
    "                                    \"items\": {\n",
    "                                        \"type\": \"object\",\n",
    "                                        \"properties\": {\n",
    "                                            \"toponym\": {\"type\": \"string\"},\n",
    "                                            \"start_idx\": {\"type\": \"integer\"},\n",
    "                                            \"end_idx\": {\"type\": \"integer\"},\n",
    "                                        },\n",
    "                                        \"required\": [\"toponym\", \"start_idx\", \"end_idx\"],\n",
    "                                        \"additionalProperties\": False\n",
    "                                    }\n",
    "                                }\n",
    "                            },\n",
    "                            \"additionalProperties\": False\n",
    "                        },\n",
    "                        \"strict\": True\n",
    "                    }\n",
    "                ],\n",
    "                temperature=1.0,\n",
    "                tool_choice=\"required\",\n",
    "                max_output_tokens=max_output_tokens,\n",
    "                top_p=1,\n",
    "                store=True\n",
    "            )\n",
    "            return extract_json_from_arguments(response), offset\n",
    "        except Exception as e:\n",
    "            wait = 2 ** attempt\n",
    "            print(f\"[API] Error: {e}\\nRetrying in {wait}s (chunk at char {offset})...\")\n",
    "            time.sleep(wait)\n",
    "    print(f\"[API] Failed after retries for chunk at {offset}\")\n",
    "    return [], offset\n",
    "\n",
    "# 4. Stage 1: Parallel Toponym Extraction\n",
    "\n",
    "# ====== Load Extraction Prompt ======\n",
    "with open(\"openai_ToponymExtraction_prompt.txt\", encoding=\"utf-8\") as f:\n",
    "    extraction_instructions = f.read()\n",
    "\n",
    "# ====== Chunk Input ======\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "input_tokens = len(enc.encode(texts))\n",
    "if input_tokens < 3000:\n",
    "    chunks = [(texts, 0)]\n",
    "    print(\"Text fits in one chunk for extraction.\")\n",
    "else:\n",
    "    chunks = chunk_text_by_tokens(texts, max_tokens=3000, overlap_tokens=600)\n",
    "    print(f\"Text split into {len(chunks)} chunks for extraction.\")\n",
    "\n",
    "# ====== Run Extraction in Parallel ======\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "max_workers = 20   # safe for modern high-tier; can adjust up/down\n",
    "extracted_toponyms = []\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    futures = [\n",
    "        executor.submit(\n",
    "            call_api_with_retry_chunk, chunk, offset, extraction_instructions, client, 16384\n",
    "        )\n",
    "        for chunk, offset in chunks\n",
    "    ]\n",
    "    for f in as_completed(futures):\n",
    "        toponyms_chunk, offset = f.result()\n",
    "        num_found = 0\n",
    "        if isinstance(toponyms_chunk, list):\n",
    "            for t in toponyms_chunk:\n",
    "                if 'start_idx' in t: t[\"start_idx\"] += offset\n",
    "                if 'end_idx' in t: t[\"end_idx\"] += offset\n",
    "            extracted_toponyms += toponyms_chunk\n",
    "            num_found = len(toponyms_chunk)\n",
    "        elif isinstance(toponyms_chunk, dict) and \"toponyms\" in toponyms_chunk:\n",
    "            for t in toponyms_chunk[\"toponyms\"]:\n",
    "                if 'start_idx' in t: t[\"start_idx\"] += offset\n",
    "                if 'end_idx' in t: t[\"end_idx\"] += offset\n",
    "            extracted_toponyms += toponyms_chunk[\"toponyms\"]\n",
    "            num_found = len(toponyms_chunk[\"toponyms\"])\n",
    "        print(f\"Extracted {num_found} toponyms from chunk at char {offset}.\")\n",
    "\n",
    "print(f\"\\nStage 1 complete: Extracted {len(extracted_toponyms)} total toponym instances.\")\n",
    "with open(\"extracted_toponyms.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(extracted_toponyms, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# 5. Stage 2: Parallel Toponym Analysis\n",
    "\n",
    "# ====== Load Analysis Prompt ======\n",
    "with open(\"openai_ToponymEmotionAnalysis_prompt.txt\", encoding=\"utf-8\") as f:\n",
    "    analysis_instructions = f.read()\n",
    "\n",
    "def call_api_with_retry_analysis(\n",
    "    toponym_obj,\n",
    "    texts,\n",
    "    client,\n",
    "    analysis_instructions,\n",
    "    max_output_tokens=2048,\n",
    "    retries=4,\n",
    "):\n",
    "    toponym_str = toponym_obj[\"toponym\"]\n",
    "    idx = texts.lower().find(toponym_str.lower())\n",
    "    if idx == -1:\n",
    "        start_idx, end_idx = 0, 0\n",
    "        context = \"\"  # fallback, though function is expected to handle context dynamically\n",
    "    else:\n",
    "        # Start with a large context, function will try multiple sizes\n",
    "        window = 600  # You may increase this further\n",
    "        start_idx = max(0, idx - window)\n",
    "        end_idx = min(len(texts), idx + len(toponym_str) + window)\n",
    "        context = texts[start_idx:end_idx]\n",
    "    user_input = {\n",
    "        \"original_text\": context if context else texts,\n",
    "        \"toponym_instances\": [{**toponym_obj, \"original_range\": [start_idx, end_idx]}]\n",
    "    }\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = client.responses.create(\n",
    "                model=\"gpt-4.1-2025-04-14\",\n",
    "                instructions=analysis_instructions,\n",
    "                input=json.dumps(user_input),\n",
    "                text={\"format\": {\"type\": \"text\"}},\n",
    "                reasoning={},\n",
    "                tools=[{\n",
    "                    \"type\": \"function\",\n",
    "                    \"name\": \"resolve_toponyms_and_detect_emotions\",\n",
    "                    \"description\": (\n",
    "                        \"Given the user input of the original text and extracted toponyms, \"\n",
    "                        \"determine latitude and longitude of each toponym and perform emotion detection. \"\n",
    "                        \"Try multiple possible context window sizes (~different context lengths) for each toponym and \"\n",
    "                        \"return the window (context) that maximizes the confidence score for the most likely detected emotion.\"\n",
    "                    ),\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"required\": [\"original_text\", \"toponym_instances\"],\n",
    "                        \"properties\": {\n",
    "                            \"original_text\": {\"type\": \"string\", \"description\": \"The text string from which to disambiguate toponyms and utilize their surrounding context.\"},\n",
    "                            \"toponym_instances\": {\n",
    "                                \"type\": \"array\",\n",
    "                \t\t\t\t\"description\": \"Array of identified toponyms, each containing properties of location details and emotional context.\",\n",
    "                                \"items\": {\n",
    "                                    \"type\": \"object\",\n",
    "                                    \"required\": [\n",
    "                                        \"toponym\", \"resolved_name\", \"latitude\",\n",
    "                                        \"longitude\", \"emotion\", \"confidence_score\",\n",
    "                                        \"context\", \"context_length\", \"original_range\"\n",
    "                                    ],\n",
    "                                    \"properties\": {\n",
    "                                        \"toponym\": {\"type\": \"string\", \"description\": \"The name of the toponym as found in the previous step.\"},\n",
    "                                        \"resolved_name\": {\"type\": \"string\", \"description\": \"The name of the resolved toponym as identified and disambiguated.\"},\n",
    "                                        \"latitude\": {\"type\": \"number\", \"description\": \"The latitude coordinate of the toponym.\"},\n",
    "                                        \"longitude\": {\"type\": \"number\", \"description\": \"The longitude coordinate of the toponym.\"},\n",
    "                                        \"emotion\": {\"type\": \"string\", \"description\": \"The most likely detected emotion around the toponym.\", \"enum\": [\n",
    "                                            \"anger\", \"disgust\", \"fear\", \"joy\", \"sadness\", \"surprise\", \"neutral\"\n",
    "                                        ]},\n",
    "                                        \"confidence_score\": {\"type\": \"number\", \"description\": \"The confidence score for the detected emotion, on a scale of 0 to 1.\"},\n",
    "                                        \"context\": {\"type\": \"string\", \"description\": \"The text block surrounding the toponym used for emotion detection, whose length is determined based on trying different lengths and seeing which one gives the highest confidence score for the most likely detected emotion.\"},\n",
    "                                        \"context_length\": {\"type\": \"number\", \"description\": \"The length, in characters including spaces, of the final text block surrounding the toponym used for emotion detection.\"},\n",
    "                                        \"original_range\": {\"type\": \"array\", \"description\": \"The original start and end position in the text of the toponym.\", \"items\": {\"type\": \"number\"}}\n",
    "                                    },\n",
    "                                    \"additionalProperties\": False,\n",
    "                                },\n",
    "                            }\n",
    "                        },\n",
    "                        \"additionalProperties\": False,\n",
    "                    },\n",
    "                    \"strict\": True\n",
    "                }],\n",
    "                temperature=1,\n",
    "                tool_choice=\"required\",\n",
    "                max_output_tokens=max_output_tokens,\n",
    "                top_p=1,\n",
    "                store=True\n",
    "            )\n",
    "            return extract_json_from_arguments(response), toponym_str\n",
    "        except Exception as e:\n",
    "            wait = 2 ** attempt\n",
    "            print(f\"[API] Analysis error for '{toponym_str}': {e}\\nRetrying in {wait}s...\")\n",
    "            time.sleep(wait)\n",
    "    print(f\"[API] Analysis failed after retries for '{toponym_str}'.\")\n",
    "    return {\"toponym\": toponym_str, \"error\": \"Failed after retries\"}, toponym_str\n",
    "\n",
    "# Run Stage 2 in Parallel\n",
    "\n",
    "# ---- Load the extracted_toponyms ----\n",
    "with open(\"extracted_toponyms.json\", encoding=\"utf-8\") as f:\n",
    "    extracted_toponyms = json.load(f)\n",
    "\n",
    "analysis_results = []\n",
    "max_workers_analysis = 20  # You can go higher if needed\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=max_workers_analysis) as executor:\n",
    "    futures = [\n",
    "        executor.submit(\n",
    "            call_api_with_retry_analysis, t, texts, client, analysis_instructions, 2048\n",
    "        )\n",
    "        for t in extracted_toponyms\n",
    "    ]\n",
    "    for f in as_completed(futures):\n",
    "        batch_result, toponym_str = f.result()\n",
    "        # Handle lists/dicts as before\n",
    "        if isinstance(batch_result, list):\n",
    "            analysis_results += batch_result\n",
    "            print(f\"Analyzed: {toponym_str} (got list)\")\n",
    "        elif isinstance(batch_result, dict) and \"toponym_instances\" in batch_result:\n",
    "            analysis_results += batch_result[\"toponym_instances\"]\n",
    "            print(f\"Analyzed: {toponym_str} (from .toponym_instances)\")\n",
    "        else:\n",
    "            analysis_results.append(batch_result)\n",
    "            print(f\"Analyzed: {toponym_str} (error or unexpected shape)\")\n",
    "\n",
    "with open(\"analysis_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(analysis_results, f, ensure_ascii=False, indent=2)\n",
    "print(f\"\\nStage 2 complete: Produced {len(analysis_results)} detailed toponym analyses.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "53d90d8a-b1f8-4507-84aa-0389fe053388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toponym</th>\n",
       "      <th>resolved_name</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>emotion</th>\n",
       "      <th>confidence_score</th>\n",
       "      <th>context</th>\n",
       "      <th>context_length</th>\n",
       "      <th>original_range</th>\n",
       "      <th>emotion_numeric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Washington</td>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[10816, 10826]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>France</td>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[954, 960]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Romania</td>\n",
       "      <td>Romania</td>\n",
       "      <td>45.943200</td>\n",
       "      <td>24.966800</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.91</td>\n",
       "      <td>biggest tragedy of that time was to have those...</td>\n",
       "      <td>691</td>\n",
       "      <td>[9668, 9675]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chambon</td>\n",
       "      <td>Le Chambon-sur-Lignon, Haute-Loire, France</td>\n",
       "      <td>45.058600</td>\n",
       "      <td>4.293600</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.98</td>\n",
       "      <td>At the same time, the thing from the UGIF [Uni...</td>\n",
       "      <td>600</td>\n",
       "      <td>[10863, 10870]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>France</td>\n",
       "      <td>France</td>\n",
       "      <td>46.603354</td>\n",
       "      <td>1.888334</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.81</td>\n",
       "      <td>We were told that we'll have to get out in two...</td>\n",
       "      <td>698</td>\n",
       "      <td>[954, 2160]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>Gurs</td>\n",
       "      <td>Gurs internment camp, Pyrénées-Atlantiques, Fr...</td>\n",
       "      <td>43.293889</td>\n",
       "      <td>-0.676389</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.94</td>\n",
       "      <td>I found an Austrian with whom I get along well...</td>\n",
       "      <td>382</td>\n",
       "      <td>[31697, 31701]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>Swiss Red Cross</td>\n",
       "      <td>Swiss Red Cross</td>\n",
       "      <td>46.798562</td>\n",
       "      <td>8.231974</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.84</td>\n",
       "      <td>Her name was Hirsch. And she took us by train ...</td>\n",
       "      <td>417</td>\n",
       "      <td>[30234, 30249]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>France</td>\n",
       "      <td>France</td>\n",
       "      <td>46.603354</td>\n",
       "      <td>1.888334</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.95</td>\n",
       "      <td>We were told that we'll have to get out in two...</td>\n",
       "      <td>635</td>\n",
       "      <td>[954, 960]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>University of Lyon</td>\n",
       "      <td>University of Lyon</td>\n",
       "      <td>45.750300</td>\n",
       "      <td>4.852700</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.97</td>\n",
       "      <td>who was also a professor of economics of the U...</td>\n",
       "      <td>588</td>\n",
       "      <td>[30311, 30329]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>Auschwitz</td>\n",
       "      <td>Auschwitz concentration camp</td>\n",
       "      <td>50.026600</td>\n",
       "      <td>19.203600</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.95</td>\n",
       "      <td>because at that time we knew already that my p...</td>\n",
       "      <td>421</td>\n",
       "      <td>[31607, 31616]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>147 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                toponym                                      resolved_name  \\\n",
       "0            Washington                                                      \n",
       "1                France                                                      \n",
       "2               Romania                                            Romania   \n",
       "3               Chambon         Le Chambon-sur-Lignon, Haute-Loire, France   \n",
       "4                France                                             France   \n",
       "..                  ...                                                ...   \n",
       "142                Gurs  Gurs internment camp, Pyrénées-Atlantiques, Fr...   \n",
       "143     Swiss Red Cross                                    Swiss Red Cross   \n",
       "144              France                                             France   \n",
       "145  University of Lyon                                 University of Lyon   \n",
       "146           Auschwitz                       Auschwitz concentration camp   \n",
       "\n",
       "      latitude  longitude  emotion  confidence_score  \\\n",
       "0     0.000000   0.000000  neutral              0.00   \n",
       "1     0.000000   0.000000  neutral              0.00   \n",
       "2    45.943200  24.966800  sadness              0.91   \n",
       "3    45.058600   4.293600  neutral              0.98   \n",
       "4    46.603354   1.888334  neutral              0.81   \n",
       "..         ...        ...      ...               ...   \n",
       "142  43.293889  -0.676389  sadness              0.94   \n",
       "143  46.798562   8.231974      joy              0.84   \n",
       "144  46.603354   1.888334  neutral              0.95   \n",
       "145  45.750300   4.852700  neutral              0.97   \n",
       "146  50.026600  19.203600  sadness              0.95   \n",
       "\n",
       "                                               context  context_length  \\\n",
       "0                                                                    0   \n",
       "1                                                                    0   \n",
       "2    biggest tragedy of that time was to have those...             691   \n",
       "3    At the same time, the thing from the UGIF [Uni...             600   \n",
       "4    We were told that we'll have to get out in two...             698   \n",
       "..                                                 ...             ...   \n",
       "142  I found an Austrian with whom I get along well...             382   \n",
       "143  Her name was Hirsch. And she took us by train ...             417   \n",
       "144  We were told that we'll have to get out in two...             635   \n",
       "145  who was also a professor of economics of the U...             588   \n",
       "146  because at that time we knew already that my p...             421   \n",
       "\n",
       "     original_range emotion_numeric  \n",
       "0    [10816, 10826]               4  \n",
       "1        [954, 960]               4  \n",
       "2      [9668, 9675]               5  \n",
       "3    [10863, 10870]               4  \n",
       "4       [954, 2160]               4  \n",
       "..              ...             ...  \n",
       "142  [31697, 31701]               5  \n",
       "143  [30234, 30249]               3  \n",
       "144      [954, 960]               4  \n",
       "145  [30311, 30329]               4  \n",
       "146  [31607, 31616]               5  \n",
       "\n",
       "[147 rows x 10 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take response output in json format, put into a dataframe, then assign numeric values \n",
    "# to the detected emotions.\n",
    "\n",
    "df = pd.DataFrame(analysis_results)\n",
    "\n",
    "conditions = [\n",
    "    df[\"emotion\"] == \"anger\",\n",
    "    df[\"emotion\"] == \"disgust\",\n",
    "    df[\"emotion\"] == \"fear\",\n",
    "    df[\"emotion\"] == \"joy\",\n",
    "    df[\"emotion\"] == \"neutral\",\n",
    "    df[\"emotion\"] == \"sadness\",\n",
    "    df[\"emotion\"] == \"surprise\"\n",
    "]\n",
    "values = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\"]\n",
    "df[\"emotion_numeric\"] = np.select(conditions, values, default=\"Unknown\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2222fa53-54b9-49a9-8fec-2d18892200cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results to csv\n",
    "\n",
    "df.to_csv(\"Results9_ToponymsEmotions_smallSubCorpus.csv\", encoding=\"utf-8-sig\", index=False, header=True, mode=\"w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f13006-7409-41d6-b74e-4a80e4ff5414",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
