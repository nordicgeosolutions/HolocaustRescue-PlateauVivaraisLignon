{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "380e7cc7-cb38-4a47-8e37-7b8b355478e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for CUDA and GPU, and if True, GPU will be used.\n",
    "\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7f7f4a3-1fc2-4369-8a5f-332f586b582d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in the sample dataset, the smaller sub-corpus.\n",
    "\n",
    "import os\n",
    "path = \"YOUR_DATA_test\"\n",
    "\n",
    "def read_txt_files(directory):\n",
    "    # Reads all .txt files in a directory and returns a combined string of their contents.\n",
    "\n",
    "    file_contents = ''\n",
    "    \n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            with open(filepath, \"r\", encoding=\"utf8\") as f:\n",
    "                file_contents = file_contents + (f.read())\n",
    "    return file_contents\n",
    "\n",
    "texts = read_txt_files(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "46fd835e-6d0d-4678-ba55-67a494f4174e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text split into 77 char-based chunks for extraction.\n",
      "\n",
      "Initial extraction stage complete: Got 163 toponym instances (with possible duplicates).\n",
      "Deduplicated toponyms: 163 → 58\n",
      "\n",
      "Stage 1 complete: Saved 58 unique toponym instances to file.\n",
      "Analyzed: Eastern Europe (from .toponym_instances)\n",
      "Analyzed: France (from .toponym_instances)\n",
      "Analyzed: Le Chambon sur Lignon (from .toponym_instances)\n",
      "Analyzed: Vichy (from .toponym_instances)\n",
      "Analyzed: Les Grillons (from .toponym_instances)\n",
      "Analyzed: Gurs (from .toponym_instances)\n",
      "Analyzed: Cheylard (from .toponym_instances)\n",
      "Analyzed: Lavoulte bridge (from .toponym_instances)\n",
      "Analyzed: St. Agrève (from .toponym_instances)\n",
      "Analyzed: Paris (from .toponym_instances)\n",
      "Analyzed: Le Chambon-sur-Lignon (from .toponym_instances)\n",
      "Analyzed: Marseilles (from .toponym_instances)\n",
      "Analyzed: Auch (from .toponym_instances)\n",
      "Analyzed: Massif Central (from .toponym_instances)\n",
      "Analyzed: College Cevenol (from .toponym_instances)\n",
      "Analyzed: Les Caillols (from .toponym_instances)\n",
      "Analyzed: Gers (from .toponym_instances)\n",
      "Analyzed: Germany (from .toponym_instances)\n",
      "Analyzed: Drancy (from .toponym_instances)\n",
      "Analyzed: Hungary (from .toponym_instances)\n",
      "Analyzed: Romania (from .toponym_instances)\n",
      "Analyzed: Alps (from .toponym_instances)\n",
      "Analyzed: Saint-Etienne (from .toponym_instances)\n",
      "Analyzed: Switzerland (from .toponym_instances)\n",
      "Analyzed: Vallorcine (from .toponym_instances)\n",
      "Analyzed: Haute-Loire (from .toponym_instances)\n",
      "Analyzed: La Rouvière (from .toponym_instances)\n",
      "Analyzed: Poland (from .toponym_instances)\n",
      "Analyzed: Austria (from .toponym_instances)\n",
      "Analyzed: Spain (from .toponym_instances)\n",
      "Analyzed: Maison des Roches (from .toponym_instances)\n",
      "Analyzed: La Guespy (from .toponym_instances)\n",
      "Analyzed: Swiss (from .toponym_instances)\n",
      "Analyzed: Swiss border (from .toponym_instances)\n",
      "Analyzed: French side (from .toponym_instances)\n",
      "Analyzed: Britain (from .toponym_instances)\n",
      "Analyzed: Rivesaltes (from .toponym_instances)\n",
      "Analyzed: Perpignan (from .toponym_instances)\n",
      "Analyzed: Algiers (from .toponym_instances)\n",
      "Analyzed: German border (from .toponym_instances)\n",
      "Analyzed: Valence (from .toponym_instances)\n",
      "Analyzed: LePuy-en-Velay (from .toponym_instances)\n",
      "Analyzed: Annecy (from .toponym_instances)\n",
      "Analyzed: Le Puy (from .toponym_instances)\n",
      "Analyzed: German (from .toponym_instances)\n",
      "Analyzed: French (from .toponym_instances)\n",
      "Analyzed: Alsace-Lorraine (from .toponym_instances)\n",
      "Analyzed: University of Lyon (from .toponym_instances)\n",
      "Analyzed: Le Chambon-sur Lignon (from .toponym_instances)\n",
      "Analyzed: Toulouse (from .toponym_instances)\n",
      "Analyzed: New York (from .toponym_instances)\n",
      "Analyzed: Camp de Gurs (from .toponym_instances)\n",
      "Analyzed: Lyon (from .toponym_instances)\n",
      "Analyzed: Washington, DC (from .toponym_instances)\n",
      "Analyzed: Vichy, France (from .toponym_instances)\n",
      "Analyzed: Grenoble (from .toponym_instances)\n",
      "Analyzed: England (from .toponym_instances)\n",
      "Analyzed: Bayside, New York (from .toponym_instances)\n",
      "\n",
      "Stage 2 complete: Produced 57 detailed toponym analyses.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Developing function to identify and resolve toponyms, and detect emotions in context \n",
    "on either side of each toponym.  Context length is based on trying different lengths,\n",
    "with the final context length chosen based on which gives the most likely detected emotion\n",
    "with the highest confidence score.\n",
    "\n",
    "\"\"\"\n",
    "# Access libraries\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "import hashlib\n",
    "from difflib import SequenceMatcher\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Set a global variable for my OpenAI API key so that the model can be accessed.\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"\n",
    "client = OpenAI()\n",
    "\n",
    "# Alternative data for testing and to DEBUG:\n",
    "# texts = \"I traveled from Paris to Berlin and saw New York on the way.  It was fantastic.  I was so happy.\"\n",
    "\n",
    "# ========== Robust OpenAI Output Extraction ==========\n",
    "def extract_json_from_arguments(response):\n",
    "    \"\"\"\n",
    "    Robust extraction for OpenAI responses.\n",
    "    Handles both function call and text output scenarios.\n",
    "    Returns dict or list or [].\n",
    "    \"\"\"\n",
    "    # Case 1: Function call pattern\n",
    "    if hasattr(response, \"output\") and response.output:\n",
    "        first = response.output[0]\n",
    "        if hasattr(first, \"arguments\"): # should be a string\n",
    "            arguments_string = first.arguments\n",
    "            if isinstance(arguments_string, (str, bytes)):\n",
    "                try:\n",
    "                    return json.loads(arguments_string)\n",
    "                except Exception as e:\n",
    "                    print(f\"JSON parsing error: {e}\")\n",
    "                    return []\n",
    "            else:\n",
    "                # If already parsed (rare)\n",
    "                return arguments_string\n",
    "        # If it's classic text response\n",
    "        if hasattr(first, \"content\") and first.content:\n",
    "            text_fragment = getattr(first.content[0], \"text\", None)\n",
    "            if text_fragment:\n",
    "                try:\n",
    "                    return json.loads(text_fragment)\n",
    "                except Exception as e:\n",
    "                    print(f\"JSON parsing error (text): {e}\\nTEXT: {text_fragment}\")\n",
    "                    return []\n",
    "    # Case 2: Tool-style .outputs (not present in your current responses)\n",
    "    if hasattr(response, \"outputs\") and response.outputs and hasattr(response.outputs[0], \"arguments\"):\n",
    "        arguments = response.outputs[0].arguments\n",
    "        if arguments is not None:\n",
    "            return arguments\n",
    "    print(\"No recognizable output format found in OpenAI response.\")\n",
    "    return []\n",
    "\n",
    "# 2. Character-based Chunking (if needed).\n",
    "# Reduced number of characters to just 600, with no overlap at all.  \n",
    "\n",
    "def chunk_text_by_chars(text, chunk_size=600, overlap=0):\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    text_len = len(text)\n",
    "    while i < text_len:\n",
    "        start_char = i\n",
    "        end_char = min(i + chunk_size, text_len)\n",
    "        chunk_text = text[start_char:end_char]\n",
    "        chunks.append((chunk_text, start_char))\n",
    "        if end_char == text_len:\n",
    "            break\n",
    "        i += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "# 3. API Call with Retry for Thread Use\n",
    "\n",
    "def call_api_with_retry_chunk(chunk, extraction_instructions, client, max_output_tokens=2048, retries=4):\n",
    "    # print(\"Chunk being sent:\", repr(chunk))     # Can use this to DEBUG chunk issues\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = client.responses.create(\n",
    "                model=\"gpt-4.1-2025-04-14\",\n",
    "                instructions=extraction_instructions,\n",
    "                input=chunk,\n",
    "                text={\"format\": {\"type\": \"text\"}},\n",
    "                reasoning={},\n",
    "                tools=[\n",
    "                    {\n",
    "                        \"type\": \"function\",\n",
    "                        \"name\": \"recognize_toponyms\",\n",
    "                        \"description\": \"Given the user input text, identify all the toponyms in the text.\",\n",
    "                        \"parameters\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"required\": [\"input_text\", \"toponyms\"],\n",
    "                            \"properties\": {\n",
    "                                \"input_text\": {\n",
    "                                    \"type\": \"string\", \n",
    "                                    \"description\": \"The text string from which to recognize and identify toponyms.\"\n",
    "                                },\n",
    "                                \"toponyms\": {\n",
    "                                    \"type\": \"array\",\n",
    "                                    \"description\": \"Array of recognized and identified toponyms.\",\n",
    "                                    \"items\": {\n",
    "                                        \"type\": \"object\",\n",
    "                                        \"properties\": {\n",
    "                                            \"toponym\": {\"type\": \"string\"}\n",
    "                                        },\n",
    "                                        \"required\": [\"toponym\"],\n",
    "                                        \"additionalProperties\": False\n",
    "                                    }\n",
    "                                }\n",
    "                            },\n",
    "                            \"additionalProperties\": False\n",
    "                        },\n",
    "                        \"strict\": True\n",
    "                    }\n",
    "                ],\n",
    "                temperature=0,\n",
    "                tool_choice=\"required\",\n",
    "                max_output_tokens=max_output_tokens,\n",
    "                top_p=1,\n",
    "                store=True\n",
    "            )\n",
    "            \"\"\" These next three lines can be used to DEBUG responses or lack thereof\n",
    "            print(\"Raw response:\", response)\n",
    "            extracted = extract_json_from_arguments(response)\n",
    "            print(\"Extracted:\", extracted)\n",
    "            \"\"\"\n",
    "            return extract_json_from_arguments(response)         \n",
    "        except Exception as e:\n",
    "            wait = 2 ** attempt\n",
    "            print(f\"[API] Error: {e}\\nRetrying in {wait}s...\")\n",
    "            time.sleep(wait)\n",
    "    print(f\"[API] Failed after retries.\")\n",
    "    return []\n",
    "\n",
    "# 4. Stage 1: Parallel Toponym Extraction\n",
    "\n",
    "# ====== Load Extraction Prompt ======\n",
    "with open(\"openai_ToponymExtraction_prompt_complicated_24.txt\", encoding=\"utf-8\") as f:\n",
    "    extraction_instructions = f.read()\n",
    "\n",
    "# ====== Chunk Input ======\n",
    "    # chunk via characters\n",
    "chunks = chunk_text_by_chars(texts, chunk_size=600, overlap=0)\n",
    "print(f\"Text split into {len(chunks)} char-based chunks for extraction.\")\n",
    "\n",
    "# ====== Run Extraction in Parallel ======\n",
    "max_workers = 20   # safe for modern high-tier; can adjust up/down\n",
    "extracted_toponyms = []\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    futures = [\n",
    "        executor.submit(\n",
    "            call_api_with_retry_chunk, chunk, extraction_instructions, client\n",
    "        )\n",
    "        for chunk, _ in chunks\n",
    "    ]\n",
    "    for f in as_completed(futures):\n",
    "        result = f.result()\n",
    "        # print(\"DEBUG:\", result)          # Can use this if need to DEBUG results\n",
    "        if isinstance(result, dict) and \"toponyms\" in result:\n",
    "            extracted_toponyms += result[\"toponyms\"]\n",
    "        elif isinstance(result, list):\n",
    "            extracted_toponyms += result\n",
    "        else:\n",
    "            print(\"Warning: Unexpected result format\", result)\n",
    "\n",
    "print(f\"\\nInitial extraction stage complete: Got {len(extracted_toponyms)} toponym instances (with possible duplicates).\")\n",
    "\n",
    "# ------------------ DEDUPLICATION STEP --------------------\n",
    "def get_local_context(text, name, window=50):\n",
    "    \"\"\"Find the first occurrence of name in text and return local context window.\"\"\"\n",
    "    lowers = text.lower()\n",
    "    name_lower = name.lower()\n",
    "    idx = lowers.find(name_lower)\n",
    "    if idx == -1:\n",
    "        # fallback: just use the first window of the text (may dedupe global substrings, edge case)\n",
    "        return text[:2*window]\n",
    "    start = max(0, idx - window)\n",
    "    end = min(len(text), idx + len(name) + window)\n",
    "    return text[start:end]\n",
    "\n",
    "def deduplicate_by_fuzzy_context_and_longest(toponym_list, texts, window=50, similarity=0.90):\n",
    "    \"\"\"\n",
    "    Group extracted toponyms by fuzzy context similarity.\n",
    "    Keeps only the longest (most specific) name in each group.\n",
    "    Can be more aggressive with wider window & lower similarity threshold!!\n",
    "    \"\"\"\n",
    "    items = [\n",
    "        (t['toponym'].strip(), get_local_context(texts, t['toponym'].strip(), window), t)\n",
    "        for t in toponym_list\n",
    "    ]\n",
    "    groups = []\n",
    "    used = set()\n",
    "    for i, (name_i, ctx_i, obj_i) in enumerate(items):\n",
    "        if i in used: continue\n",
    "        group = [(name_i, ctx_i, obj_i)]\n",
    "        used.add(i)\n",
    "        for j, (name_j, ctx_j, obj_j) in enumerate(items):\n",
    "            if j <= i or j in used: continue\n",
    "            if ctx_i and ctx_j:\n",
    "                score = SequenceMatcher(None, ctx_i, ctx_j).ratio()\n",
    "                if score >= similarity:\n",
    "                    group.append((name_j, ctx_j, obj_j))\n",
    "                    used.add(j)\n",
    "        # Within the group, eliminate all substrings: keep only the longest(s)\n",
    "        group.sort(key=lambda g: len(g[0]), reverse=True)\n",
    "        deduped_names = set()\n",
    "        deduped_objs = []\n",
    "        for name, ctx, obj in group:\n",
    "            if not any(name in longer for longer in deduped_names if len(name) < len(longer)):\n",
    "                deduped_names.add(name)\n",
    "                deduped_objs.append(obj)\n",
    "        # Option 1: Only keep the very longest:\n",
    "        groups.append(deduped_objs[0])\n",
    "        # Option 2: To keep all equally-long max variants, use:\n",
    "        # groups.extend(deduped_objs[:1])  # or groups.extend(deduped_objs)\n",
    "    return groups\n",
    "    \n",
    "before = len(extracted_toponyms)\n",
    "extracted_toponyms = deduplicate_by_fuzzy_context_and_longest(extracted_toponyms, texts, window=50, similarity=0.90)\n",
    "after = len(extracted_toponyms)\n",
    "print(f\"Deduplicated toponyms: {before} → {after}\")\n",
    "\n",
    "# ------------------ END DEDUPLICATION STEP --------------------\n",
    "\n",
    "\n",
    "with open(\"extracted_toponyms.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(extracted_toponyms, f, ensure_ascii=False, indent=2)\n",
    "print(f\"\\nStage 1 complete: Saved {len(extracted_toponyms)} unique toponym instances to file.\")\n",
    "\n",
    "# 5. Stage 2: Parallel Toponym Analysis\n",
    "\n",
    "# ====== Load Analysis Prompt ======\n",
    "with open(\"openai_ToponymEmotionAnalysis_prompt_complicated_24.txt\", encoding=\"utf-8\") as f:\n",
    "    analysis_instructions = f.read()\n",
    "\n",
    "def call_api_with_retry_analysis(\n",
    "    toponym_obj,\n",
    "    texts,\n",
    "    client,\n",
    "    analysis_instructions,\n",
    "    max_output_tokens=32000,\n",
    "    retries=4,\n",
    "):\n",
    "    toponym_str = toponym_obj[\"toponym\"]\n",
    "    user_input = {\n",
    "        \"original_text\": texts,\n",
    "        \"toponym_instances\": [{**toponym_obj}]\n",
    "    }\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = client.responses.create(\n",
    "                model=\"gpt-4.1-2025-04-14\",\n",
    "                instructions=analysis_instructions,\n",
    "                input=json.dumps(user_input),\n",
    "                text={\"format\": {\"type\": \"text\"}},\n",
    "                reasoning={},\n",
    "                tools=[{\n",
    "                    \"type\": \"function\",\n",
    "                    \"name\": \"resolve_toponyms_and_detect_emotions\",\n",
    "                    \"description\": (\n",
    "                        \"Given the user input of the original text and extracted toponyms, \"\n",
    "                        \"determine latitude and longitude of each toponym and perform emotion detection. \"\n",
    "                        \"Try multiple possible context window sizes (~different context lengths) for each toponym and \"\n",
    "                        \"return the window (context) that maximizes the confidence score for the most likely detected emotion.\"\n",
    "                    ),\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"required\": [\"original_text\", \"toponym_instances\"],\n",
    "                        \"properties\": {\n",
    "                            \"original_text\": {\"type\": \"string\", \"description\": \"The text string from which to disambiguate toponyms and utilize their surrounding context.\"},\n",
    "                            \"toponym_instances\": {\n",
    "                                \"type\": \"array\",\n",
    "                \t\t\t\t\"description\": \"Array of identified toponyms, each containing properties of location details and emotional context.\",\n",
    "                                \"items\": {\n",
    "                                    \"type\": \"object\",\n",
    "                                    \"required\": [\n",
    "                                        \"toponym\", \"resolved_name\", \"latitude\",\n",
    "                                        \"longitude\", \"emotion\", \"confidence_score\",\n",
    "                                        \"context\", \"context_length\"\n",
    "                                    ],\n",
    "                                    \"properties\": {\n",
    "                                        \"toponym\": {\"type\": \"string\", \"description\": \"The name of the toponym as found in the previous step.\"},\n",
    "                                        \"resolved_name\": {\"type\": \"string\", \"description\": \"The name of the resolved toponym as identified and disambiguated.\"},\n",
    "                                        \"latitude\": {\"type\": \"number\", \"description\": \"The latitude coordinate of the toponym.\"},\n",
    "                                        \"longitude\": {\"type\": \"number\", \"description\": \"The longitude coordinate of the toponym.\"},\n",
    "                                        \"emotion\": {\"type\": \"string\", \"description\": \"The most likely detected emotion around the toponym.\", \"enum\": [\n",
    "                                            \"anger\", \"disgust\", \"fear\", \"joy\", \"sadness\", \"surprise\", \"neutral\"\n",
    "                                        ]},\n",
    "                                        \"confidence_score\": {\"type\": \"number\", \"description\": \"The confidence score for the detected emotion, on a scale of 0 to 1.\"},\n",
    "                                        \"context\": {\"type\": \"string\", \"description\": \"The text block surrounding the toponym used for emotion detection, whose length is determined based on trying different lengths and seeing which one gives the highest confidence score for the most likely detected emotion.\"},\n",
    "                                        \"context_length\": {\"type\": \"number\", \"description\": \"The length, in characters including spaces, of the final text block surrounding the toponym used for emotion detection.\"}\n",
    "                                    },\n",
    "                                    \"additionalProperties\": False,\n",
    "                                },\n",
    "                            }\n",
    "                        },\n",
    "                        \"additionalProperties\": False,\n",
    "                    },\n",
    "                    \"strict\": True\n",
    "                }],\n",
    "                temperature=1,\n",
    "                tool_choice=\"required\",\n",
    "                max_output_tokens=max_output_tokens,\n",
    "                top_p=1,\n",
    "                store=True\n",
    "            )\n",
    "            return extract_json_from_arguments(response), toponym_str\n",
    "        except Exception as e:\n",
    "            wait = 2 ** attempt\n",
    "            print(f\"[API] Analysis error for '{toponym_str}': {e}\\nRetrying in {wait}s...\")\n",
    "            time.sleep(wait)\n",
    "    print(f\"[API] Analysis failed after retries for '{toponym_str}'.\")\n",
    "    return {\"toponym\": toponym_str, \"error\": \"Failed after retries\"}, toponym_str\n",
    "\n",
    "# Run Stage 2 in Parallel\n",
    "\n",
    "# ---- Load the extracted_toponyms ----\n",
    "with open(\"extracted_toponyms.json\", encoding=\"utf-8\") as f:\n",
    "    extracted_toponyms = json.load(f)\n",
    "\"\"\"    \n",
    "# This sets a context window that is at the maximum of 600 characters to avoid the sometimes random \n",
    "5000-character context windows that the model decides to use when calling the function, even though\n",
    "I told it to not give me more than 600 character windows\n",
    "\"\"\"\n",
    "def get_context(text, toponym, window=600):\n",
    "    idx = text.lower().find(toponym.lower())\n",
    "    if idx == -1:\n",
    "        print(f\"Warning: Toponym {toponym} not found in text.\")\n",
    "        return text\n",
    "    start = max(0, idx - window)\n",
    "    end = min(len(text), idx + len(toponym) + window)\n",
    "    return text[start:end]\n",
    "\n",
    "analysis_results = []\n",
    "\"\"\"\n",
    "Keep max_workers relatively low to prevent truncation of output (which results in \"JSON parsing errors\" \n",
    "due to attempting this on a truncated \"list\" rather than the actual dictionary that it is).  \n",
    "Also keeps it below rate limits.\n",
    "\"\"\"\n",
    "max_workers_analysis = 6\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=max_workers_analysis) as executor:\n",
    "    futures = [\n",
    "        executor.submit(\n",
    "            call_api_with_retry_analysis, t, get_context(texts, t[\"toponym\"]), client, analysis_instructions, 32000\n",
    "        )\n",
    "        for t in extracted_toponyms\n",
    "    ]\n",
    "    for f in as_completed(futures):\n",
    "        batch_result, toponym_str = f.result()\n",
    "        # Handle lists/dicts as before\n",
    "        if isinstance(batch_result, list):\n",
    "            analysis_results += batch_result\n",
    "            print(f\"Analyzed: {toponym_str} (got list)\")\n",
    "        elif isinstance(batch_result, dict) and \"toponym_instances\" in batch_result:\n",
    "            analysis_results += batch_result[\"toponym_instances\"]\n",
    "            print(f\"Analyzed: {toponym_str} (from .toponym_instances)\")\n",
    "        else:\n",
    "            analysis_results.append(batch_result)\n",
    "            print(f\"Analyzed: {toponym_str} (error or unexpected shape)\")\n",
    "\n",
    "with open(\"analysis_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(analysis_results, f, ensure_ascii=False, indent=2)\n",
    "print(f\"\\nStage 2 complete: Produced {len(analysis_results)} detailed toponym analyses.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "53d90d8a-b1f8-4507-84aa-0389fe053388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toponym</th>\n",
       "      <th>resolved_name</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>emotion</th>\n",
       "      <th>confidence_score</th>\n",
       "      <th>context</th>\n",
       "      <th>context_length</th>\n",
       "      <th>emotion_numeric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Eastern Europe</td>\n",
       "      <td>Eastern Europe</td>\n",
       "      <td>54.526000</td>\n",
       "      <td>21.255100</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>France</td>\n",
       "      <td>France</td>\n",
       "      <td>46.603354</td>\n",
       "      <td>1.888334</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.70</td>\n",
       "      <td>At the same time, the thing from the UGIF [Uni...</td>\n",
       "      <td>406</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Le Chambon sur Lignon</td>\n",
       "      <td>Le Chambon-sur-Lignon</td>\n",
       "      <td>45.060810</td>\n",
       "      <td>4.302941</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.89</td>\n",
       "      <td>We were told that we'll have to get out in two...</td>\n",
       "      <td>508</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vichy</td>\n",
       "      <td>Vichy</td>\n",
       "      <td>46.127800</td>\n",
       "      <td>3.426400</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.70</td>\n",
       "      <td>His father [is] in [the French internment camp...</td>\n",
       "      <td>347</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Les Grillons</td>\n",
       "      <td>Les Grillons</td>\n",
       "      <td>45.037800</td>\n",
       "      <td>4.320800</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.83</td>\n",
       "      <td>Mr. Trocmé came on his bicycle to meet me (sti...</td>\n",
       "      <td>600</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Gurs</td>\n",
       "      <td>Gurs internment camp</td>\n",
       "      <td>43.350900</td>\n",
       "      <td>-0.800700</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.91</td>\n",
       "      <td>I found an Austrian with whom I get along well...</td>\n",
       "      <td>303</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Cheylard</td>\n",
       "      <td>Le Cheylard</td>\n",
       "      <td>44.917200</td>\n",
       "      <td>4.419400</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.61</td>\n",
       "      <td>Marseilles to Paris express (travel, change tr...</td>\n",
       "      <td>397</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Lavoulte bridge</td>\n",
       "      <td>Le Pouzin (Lavoûlte-sur-Rhône) bridge</td>\n",
       "      <td>44.800600</td>\n",
       "      <td>4.774800</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.79</td>\n",
       "      <td>at 7:20 a.m. I left aboard the \\nMarseilles to...</td>\n",
       "      <td>387</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>St. Agrève</td>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Paris</td>\n",
       "      <td>Paris</td>\n",
       "      <td>48.856600</td>\n",
       "      <td>2.352200</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.81</td>\n",
       "      <td>I left aboard the Marseilles to Paris express ...</td>\n",
       "      <td>422</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Le Chambon-sur-Lignon</td>\n",
       "      <td>Le Chambon-sur-Lignon</td>\n",
       "      <td>45.060810</td>\n",
       "      <td>4.302941</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.50</td>\n",
       "      <td>And I was told to \\nget off at a place called ...</td>\n",
       "      <td>525</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Marseilles</td>\n",
       "      <td>Marseille</td>\n",
       "      <td>43.296482</td>\n",
       "      <td>5.369780</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.87</td>\n",
       "      <td>We were told that we'll have to get out in two...</td>\n",
       "      <td>367</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Auch</td>\n",
       "      <td>Auch, Gers, France</td>\n",
       "      <td>43.646100</td>\n",
       "      <td>0.584900</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.77</td>\n",
       "      <td>And in due time, I was given a \\nFrench identi...</td>\n",
       "      <td>372</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Massif Central</td>\n",
       "      <td>Massif Central</td>\n",
       "      <td>45.500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.64</td>\n",
       "      <td>It is located at about 3,000 feet elevation in...</td>\n",
       "      <td>244</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>College Cevenol</td>\n",
       "      <td>Collège Cévenol</td>\n",
       "      <td>45.060810</td>\n",
       "      <td>4.302941</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.75</td>\n",
       "      <td>I was sent to school, to a school that \\nthey ...</td>\n",
       "      <td>448</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Les Caillols</td>\n",
       "      <td>Les Caillols</td>\n",
       "      <td>43.304944</td>\n",
       "      <td>5.443776</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.75</td>\n",
       "      <td>Tuesday, January 5, 1943 [Les Caillols] \\nMrs....</td>\n",
       "      <td>443</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Gers</td>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Germany</td>\n",
       "      <td>Germany</td>\n",
       "      <td>51.165691</td>\n",
       "      <td>10.451526</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.88</td>\n",
       "      <td>France. They didn't realize that they would be...</td>\n",
       "      <td>364</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Drancy</td>\n",
       "      <td>Drancy internment camp</td>\n",
       "      <td>48.925200</td>\n",
       "      <td>2.450900</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.98</td>\n",
       "      <td>this idyll didn't last too long, because alrea...</td>\n",
       "      <td>291</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Hungary</td>\n",
       "      <td>Hungary</td>\n",
       "      <td>47.162494</td>\n",
       "      <td>19.503304</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.89</td>\n",
       "      <td>And France was the haven. I mean, they thought...</td>\n",
       "      <td>345</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Romania</td>\n",
       "      <td>Romania</td>\n",
       "      <td>45.943200</td>\n",
       "      <td>24.966800</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.88</td>\n",
       "      <td>They didn't realize that they would be picked ...</td>\n",
       "      <td>390</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Alps</td>\n",
       "      <td>Alps</td>\n",
       "      <td>45.832622</td>\n",
       "      <td>6.865204</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.91</td>\n",
       "      <td>Now I have described the happenings from then,...</td>\n",
       "      <td>564</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Saint-Etienne</td>\n",
       "      <td>Saint-Étienne</td>\n",
       "      <td>45.439700</td>\n",
       "      <td>4.387200</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.87</td>\n",
       "      <td>It was a little village up above Saint-Etienne...</td>\n",
       "      <td>342</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Switzerland</td>\n",
       "      <td>Switzerland</td>\n",
       "      <td>46.818188</td>\n",
       "      <td>8.227512</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.97</td>\n",
       "      <td>Now I have described the happenings from then,...</td>\n",
       "      <td>517</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Vallorcine</td>\n",
       "      <td>Vallorcine</td>\n",
       "      <td>46.016700</td>\n",
       "      <td>6.933330</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.93</td>\n",
       "      <td>And they ordered a truck. And they brought us ...</td>\n",
       "      <td>260</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Haute-Loire</td>\n",
       "      <td>Haute-Loire</td>\n",
       "      <td>45.066700</td>\n",
       "      <td>3.783300</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.85</td>\n",
       "      <td>I am sad and weary. During rest hour Mrs. Cava...</td>\n",
       "      <td>349</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>La Rouvi\u0000e8re</td>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Poland</td>\n",
       "      <td>Poland</td>\n",
       "      <td>51.919438</td>\n",
       "      <td>19.145136</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.81</td>\n",
       "      <td>which was the home for students from all \\ncou...</td>\n",
       "      <td>366</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Austria</td>\n",
       "      <td>Austria</td>\n",
       "      <td>47.516200</td>\n",
       "      <td>14.550100</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.66</td>\n",
       "      <td>I go to school. Often think of you. I found an...</td>\n",
       "      <td>367</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Spain</td>\n",
       "      <td>Spain</td>\n",
       "      <td>40.463667</td>\n",
       "      <td>-3.749220</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.73</td>\n",
       "      <td>..., which was the home for students from all ...</td>\n",
       "      <td>386</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Maison des Roches</td>\n",
       "      <td>Maison des Roches, Le Chambon-sur-Lignon, France</td>\n",
       "      <td>45.060810</td>\n",
       "      <td>4.302941</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.81</td>\n",
       "      <td>I was assigned to Maison des Roches. Amazingly...</td>\n",
       "      <td>542</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>La Guespy</td>\n",
       "      <td>La Guespy</td>\n",
       "      <td>45.062500</td>\n",
       "      <td>4.294444</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.77</td>\n",
       "      <td>I'm in Le Chambon-sur-Lignon. It's way up on a...</td>\n",
       "      <td>485</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Swiss</td>\n",
       "      <td>Switzerland</td>\n",
       "      <td>46.818188</td>\n",
       "      <td>8.227512</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.88</td>\n",
       "      <td>and my first flight to \\nSwitzerland over the ...</td>\n",
       "      <td>570</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Swiss border</td>\n",
       "      <td>Swiss–French border</td>\n",
       "      <td>46.236200</td>\n",
       "      <td>6.022600</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.84</td>\n",
       "      <td>preparations were made to get us false identit...</td>\n",
       "      <td>463</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>French side</td>\n",
       "      <td>French side of the Swiss border</td>\n",
       "      <td>46.448137</td>\n",
       "      <td>6.966317</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.77</td>\n",
       "      <td>...emigration visas which we couldn't get on t...</td>\n",
       "      <td>340</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Britain</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>54.702354</td>\n",
       "      <td>-3.276575</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.91</td>\n",
       "      <td>We're lucky because there's a shortwave radio ...</td>\n",
       "      <td>376</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Rivesaltes</td>\n",
       "      <td>Rivesaltes</td>\n",
       "      <td>42.799300</td>\n",
       "      <td>2.872800</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.91</td>\n",
       "      <td>And put us on a train. We were put into handcu...</td>\n",
       "      <td>570</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Perpignan</td>\n",
       "      <td>Perpignan</td>\n",
       "      <td>42.698611</td>\n",
       "      <td>2.895556</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.93</td>\n",
       "      <td>But soon, unbelievable gesture of the French V...</td>\n",
       "      <td>427</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Algiers</td>\n",
       "      <td>Algiers</td>\n",
       "      <td>36.753800</td>\n",
       "      <td>3.058800</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.92</td>\n",
       "      <td>Her husband is fighting the Germans with de Ga...</td>\n",
       "      <td>413</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>German border</td>\n",
       "      <td>German-French Border</td>\n",
       "      <td>49.017000</td>\n",
       "      <td>7.100000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Valence</td>\n",
       "      <td>Valence, Drôme, France</td>\n",
       "      <td>44.933300</td>\n",
       "      <td>4.892400</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.88</td>\n",
       "      <td>Jean-Pierre, Pastor Trocme's son, and Marco Da...</td>\n",
       "      <td>377</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>LePuy-en-Velay</td>\n",
       "      <td>Le Puy-en-Velay</td>\n",
       "      <td>45.044440</td>\n",
       "      <td>3.880000</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.92</td>\n",
       "      <td>They say they're taking us to Le Puy, which is...</td>\n",
       "      <td>301</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Annecy</td>\n",
       "      <td>Annecy</td>\n",
       "      <td>45.899247</td>\n",
       "      <td>6.129384</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.81</td>\n",
       "      <td>I'm not supposed to understand \\nGerman, I'm s...</td>\n",
       "      <td>306</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Le Puy</td>\n",
       "      <td>Le Puy-en-Velay</td>\n",
       "      <td>45.044910</td>\n",
       "      <td>3.885870</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.97</td>\n",
       "      <td>They're starting the bus. They say they're tak...</td>\n",
       "      <td>341</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>French</td>\n",
       "      <td>French internment camp Gurs</td>\n",
       "      <td>43.318831</td>\n",
       "      <td>-0.758236</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.81</td>\n",
       "      <td>I found an Austrian with whom I get along well...</td>\n",
       "      <td>348</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Alsace-Lorraine</td>\n",
       "      <td>Alsace-Lorraine</td>\n",
       "      <td>48.250000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.00</td>\n",
       "      <td>and is a neutral country in the war, so Jews a...</td>\n",
       "      <td>990</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>University of Lyon</td>\n",
       "      <td>University of Lyon</td>\n",
       "      <td>45.729600</td>\n",
       "      <td>4.837800</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.83</td>\n",
       "      <td>before Mr. Philip, who was also a professor of...</td>\n",
       "      <td>519</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Le Chambon-sur Lignon</td>\n",
       "      <td>Le Chambon-sur-Lignon</td>\n",
       "      <td>45.060810</td>\n",
       "      <td>4.302941</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.95</td>\n",
       "      <td>And she took us by train to Toulouse. \\nWe sta...</td>\n",
       "      <td>565</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Toulouse</td>\n",
       "      <td>Toulouse</td>\n",
       "      <td>43.604652</td>\n",
       "      <td>1.444209</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.67</td>\n",
       "      <td>So now we, in September or October, I don't re...</td>\n",
       "      <td>322</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>New York</td>\n",
       "      <td>New York City</td>\n",
       "      <td>40.712776</td>\n",
       "      <td>-74.005974</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.79</td>\n",
       "      <td>which was organized by the Swiss Red Cross chi...</td>\n",
       "      <td>285</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Camp de Gurs</td>\n",
       "      <td>Camp de Gurs</td>\n",
       "      <td>43.316700</td>\n",
       "      <td>-0.716700</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.70</td>\n",
       "      <td>effWell, this comes, the, in, in the camp, in...</td>\n",
       "      <td>551</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Lyon</td>\n",
       "      <td>Lyon, France</td>\n",
       "      <td>45.750000</td>\n",
       "      <td>4.850000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.71</td>\n",
       "      <td>He was minister of finance under de Gaulle, wh...</td>\n",
       "      <td>483</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Washington, DC</td>\n",
       "      <td>Washington, D.C.</td>\n",
       "      <td>38.895110</td>\n",
       "      <td>-77.036370</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.81</td>\n",
       "      <td>Then Mademoiselle \\nUsach and the other one, t...</td>\n",
       "      <td>205</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Vichy, France</td>\n",
       "      <td>Vichy, France</td>\n",
       "      <td>46.128700</td>\n",
       "      <td>3.421500</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.93</td>\n",
       "      <td>So I was in,  we, so we got to this home from ...</td>\n",
       "      <td>568</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Grenoble</td>\n",
       "      <td>Grenoble</td>\n",
       "      <td>45.188529</td>\n",
       "      <td>5.724524</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.84</td>\n",
       "      <td>And she took a, and then, it was a Monday she ...</td>\n",
       "      <td>303</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>England</td>\n",
       "      <td>England</td>\n",
       "      <td>52.355518</td>\n",
       "      <td>-1.174320</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.71</td>\n",
       "      <td>He was minister of finance under de Gaulle, wh...</td>\n",
       "      <td>351</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Bayside, New York</td>\n",
       "      <td>Bayside, Queens, New York, United States</td>\n",
       "      <td>40.763416</td>\n",
       "      <td>-73.773056</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.81</td>\n",
       "      <td>her is Lilli Brown. Her name is Feihan now. Sh...</td>\n",
       "      <td>313</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  toponym                                     resolved_name  \\\n",
       "0          Eastern Europe                                    Eastern Europe   \n",
       "1                  France                                            France   \n",
       "2   Le Chambon sur Lignon                             Le Chambon-sur-Lignon   \n",
       "3                   Vichy                                             Vichy   \n",
       "4            Les Grillons                                      Les Grillons   \n",
       "5                    Gurs                              Gurs internment camp   \n",
       "6                Cheylard                                       Le Cheylard   \n",
       "7         Lavoulte bridge             Le Pouzin (Lavoûlte-sur-Rhône) bridge   \n",
       "8              St. Agrève                                                     \n",
       "9                   Paris                                             Paris   \n",
       "10  Le Chambon-sur-Lignon                             Le Chambon-sur-Lignon   \n",
       "11             Marseilles                                         Marseille   \n",
       "12                   Auch                                Auch, Gers, France   \n",
       "13         Massif Central                                    Massif Central   \n",
       "14        College Cevenol                                   Collège Cévenol   \n",
       "15           Les Caillols                                      Les Caillols   \n",
       "16                   Gers                                                     \n",
       "17                Germany                                           Germany   \n",
       "18                 Drancy                            Drancy internment camp   \n",
       "19                Hungary                                           Hungary   \n",
       "20                Romania                                           Romania   \n",
       "21                   Alps                                              Alps   \n",
       "22          Saint-Etienne                                     Saint-Étienne   \n",
       "23            Switzerland                                       Switzerland   \n",
       "24             Vallorcine                                        Vallorcine   \n",
       "25            Haute-Loire                                       Haute-Loire   \n",
       "26          La Rouvi\u0000e8re                                                     \n",
       "27                 Poland                                            Poland   \n",
       "28                Austria                                           Austria   \n",
       "29                  Spain                                             Spain   \n",
       "30      Maison des Roches  Maison des Roches, Le Chambon-sur-Lignon, France   \n",
       "31              La Guespy                                         La Guespy   \n",
       "32                  Swiss                                       Switzerland   \n",
       "33           Swiss border                               Swiss–French border   \n",
       "34            French side                   French side of the Swiss border   \n",
       "35                Britain                                    United Kingdom   \n",
       "36             Rivesaltes                                        Rivesaltes   \n",
       "37              Perpignan                                         Perpignan   \n",
       "38                Algiers                                           Algiers   \n",
       "39          German border                              German-French Border   \n",
       "40                Valence                            Valence, Drôme, France   \n",
       "41         LePuy-en-Velay                                   Le Puy-en-Velay   \n",
       "42                 Annecy                                            Annecy   \n",
       "43                 Le Puy                                   Le Puy-en-Velay   \n",
       "44                 French                       French internment camp Gurs   \n",
       "45        Alsace-Lorraine                                   Alsace-Lorraine   \n",
       "46     University of Lyon                                University of Lyon   \n",
       "47  Le Chambon-sur Lignon                             Le Chambon-sur-Lignon   \n",
       "48               Toulouse                                          Toulouse   \n",
       "49               New York                                     New York City   \n",
       "50           Camp de Gurs                                      Camp de Gurs   \n",
       "51                   Lyon                                      Lyon, France   \n",
       "52         Washington, DC                                  Washington, D.C.   \n",
       "53          Vichy, France                                     Vichy, France   \n",
       "54               Grenoble                                          Grenoble   \n",
       "55                England                                           England   \n",
       "56      Bayside, New York          Bayside, Queens, New York, United States   \n",
       "\n",
       "     latitude  longitude  emotion  confidence_score  \\\n",
       "0   54.526000  21.255100  neutral              0.00   \n",
       "1   46.603354   1.888334  neutral              0.70   \n",
       "2   45.060810   4.302941      joy              0.89   \n",
       "3   46.127800   3.426400  neutral              0.70   \n",
       "4   45.037800   4.320800      joy              0.83   \n",
       "5   43.350900  -0.800700  sadness              0.91   \n",
       "6   44.917200   4.419400  neutral              0.61   \n",
       "7   44.800600   4.774800  neutral              0.79   \n",
       "8    0.000000   0.000000  neutral              0.00   \n",
       "9   48.856600   2.352200  neutral              0.81   \n",
       "10  45.060810   4.302941  neutral              0.50   \n",
       "11  43.296482   5.369780  neutral              0.87   \n",
       "12  43.646100   0.584900  neutral              0.77   \n",
       "13  45.500000   3.000000  neutral              0.64   \n",
       "14  45.060810   4.302941  neutral              0.75   \n",
       "15  43.304944   5.443776  neutral              0.75   \n",
       "16   0.000000   0.000000  neutral              0.00   \n",
       "17  51.165691  10.451526  sadness              0.88   \n",
       "18  48.925200   2.450900     fear              0.98   \n",
       "19  47.162494  19.503304  sadness              0.89   \n",
       "20  45.943200  24.966800  sadness              0.88   \n",
       "21  45.832622   6.865204     fear              0.91   \n",
       "22  45.439700   4.387200  neutral              0.87   \n",
       "23  46.818188   8.227512     fear              0.97   \n",
       "24  46.016700   6.933330     fear              0.93   \n",
       "25  45.066700   3.783300  sadness              0.85   \n",
       "26   0.000000   0.000000  neutral              0.00   \n",
       "27  51.919438  19.145136  neutral              0.81   \n",
       "28  47.516200  14.550100  neutral              0.66   \n",
       "29  40.463667  -3.749220  neutral              0.73   \n",
       "30  45.060810   4.302941  neutral              0.81   \n",
       "31  45.062500   4.294444  neutral              0.77   \n",
       "32  46.818188   8.227512     fear              0.88   \n",
       "33  46.236200   6.022600  neutral              0.84   \n",
       "34  46.448137   6.966317  neutral              0.77   \n",
       "35  54.702354  -3.276575      joy              0.91   \n",
       "36  42.799300   2.872800  sadness              0.91   \n",
       "37  42.698611   2.895556     fear              0.93   \n",
       "38  36.753800   3.058800      joy              0.92   \n",
       "39  49.017000   7.100000  neutral              0.00   \n",
       "40  44.933300   4.892400     fear              0.88   \n",
       "41  45.044440   3.880000  sadness              0.92   \n",
       "42  45.899247   6.129384  neutral              0.81   \n",
       "43  45.044910   3.885870     fear              0.97   \n",
       "44  43.318831  -0.758236  sadness              0.81   \n",
       "45  48.250000   7.000000  neutral              0.00   \n",
       "46  45.729600   4.837800  neutral              0.83   \n",
       "47  45.060810   4.302941  sadness              0.95   \n",
       "48  43.604652   1.444209  neutral              0.67   \n",
       "49  40.712776 -74.005974  neutral              0.79   \n",
       "50  43.316700  -0.716700  neutral              0.70   \n",
       "51  45.750000   4.850000  neutral              0.71   \n",
       "52  38.895110 -77.036370  neutral              0.81   \n",
       "53  46.128700   3.421500      joy              0.93   \n",
       "54  45.188529   5.724524  neutral              0.84   \n",
       "55  52.355518  -1.174320  neutral              0.71   \n",
       "56  40.763416 -73.773056  neutral              0.81   \n",
       "\n",
       "                                              context  context_length  \\\n",
       "0                                                                   0   \n",
       "1   At the same time, the thing from the UGIF [Uni...             406   \n",
       "2   We were told that we'll have to get out in two...             508   \n",
       "3   His father [is] in [the French internment camp...             347   \n",
       "4   Mr. Trocmé came on his bicycle to meet me (sti...             600   \n",
       "5   I found an Austrian with whom I get along well...             303   \n",
       "6   Marseilles to Paris express (travel, change tr...             397   \n",
       "7   at 7:20 a.m. I left aboard the \\nMarseilles to...             387   \n",
       "8                                                                   0   \n",
       "9   I left aboard the Marseilles to Paris express ...             422   \n",
       "10  And I was told to \\nget off at a place called ...             525   \n",
       "11  We were told that we'll have to get out in two...             367   \n",
       "12  And in due time, I was given a \\nFrench identi...             372   \n",
       "13  It is located at about 3,000 feet elevation in...             244   \n",
       "14  I was sent to school, to a school that \\nthey ...             448   \n",
       "15  Tuesday, January 5, 1943 [Les Caillols] \\nMrs....             443   \n",
       "16                                                                  0   \n",
       "17  France. They didn't realize that they would be...             364   \n",
       "18  this idyll didn't last too long, because alrea...             291   \n",
       "19  And France was the haven. I mean, they thought...             345   \n",
       "20  They didn't realize that they would be picked ...             390   \n",
       "21  Now I have described the happenings from then,...             564   \n",
       "22  It was a little village up above Saint-Etienne...             342   \n",
       "23  Now I have described the happenings from then,...             517   \n",
       "24  And they ordered a truck. And they brought us ...             260   \n",
       "25  I am sad and weary. During rest hour Mrs. Cava...             349   \n",
       "26                                                                  0   \n",
       "27  which was the home for students from all \\ncou...             366   \n",
       "28  I go to school. Often think of you. I found an...             367   \n",
       "29  ..., which was the home for students from all ...             386   \n",
       "30  I was assigned to Maison des Roches. Amazingly...             542   \n",
       "31  I'm in Le Chambon-sur-Lignon. It's way up on a...             485   \n",
       "32  and my first flight to \\nSwitzerland over the ...             570   \n",
       "33  preparations were made to get us false identit...             463   \n",
       "34  ...emigration visas which we couldn't get on t...             340   \n",
       "35  We're lucky because there's a shortwave radio ...             376   \n",
       "36  And put us on a train. We were put into handcu...             570   \n",
       "37  But soon, unbelievable gesture of the French V...             427   \n",
       "38  Her husband is fighting the Germans with de Ga...             413   \n",
       "39                                                                  0   \n",
       "40  Jean-Pierre, Pastor Trocme's son, and Marco Da...             377   \n",
       "41  They say they're taking us to Le Puy, which is...             301   \n",
       "42  I'm not supposed to understand \\nGerman, I'm s...             306   \n",
       "43  They're starting the bus. They say they're tak...             341   \n",
       "44  I found an Austrian with whom I get along well...             348   \n",
       "45  and is a neutral country in the war, so Jews a...             990   \n",
       "46  before Mr. Philip, who was also a professor of...             519   \n",
       "47  And she took us by train to Toulouse. \\nWe sta...             565   \n",
       "48  So now we, in September or October, I don't re...             322   \n",
       "49  which was organized by the Swiss Red Cross chi...             285   \n",
       "50  \n",
       "effWell, this comes, the, in, in the camp, in...             551   \n",
       "51  He was minister of finance under de Gaulle, wh...             483   \n",
       "52  Then Mademoiselle \\nUsach and the other one, t...             205   \n",
       "53  So I was in,  we, so we got to this home from ...             568   \n",
       "54  And she took a, and then, it was a Monday she ...             303   \n",
       "55  He was minister of finance under de Gaulle, wh...             351   \n",
       "56  her is Lilli Brown. Her name is Feihan now. Sh...             313   \n",
       "\n",
       "   emotion_numeric  \n",
       "0                4  \n",
       "1                4  \n",
       "2                3  \n",
       "3                4  \n",
       "4                3  \n",
       "5                5  \n",
       "6                4  \n",
       "7                4  \n",
       "8                4  \n",
       "9                4  \n",
       "10               4  \n",
       "11               4  \n",
       "12               4  \n",
       "13               4  \n",
       "14               4  \n",
       "15               4  \n",
       "16               4  \n",
       "17               5  \n",
       "18               2  \n",
       "19               5  \n",
       "20               5  \n",
       "21               2  \n",
       "22               4  \n",
       "23               2  \n",
       "24               2  \n",
       "25               5  \n",
       "26               4  \n",
       "27               4  \n",
       "28               4  \n",
       "29               4  \n",
       "30               4  \n",
       "31               4  \n",
       "32               2  \n",
       "33               4  \n",
       "34               4  \n",
       "35               3  \n",
       "36               5  \n",
       "37               2  \n",
       "38               3  \n",
       "39               4  \n",
       "40               2  \n",
       "41               5  \n",
       "42               4  \n",
       "43               2  \n",
       "44               5  \n",
       "45               4  \n",
       "46               4  \n",
       "47               5  \n",
       "48               4  \n",
       "49               4  \n",
       "50               4  \n",
       "51               4  \n",
       "52               4  \n",
       "53               3  \n",
       "54               4  \n",
       "55               4  \n",
       "56               4  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take response output in json format, put into a dataframe, then assign numeric values \n",
    "# to the detected emotions.\n",
    "\n",
    "df = pd.DataFrame(analysis_results)\n",
    "\n",
    "conditions = [\n",
    "    df[\"emotion\"] == \"anger\",\n",
    "    df[\"emotion\"] == \"disgust\",\n",
    "    df[\"emotion\"] == \"fear\",\n",
    "    df[\"emotion\"] == \"joy\",\n",
    "    df[\"emotion\"] == \"neutral\",\n",
    "    df[\"emotion\"] == \"sadness\",\n",
    "    df[\"emotion\"] == \"surprise\"\n",
    "]\n",
    "values = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\"]\n",
    "df[\"emotion_numeric\"] = np.select(conditions, values, default=\"Unknown\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2222fa53-54b9-49a9-8fec-2d18892200cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results to csv\n",
    "\n",
    "df.to_csv(\"Results24J_ToponymsEmotions_smallSubCorpus.csv\", encoding=\"utf-8-sig\", index=False, header=True, mode=\"w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f13006-7409-41d6-b74e-4a80e4ff5414",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
