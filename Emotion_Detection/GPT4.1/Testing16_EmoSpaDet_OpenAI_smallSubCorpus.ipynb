{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "380e7cc7-cb38-4a47-8e37-7b8b355478e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for CUDA and GPU, and if True, GPU will be used.\n",
    "\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7f7f4a3-1fc2-4369-8a5f-332f586b582d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in the sample dataset, the smaller sub-corpus.\n",
    "\n",
    "import os\n",
    "path = \"YOUR_DATA_test\"\n",
    "\n",
    "def read_txt_files(directory):\n",
    "    # Reads all .txt files in a directory and returns a combined string of their contents.\n",
    "\n",
    "    file_contents = ''\n",
    "    \n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            with open(filepath, \"r\", encoding=\"utf8\") as f:\n",
    "                file_contents = file_contents + (f.read())\n",
    "    return file_contents\n",
    "\n",
    "texts = read_txt_files(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46fd835e-6d0d-4678-ba55-67a494f4174e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text split into 13 chunks for extraction (safe mode).\n",
      "Extracted 0 toponyms from chunk at char 22483.\n",
      "Extracted 11 toponyms from chunk at char 44090.\n",
      "Extracted 0 toponyms from chunk at char 14995.\n",
      "Extracted 0 toponyms from chunk at char 3374.\n",
      "Extracted 0 toponyms from chunk at char 33339.\n",
      "Extracted 10 toponyms from chunk at char 7201.\n",
      "Extracted 11 toponyms from chunk at char 40513.\n",
      "Extracted 18 toponyms from chunk at char 0.\n",
      "Extracted 6 toponyms from chunk at char 36849.\n",
      "Extracted 11 toponyms from chunk at char 29766.\n",
      "Extracted 15 toponyms from chunk at char 18799.\n",
      "Extracted 24 toponyms from chunk at char 26140.\n",
      "Extracted 20 toponyms from chunk at char 11214.\n",
      "\n",
      "Initial extraction stage complete: Got 126 toponym instances (with possible duplicates).\n",
      "Deduplicated (strict) toponym count: 126 → 117\n",
      "\n",
      "Stage 1 complete: Saved 117 unique toponym instances to file.\n",
      "Analyzed: Germany (from .toponym_instances)\n",
      "Analyzed: Switzerland (from .toponym_instances)\n",
      "Analyzed: Spain (from .toponym_instances)\n",
      "Analyzed: Camp de Gurs (from .toponym_instances)\n",
      "Analyzed: Le Chambon (from .toponym_instances)\n",
      "Analyzed: France (from .toponym_instances)\n",
      "Analyzed: Le Chambon (from .toponym_instances)\n",
      "Analyzed: Camp de Gurs (from .toponym_instances)\n",
      "Analyzed: Washington, DC (from .toponym_instances)\n",
      "Analyzed: Le Chambon (from .toponym_instances)\n",
      "Analyzed: Auch (from .toponym_instances)\n",
      "Analyzed: Romania (from .toponym_instances)\n",
      "Analyzed: France (from .toponym_instances)\n",
      "Analyzed: Gers (from .toponym_instances)\n",
      "Analyzed: Switzerland (from .toponym_instances)\n",
      "Analyzed: Hungary (from .toponym_instances)\n",
      "Analyzed: Le Chambon (from .toponym_instances)\n",
      "Analyzed: Alsace-Lorraine (from .toponym_instances)\n",
      "Analyzed: Le Chambon-sur-Lignon (from .toponym_instances)\n",
      "Analyzed: Camp de Gurs (from .toponym_instances)\n",
      "Analyzed: France (from .toponym_instances)\n",
      "Analyzed: Grenoble (from .toponym_instances)\n",
      "Analyzed: Valence (from .toponym_instances)\n",
      "Analyzed: Le Chambon (from .toponym_instances)\n",
      "Analyzed: La Guespy (from .toponym_instances)\n",
      "Analyzed: Le Chambon (from .toponym_instances)\n",
      "Analyzed: Les Grillons (from .toponym_instances)\n",
      "Analyzed: Paris (from .toponym_instances)\n",
      "Analyzed: La Rouvière (from .toponym_instances)\n",
      "Analyzed: Annecy (from .toponym_instances)\n",
      "Analyzed: Le Chambon sur Lignon (from .toponym_instances)\n",
      "Analyzed: Marseilles (from .toponym_instances)\n",
      "Analyzed: Le Chambon-sur-Lignon (from .toponym_instances)\n",
      "Analyzed: Le Chambon (from .toponym_instances)\n",
      "Analyzed: Lavoulte bridge (from .toponym_instances)\n",
      "Analyzed: France (from .toponym_instances)\n",
      "Analyzed: Cheylard (from .toponym_instances)\n",
      "Analyzed: Les Caillols (from .toponym_instances)\n",
      "Analyzed: Drancy (from .toponym_instances)\n",
      "Analyzed: Haute-Loire (from .toponym_instances)\n",
      "Analyzed: Les Caillols (from .toponym_instances)\n",
      "Analyzed: Marseilles (from .toponym_instances)\n",
      "Analyzed: Marseilles (from .toponym_instances)\n",
      "Analyzed: Massif Central (from .toponym_instances)\n",
      "Analyzed: St. Agrève (from .toponym_instances)\n",
      "Analyzed: Drancy (from .toponym_instances)\n",
      "Analyzed: Gurs (from .toponym_instances)\n",
      "Analyzed: Le Puy (from .toponym_instances)\n",
      "Analyzed: Le Chambon-sur Lignon (from .toponym_instances)\n",
      "Analyzed: Lyon (from .toponym_instances)\n",
      "Analyzed: England (from .toponym_instances)\n",
      "Analyzed: Les Grillons (from .toponym_instances)\n",
      "Analyzed: Gurs (from .toponym_instances)\n",
      "Analyzed: Vichy (from .toponym_instances)\n",
      "Analyzed: Algiers (from .toponym_instances)\n",
      "Analyzed: Toulouse (from .toponym_instances)\n",
      "Analyzed: Vichy (from .toponym_instances)\n",
      "Analyzed: Gurs (from .toponym_instances)\n",
      "Analyzed: Camp de Gurs (from .toponym_instances)\n",
      "Analyzed: Vichy (from .toponym_instances)\n",
      "Analyzed: La Guespy (from .toponym_instances)\n",
      "Analyzed: France (from .toponym_instances)\n",
      "Analyzed: Bayside, New York (from .toponym_instances)\n",
      "Analyzed: Chambon-sur-Lignon (from .toponym_instances)\n",
      "Analyzed: France (from .toponym_instances)\n",
      "Analyzed: France (from .toponym_instances)\n",
      "Analyzed: La Guespy (from .toponym_instances)\n",
      "Analyzed: Le Chambon-sur-Lignon (from .toponym_instances)\n",
      "Analyzed: France (from .toponym_instances)\n",
      "Analyzed: Le Chambon-sur-Lignon (from .toponym_instances)\n",
      "Analyzed: Le Chambon-sur-Lignon (from .toponym_instances)\n",
      "Analyzed: LePuy-en-Velay (from .toponym_instances)\n",
      "Analyzed: Le Chambon (from .toponym_instances)\n",
      "Analyzed: Chambon-sur-Lignon (from .toponym_instances)\n",
      "Analyzed: Switzerland (from .toponym_instances)\n",
      "Analyzed: Algiers (from .toponym_instances)\n",
      "Analyzed: Alsace-Lorraine (from .toponym_instances)\n",
      "Analyzed: Le Chambon (from .toponym_instances)\n",
      "Analyzed: France (from .toponym_instances)\n",
      "Analyzed: Auschwitz (from .toponym_instances)\n",
      "Analyzed: Le Chambon-sur-Lignon (from .toponym_instances)\n",
      "Analyzed: Annecy (from .toponym_instances)\n",
      "Analyzed: France (from .toponym_instances)\n",
      "Analyzed: Switzerland (from .toponym_instances)\n",
      "Analyzed: Annecy (from .toponym_instances)\n",
      "Analyzed: France (from .toponym_instances)\n",
      "Analyzed: Switzerland (from .toponym_instances)\n",
      "Analyzed: Le Chambon (from .toponym_instances)\n",
      "Analyzed: Switzerland (from .toponym_instances)\n",
      "Analyzed: Algiers (from .toponym_instances)\n",
      "Analyzed: Le Chambon (from .toponym_instances)\n",
      "Analyzed: Washington (from .toponym_instances)\n",
      "Analyzed: France (from .toponym_instances)\n",
      "Analyzed: Valence (from .toponym_instances)\n",
      "Analyzed: Camp de Gurs (from .toponym_instances)\n",
      "Analyzed: Le Chambon (from .toponym_instances)\n",
      "Analyzed: France (from .toponym_instances)\n",
      "Analyzed: Chambon-sur-Lignon (from .toponym_instances)\n",
      "Analyzed: Chambon (from .toponym_instances)\n",
      "Analyzed: Spain (from .toponym_instances)\n",
      "Analyzed: Europe (from .toponym_instances)\n",
      "Analyzed: Belgium (from .toponym_instances)\n",
      "Analyzed: France (from .toponym_instances)\n",
      "Analyzed: Austria (from .toponym_instances)\n",
      "Analyzed: Le Chambon (from .toponym_instances)\n",
      "Analyzed: Maison des Roches (from .toponym_instances)\n",
      "Analyzed: Haute-Loire (from .toponym_instances)\n",
      "Analyzed: Poland (from .toponym_instances)\n",
      "Analyzed: Russia (from .toponym_instances)\n",
      "Analyzed: Chambon (from .toponym_instances)\n",
      "Analyzed: Switzerland (from .toponym_instances)\n",
      "Analyzed: Chambon (from .toponym_instances)\n",
      "Analyzed: Chambon-sur-Lignon (from .toponym_instances)\n",
      "Analyzed: Drancy (from .toponym_instances)\n",
      "Analyzed: Le Chambon (from .toponym_instances)\n",
      "Analyzed: Maison des Roches (from .toponym_instances)\n",
      "Analyzed: Saint-Etienne (from .toponym_instances)\n",
      "\n",
      "Stage 2 complete: Produced 117 detailed toponym analyses.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Developing function to identify and resolve toponyms, and detect emotions in context \n",
    "on either side of each toponym.  Context length is based on trying different lengths,\n",
    "with the final context length chosen based on which gives the most likely detected emotion\n",
    "with the highest confidence score.\n",
    "\n",
    "\"\"\"\n",
    "# Access libraries\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import tiktoken\n",
    "import difflib # <-- for similarity\n",
    "\n",
    "\n",
    "# Set a global variable for my OpenAI API key so that the model can be accessed.\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"\n",
    "client = OpenAI()\n",
    "\n",
    "# Alternative data for testing:\n",
    "#texts = \"I traveled from Paris to Berlin and saw New York on the way.  It was fantastic.  I was so happy.\"\n",
    "\n",
    "# ========== Robust OpenAI Output Extraction ==========\n",
    "def extract_json_from_arguments(response):\n",
    "    \"\"\"\n",
    "    Robust extraction for OpenAI responses.\n",
    "    Handles both function call and text output scenarios.\n",
    "    Returns dict or list or [].\n",
    "    \"\"\"\n",
    "    # Case 1: Function call pattern\n",
    "    if hasattr(response, \"output\") and response.output:\n",
    "        first = response.output[0]\n",
    "        if hasattr(first, \"arguments\"): # should be a string\n",
    "            arguments_string = first.arguments\n",
    "            if isinstance(arguments_string, (str, bytes)):\n",
    "                try:\n",
    "                    return json.loads(arguments_string)\n",
    "                except Exception as e:\n",
    "                    print(f\"JSON parsing error: {e}\\nARGUMENTS STRING: {arguments_string}\")\n",
    "                    return []\n",
    "            else:\n",
    "                # If already parsed (rare)\n",
    "                return arguments_string\n",
    "        # If it's classic text response\n",
    "        if hasattr(first, \"content\") and first.content:\n",
    "            text_fragment = getattr(first.content[0], \"text\", None)\n",
    "            if text_fragment:\n",
    "                try:\n",
    "                    return json.loads(text_fragment)\n",
    "                except Exception as e:\n",
    "                    print(f\"JSON parsing error (text): {e}\\nTEXT: {text_fragment}\")\n",
    "                    return []\n",
    "    # Case 2: Tool-style .outputs (not present in your current responses)\n",
    "    if hasattr(response, \"outputs\") and response.outputs and hasattr(response.outputs[0], \"arguments\"):\n",
    "        arguments = response.outputs[0].arguments\n",
    "        if arguments is not None:\n",
    "            return arguments\n",
    "    print(\"No recognizable output format found in OpenAI response.\")\n",
    "    return []\n",
    "\n",
    "# 2. Token-based Chunking (if needed).\n",
    "# For iteration 13 and following, reduced tokens to 1000 and overlap to 50 to try to improve performance and reduce duplicates.\n",
    "\n",
    "def chunk_text_by_tokens_safe(text, max_tokens=1000, overlap_tokens=50):\n",
    "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    all_tokens = enc.encode(text)\n",
    "\n",
    "    # Build a mapping from token indices to character offsets\n",
    "    char_offsets = []\n",
    "    curr_char = 0\n",
    "    for tok in all_tokens:\n",
    "        piece = enc.decode([tok])\n",
    "        char_offsets.append(curr_char)\n",
    "        curr_char += len(piece)\n",
    "\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(all_tokens):\n",
    "        start_tok = i\n",
    "        end_tok = min(i + max_tokens, len(all_tokens))\n",
    "        start_char = char_offsets[start_tok]\n",
    "        if end_tok > 0:\n",
    "            end_char = char_offsets[end_tok-1] + len(enc.decode([all_tokens[end_tok-1]]))\n",
    "        else:\n",
    "            end_char = 0\n",
    "        chunk_text = text[start_char:end_char]\n",
    "        chunks.append((chunk_text, start_char))\n",
    "        if end_tok == len(all_tokens):\n",
    "            break\n",
    "        i += max_tokens - overlap_tokens\n",
    "    return chunks\n",
    "\n",
    "# 3. API Call with Retry for Thread Use\n",
    "\n",
    "def call_api_with_retry_chunk(chunk, offset, extraction_instructions, client, max_output_tokens=2048, retries=4):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = client.responses.create(\n",
    "                model=\"gpt-4.1-2025-04-14\",\n",
    "                instructions=extraction_instructions,\n",
    "                input=chunk,\n",
    "                text={\"format\": {\"type\": \"text\"}},\n",
    "                reasoning={},\n",
    "                tools=[\n",
    "                    {\n",
    "                        \"type\": \"function\",\n",
    "                        \"name\": \"recognize_toponyms\",\n",
    "                        \"description\": \"Given the user input text, identify all the toponyms in the text.\",\n",
    "                        \"parameters\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"required\": [\"input_text\", \"toponyms\"],\n",
    "                            \"properties\": {\n",
    "                                \"input_text\": {\"type\": \"string\", \"description\": \"The text string from which to recognize and identify toponyms.\"},\n",
    "                                \"toponyms\": {\n",
    "                                    \"type\": \"array\",\n",
    "                \t\t\t\t    \"description\": \"Array of recognized and identified toponyms.\",\n",
    "                                    \"items\": {\n",
    "                                        \"type\": \"object\",\n",
    "                                        \"properties\": {\n",
    "                                            \"toponym\": {\"type\": \"string\"},\n",
    "                                            \"start_idx\": {\"type\": \"integer\"},\n",
    "                                            \"end_idx\": {\"type\": \"integer\"},\n",
    "                                        },\n",
    "                                        \"required\": [\"toponym\", \"start_idx\", \"end_idx\"],\n",
    "                                        \"additionalProperties\": False\n",
    "                                    }\n",
    "                                }\n",
    "                            },\n",
    "                            \"additionalProperties\": False\n",
    "                        },\n",
    "                        \"strict\": True\n",
    "                    }\n",
    "                ],\n",
    "                temperature=1.0,\n",
    "                tool_choice=\"required\",\n",
    "                max_output_tokens=max_output_tokens,\n",
    "                top_p=1,\n",
    "                store=True\n",
    "            )\n",
    "            return extract_json_from_arguments(response), offset\n",
    "        except Exception as e:\n",
    "            wait = 2 ** attempt\n",
    "            print(f\"[API] Error: {e}\\nRetrying in {wait}s (chunk at char {offset})...\")\n",
    "            time.sleep(wait)\n",
    "    print(f\"[API] Failed after retries for chunk at {offset}\")\n",
    "    return [], offset\n",
    "\n",
    "# 4. Stage 1: Parallel Toponym Extraction\n",
    "\n",
    "# ====== Load Extraction Prompt ======\n",
    "with open(\"openai_ToponymExtraction_prompt_complicated_16.txt\", encoding=\"utf-8\") as f:\n",
    "    extraction_instructions = f.read()\n",
    "\n",
    "# ====== Chunk Input ======\n",
    "    # now using token-to-char mapping (SAFE)\n",
    "# As noted above, for this iteration reduced chunk size to 1000 and overlap to 50\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "input_tokens = len(enc.encode(texts))\n",
    "if input_tokens < 1000:\n",
    "    chunks = [(texts, 0)]\n",
    "    print(\"Text fits in one chunk for extraction.\")\n",
    "else:\n",
    "    chunks = chunk_text_by_tokens_safe(texts, max_tokens=1000, overlap_tokens=50)\n",
    "    print(f\"Text split into {len(chunks)} chunks for extraction (safe mode).\")\n",
    "\n",
    "# ====== Run Extraction in Parallel ======\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "max_workers = 20   # safe for modern high-tier; can adjust up/down\n",
    "extracted_toponyms = []\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    futures = [\n",
    "        executor.submit(\n",
    "            call_api_with_retry_chunk, chunk, offset, extraction_instructions, client, 2048\n",
    "        )\n",
    "        for chunk, offset in chunks\n",
    "    ]\n",
    "    for f in as_completed(futures):\n",
    "        toponyms_chunk, offset = f.result()\n",
    "        num_found = 0\n",
    "        if isinstance(toponyms_chunk, list):\n",
    "            for t in toponyms_chunk:\n",
    "                if 'start_idx' in t: t[\"start_idx\"] += offset\n",
    "                if 'end_idx' in t: t[\"end_idx\"] += offset\n",
    "            extracted_toponyms += toponyms_chunk\n",
    "            num_found = len(toponyms_chunk)\n",
    "        elif isinstance(toponyms_chunk, dict) and \"toponyms\" in toponyms_chunk:\n",
    "            for t in toponyms_chunk[\"toponyms\"]:\n",
    "                if 'start_idx' in t: t[\"start_idx\"] += offset\n",
    "                if 'end_idx' in t: t[\"end_idx\"] += offset\n",
    "            extracted_toponyms += toponyms_chunk[\"toponyms\"]\n",
    "            num_found = len(toponyms_chunk[\"toponyms\"])\n",
    "        print(f\"Extracted {num_found} toponyms from chunk at char {offset}.\")\n",
    "\n",
    "print(f\"\\nInitial extraction stage complete: Got {len(extracted_toponyms)} toponym instances (with possible duplicates).\")\n",
    "\n",
    "# ------------------ DEDUPLICATION STEP --------------------\n",
    "    # nearby, identical name (window adjustable)\n",
    "def deduplicate_toponyms_strict(toponym_list, idx_threshold=70):\n",
    "    toponym_clusters = []\n",
    "    for t in toponym_list:\n",
    "        found = False\n",
    "        norm_name = t[\"toponym\"].strip().lower().replace('-', ' ').replace('_',' ')\n",
    "        for c in toponym_clusters:\n",
    "            c_name = c[\"toponym\"].strip().lower().replace('-', ' ').replace('_',' ')\n",
    "            if norm_name == c_name and abs(t[\"start_idx\"] - c[\"start_idx\"]) < idx_threshold:\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            toponym_clusters.append(t)\n",
    "    return toponym_clusters\n",
    "\n",
    "before = len(extracted_toponyms)\n",
    "extracted_toponyms = deduplicate_toponyms_strict(extracted_toponyms, idx_threshold=70)\n",
    "after = len(extracted_toponyms)\n",
    "print(f\"Deduplicated (strict) toponym count: {before} → {after}\")\n",
    "\n",
    "# ------------------ END DEDUPLICATION STEP --------------------\n",
    "\n",
    "with open(\"extracted_toponyms.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(extracted_toponyms, f, ensure_ascii=False, indent=2)\n",
    "print(f\"\\nStage 1 complete: Saved {after} unique toponym instances to file.\")\n",
    "\n",
    "# 5. Stage 2: Parallel Toponym Analysis\n",
    "\n",
    "# ====== Load Analysis Prompt ======\n",
    "with open(\"openai_ToponymEmotionAnalysis_prompt_complicated_16.txt\", encoding=\"utf-8\") as f:\n",
    "    analysis_instructions = f.read()\n",
    "\n",
    "def call_api_with_retry_analysis(\n",
    "    toponym_obj,\n",
    "    texts,\n",
    "    client,\n",
    "    analysis_instructions,\n",
    "    max_output_tokens=2048,\n",
    "    retries=4,\n",
    "):\n",
    "    toponym_str = toponym_obj[\"toponym\"]\n",
    "    idx = texts.lower().find(toponym_str.lower())\n",
    "    if idx == -1:\n",
    "        start_idx, end_idx = 0, 0\n",
    "        context = \"\"  # fallback, though function is expected to handle context dynamically\n",
    "    else:\n",
    "        # Start with a large context, function will try multiple sizes\n",
    "        window = 600  # You may increase this further\n",
    "        start_idx = max(0, idx - window)\n",
    "        end_idx = min(len(texts), idx + len(toponym_str) + window)\n",
    "        context = texts[start_idx:end_idx]\n",
    "    user_input = {\n",
    "        \"original_text\": context if context else texts,\n",
    "        \"toponym_instances\": [{**toponym_obj, \"original_range\": [start_idx, end_idx]}]\n",
    "    }\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = client.responses.create(\n",
    "                model=\"gpt-4.1-2025-04-14\",\n",
    "                instructions=analysis_instructions,\n",
    "                input=json.dumps(user_input),\n",
    "                text={\"format\": {\"type\": \"text\"}},\n",
    "                reasoning={},\n",
    "                tools=[{\n",
    "                    \"type\": \"function\",\n",
    "                    \"name\": \"resolve_toponyms_and_detect_emotions\",\n",
    "                    \"description\": (\n",
    "                        \"Given the user input of the original text and extracted toponyms, \"\n",
    "                        \"determine latitude and longitude of each toponym and perform emotion detection. \"\n",
    "                        \"Try multiple possible context window sizes (~different context lengths) for each toponym and \"\n",
    "                        \"return the window (context) that maximizes the confidence score for the most likely detected emotion.\"\n",
    "                    ),\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"required\": [\"original_text\", \"toponym_instances\"],\n",
    "                        \"properties\": {\n",
    "                            \"original_text\": {\"type\": \"string\", \"description\": \"The text string from which to disambiguate toponyms and utilize their surrounding context.\"},\n",
    "                            \"toponym_instances\": {\n",
    "                                \"type\": \"array\",\n",
    "                \t\t\t\t\"description\": \"Array of identified toponyms, each containing properties of location details and emotional context.\",\n",
    "                                \"items\": {\n",
    "                                    \"type\": \"object\",\n",
    "                                    \"required\": [\n",
    "                                        \"toponym\", \"resolved_name\", \"latitude\",\n",
    "                                        \"longitude\", \"emotion\", \"confidence_score\",\n",
    "                                        \"context\", \"context_length\", \"original_range\"\n",
    "                                    ],\n",
    "                                    \"properties\": {\n",
    "                                        \"toponym\": {\"type\": \"string\", \"description\": \"The name of the toponym as found in the previous step.\"},\n",
    "                                        \"resolved_name\": {\"type\": \"string\", \"description\": \"The name of the resolved toponym as identified and disambiguated.\"},\n",
    "                                        \"latitude\": {\"type\": \"number\", \"description\": \"The latitude coordinate of the toponym.\"},\n",
    "                                        \"longitude\": {\"type\": \"number\", \"description\": \"The longitude coordinate of the toponym.\"},\n",
    "                                        \"emotion\": {\"type\": \"string\", \"description\": \"The most likely detected emotion around the toponym.\", \"enum\": [\n",
    "                                            \"anger\", \"disgust\", \"fear\", \"joy\", \"sadness\", \"surprise\", \"neutral\"\n",
    "                                        ]},\n",
    "                                        \"confidence_score\": {\"type\": \"number\", \"description\": \"The confidence score for the detected emotion, on a scale of 0 to 1.\"},\n",
    "                                        \"context\": {\"type\": \"string\", \"description\": \"The text block surrounding the toponym used for emotion detection, whose length is determined based on trying different lengths and seeing which one gives the highest confidence score for the most likely detected emotion.\"},\n",
    "                                        \"context_length\": {\"type\": \"number\", \"description\": \"The length, in characters including spaces, of the final text block surrounding the toponym used for emotion detection.\"},\n",
    "                                        \"original_range\": {\"type\": \"array\", \"description\": \"The original start and end position in the text of the toponym.\", \"items\": {\"type\": \"number\"}}\n",
    "                                    },\n",
    "                                    \"additionalProperties\": False,\n",
    "                                },\n",
    "                            }\n",
    "                        },\n",
    "                        \"additionalProperties\": False,\n",
    "                    },\n",
    "                    \"strict\": True\n",
    "                }],\n",
    "                temperature=1,\n",
    "                tool_choice=\"required\",\n",
    "                max_output_tokens=max_output_tokens,\n",
    "                top_p=1,\n",
    "                store=True\n",
    "            )\n",
    "            return extract_json_from_arguments(response), toponym_str\n",
    "        except Exception as e:\n",
    "            wait = 2 ** attempt\n",
    "            print(f\"[API] Analysis error for '{toponym_str}': {e}\\nRetrying in {wait}s...\")\n",
    "            time.sleep(wait)\n",
    "    print(f\"[API] Analysis failed after retries for '{toponym_str}'.\")\n",
    "    return {\"toponym\": toponym_str, \"error\": \"Failed after retries\"}, toponym_str\n",
    "\n",
    "# Run Stage 2 in Parallel\n",
    "\n",
    "# ---- Load the extracted_toponyms ----\n",
    "with open(\"extracted_toponyms.json\", encoding=\"utf-8\") as f:\n",
    "    extracted_toponyms = json.load(f)\n",
    "\n",
    "analysis_results = []\n",
    "max_workers_analysis = 20  # You can go higher if needed\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=max_workers_analysis) as executor:\n",
    "    futures = [\n",
    "        executor.submit(\n",
    "            call_api_with_retry_analysis, t, texts, client, analysis_instructions, 2048\n",
    "        )\n",
    "        for t in extracted_toponyms\n",
    "    ]\n",
    "    for f in as_completed(futures):\n",
    "        batch_result, toponym_str = f.result()\n",
    "        # Handle lists/dicts as before\n",
    "        if isinstance(batch_result, list):\n",
    "            analysis_results += batch_result\n",
    "            print(f\"Analyzed: {toponym_str} (got list)\")\n",
    "        elif isinstance(batch_result, dict) and \"toponym_instances\" in batch_result:\n",
    "            analysis_results += batch_result[\"toponym_instances\"]\n",
    "            print(f\"Analyzed: {toponym_str} (from .toponym_instances)\")\n",
    "        else:\n",
    "            analysis_results.append(batch_result)\n",
    "            print(f\"Analyzed: {toponym_str} (error or unexpected shape)\")\n",
    "\n",
    "with open(\"analysis_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(analysis_results, f, ensure_ascii=False, indent=2)\n",
    "print(f\"\\nStage 2 complete: Produced {len(analysis_results)} detailed toponym analyses.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53d90d8a-b1f8-4507-84aa-0389fe053388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toponym</th>\n",
       "      <th>resolved_name</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>emotion</th>\n",
       "      <th>confidence_score</th>\n",
       "      <th>context</th>\n",
       "      <th>context_length</th>\n",
       "      <th>original_range</th>\n",
       "      <th>emotion_numeric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Germany</td>\n",
       "      <td>Germany</td>\n",
       "      <td>51.165691</td>\n",
       "      <td>10.451526</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.89</td>\n",
       "      <td>France. They didn't realize that they would be...</td>\n",
       "      <td>328</td>\n",
       "      <td>[206, 213]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Switzerland</td>\n",
       "      <td>Switzerland</td>\n",
       "      <td>46.798562</td>\n",
       "      <td>8.231974</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.94</td>\n",
       "      <td>Now I have described the happenings from then,...</td>\n",
       "      <td>384</td>\n",
       "      <td>[45196, 45207]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spain</td>\n",
       "      <td>Spain</td>\n",
       "      <td>40.463667</td>\n",
       "      <td>-3.749220</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.88</td>\n",
       "      <td>which was the home for students from all \\ncou...</td>\n",
       "      <td>382</td>\n",
       "      <td>[180, 185]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Camp de Gurs</td>\n",
       "      <td>Camp de Gurs</td>\n",
       "      <td>43.340278</td>\n",
       "      <td>-0.724444</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.68</td>\n",
       "      <td>Well, this comes, the, in, in the camp, in Cam...</td>\n",
       "      <td>502</td>\n",
       "      <td>[44505, 44517]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Le Chambon</td>\n",
       "      <td>Le Chambon-sur-Lignon</td>\n",
       "      <td>45.060810</td>\n",
       "      <td>4.302941</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.00</td>\n",
       "      <td>od there is fit for \\nkings (butter, potatoes,...</td>\n",
       "      <td>1098</td>\n",
       "      <td>[1001, 2211]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>Chambon-sur-Lignon</td>\n",
       "      <td>Le Chambon-sur-Lignon</td>\n",
       "      <td>45.060810</td>\n",
       "      <td>4.302941</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.93</td>\n",
       "      <td>December 31, I was notified that I would be sh...</td>\n",
       "      <td>414</td>\n",
       "      <td>[13056, 13075]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>Drancy</td>\n",
       "      <td>Drancy, France</td>\n",
       "      <td>48.925800</td>\n",
       "      <td>2.445600</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.94</td>\n",
       "      <td>This idyll didn't last too long, because alrea...</td>\n",
       "      <td>435</td>\n",
       "      <td>[13800, 13806]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>Le Chambon</td>\n",
       "      <td>Le Chambon-sur-Lignon</td>\n",
       "      <td>45.060810</td>\n",
       "      <td>4.302941</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.72</td>\n",
       "      <td>Saturday, January 16, 1943 [ Le Chambon sur Li...</td>\n",
       "      <td>596</td>\n",
       "      <td>[1001, 1021]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>Maison des Roches</td>\n",
       "      <td>Maison des Roches, Le Chambon-sur-Lignon</td>\n",
       "      <td>45.058400</td>\n",
       "      <td>4.295900</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.79</td>\n",
       "      <td>I received, through, again, Tracy Strong, infl...</td>\n",
       "      <td>709</td>\n",
       "      <td>[13314, 13331]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>Saint-Etienne</td>\n",
       "      <td>Saint-Étienne</td>\n",
       "      <td>45.439700</td>\n",
       "      <td>4.387200</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.83</td>\n",
       "      <td>It was a little village up above Saint-Etienne...</td>\n",
       "      <td>434</td>\n",
       "      <td>[11886, 11900]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>117 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                toponym                             resolved_name   latitude  \\\n",
       "0               Germany                                   Germany  51.165691   \n",
       "1           Switzerland                               Switzerland  46.798562   \n",
       "2                 Spain                                     Spain  40.463667   \n",
       "3          Camp de Gurs                              Camp de Gurs  43.340278   \n",
       "4            Le Chambon                     Le Chambon-sur-Lignon  45.060810   \n",
       "..                  ...                                       ...        ...   \n",
       "112  Chambon-sur-Lignon                     Le Chambon-sur-Lignon  45.060810   \n",
       "113              Drancy                            Drancy, France  48.925800   \n",
       "114          Le Chambon                     Le Chambon-sur-Lignon  45.060810   \n",
       "115   Maison des Roches  Maison des Roches, Le Chambon-sur-Lignon  45.058400   \n",
       "116       Saint-Etienne                             Saint-Étienne  45.439700   \n",
       "\n",
       "     longitude  emotion  confidence_score  \\\n",
       "0    10.451526     fear              0.89   \n",
       "1     8.231974     fear              0.94   \n",
       "2    -3.749220  neutral              0.88   \n",
       "3    -0.724444  neutral              0.68   \n",
       "4     4.302941  neutral              0.00   \n",
       "..         ...      ...               ...   \n",
       "112   4.302941     fear              0.93   \n",
       "113   2.445600     fear              0.94   \n",
       "114   4.302941  neutral              0.72   \n",
       "115   4.295900  neutral              0.79   \n",
       "116   4.387200  neutral              0.83   \n",
       "\n",
       "                                               context  context_length  \\\n",
       "0    France. They didn't realize that they would be...             328   \n",
       "1    Now I have described the happenings from then,...             384   \n",
       "2    which was the home for students from all \\ncou...             382   \n",
       "3    Well, this comes, the, in, in the camp, in Cam...             502   \n",
       "4    od there is fit for \\nkings (butter, potatoes,...            1098   \n",
       "..                                                 ...             ...   \n",
       "112  December 31, I was notified that I would be sh...             414   \n",
       "113  This idyll didn't last too long, because alrea...             435   \n",
       "114  Saturday, January 16, 1943 [ Le Chambon sur Li...             596   \n",
       "115  I received, through, again, Tracy Strong, infl...             709   \n",
       "116  It was a little village up above Saint-Etienne...             434   \n",
       "\n",
       "     original_range emotion_numeric  \n",
       "0        [206, 213]               2  \n",
       "1    [45196, 45207]               2  \n",
       "2        [180, 185]               4  \n",
       "3    [44505, 44517]               4  \n",
       "4      [1001, 2211]               4  \n",
       "..              ...             ...  \n",
       "112  [13056, 13075]               2  \n",
       "113  [13800, 13806]               2  \n",
       "114    [1001, 1021]               4  \n",
       "115  [13314, 13331]               4  \n",
       "116  [11886, 11900]               4  \n",
       "\n",
       "[117 rows x 10 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take response output in json format, put into a dataframe, then assign numeric values \n",
    "# to the detected emotions.\n",
    "\n",
    "df = pd.DataFrame(analysis_results)\n",
    "\n",
    "conditions = [\n",
    "    df[\"emotion\"] == \"anger\",\n",
    "    df[\"emotion\"] == \"disgust\",\n",
    "    df[\"emotion\"] == \"fear\",\n",
    "    df[\"emotion\"] == \"joy\",\n",
    "    df[\"emotion\"] == \"neutral\",\n",
    "    df[\"emotion\"] == \"sadness\",\n",
    "    df[\"emotion\"] == \"surprise\"\n",
    "]\n",
    "values = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\"]\n",
    "df[\"emotion_numeric\"] = np.select(conditions, values, default=\"Unknown\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2222fa53-54b9-49a9-8fec-2d18892200cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results to csv\n",
    "\n",
    "df.to_csv(\"Results16_ToponymsEmotions_smallSubCorpus.csv\", encoding=\"utf-8-sig\", index=False, header=True, mode=\"w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f13006-7409-41d6-b74e-4a80e4ff5414",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
