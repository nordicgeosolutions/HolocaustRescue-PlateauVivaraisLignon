{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "380e7cc7-cb38-4a47-8e37-7b8b355478e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for CUDA and GPU, and if True, GPU will be used.\n",
    "\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7f7f4a3-1fc2-4369-8a5f-332f586b582d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in the sample dataset, the smaller sub-corpus.\n",
    "\n",
    "import os\n",
    "path = \"YOUR_DATA_test\"\n",
    "\n",
    "def read_txt_files(directory):\n",
    "    # Reads all .txt files in a directory and returns a combined string of their contents.\n",
    "\n",
    "    file_contents = ''\n",
    "    \n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            with open(filepath, \"r\", encoding=\"utf8\") as f:\n",
    "                file_contents = file_contents + (f.read())\n",
    "    return file_contents\n",
    "\n",
    "texts = read_txt_files(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46fd835e-6d0d-4678-ba55-67a494f4174e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text split into 77 char-based chunks for extraction.\n",
      "\n",
      "Initial extraction stage complete: Got 165 toponym instances (with possible duplicates).\n",
      "Deduplicated toponyms: 165 → 58\n",
      "\n",
      "Stage 1 complete: Saved 58 unique toponym instances to file.\n",
      "Analyzed: Gers (from .toponym_instances)\n",
      "Analyzed: Auch (from .toponym_instances)\n",
      "Analyzed: Le Chambon-sur-Lignon (from .toponym_instances)\n",
      "Analyzed: Eastern Europe (from .toponym_instances)\n",
      "Analyzed: France (from .toponym_instances)\n",
      "Analyzed: Germany (from .toponym_instances)\n",
      "Analyzed: Marseilles (from .toponym_instances)\n",
      "Analyzed: Les Grillons (from .toponym_instances)\n",
      "Analyzed: Le Chambon sur Lignon (from .toponym_instances)\n",
      "Analyzed: Romania (from .toponym_instances)\n",
      "Analyzed: Haute-Loire (from .toponym_instances)\n",
      "Analyzed: Hungary (from .toponym_instances)\n",
      "Analyzed: Cheylard (from .toponym_instances)\n",
      "Analyzed: Massif Central (from .toponym_instances)\n",
      "Analyzed: La Rouvière (from .toponym_instances)\n",
      "Analyzed: St. Agrève (from .toponym_instances)\n",
      "Analyzed: Paris (from .toponym_instances)\n",
      "Analyzed: College Cevenol (from .toponym_instances)\n",
      "Analyzed: Lavoulte bridge (from .toponym_instances)\n",
      "Analyzed: Saint-Etienne (from .toponym_instances)\n",
      "Analyzed: Drancy (from .toponym_instances)\n",
      "Analyzed: Switzerland (from .toponym_instances)\n",
      "Analyzed: Alps (from .toponym_instances)\n",
      "Analyzed: Vallorcine (from .toponym_instances)\n",
      "Analyzed: Spain (from .toponym_instances)\n",
      "Analyzed: Austria (from .toponym_instances)\n",
      "Analyzed: Poland (from .toponym_instances)\n",
      "Analyzed: Maison des Roches (from .toponym_instances)\n",
      "Analyzed: Gurs (from .toponym_instances)\n",
      "Analyzed: Perpignan (from .toponym_instances)\n",
      "Analyzed: Swiss (from .toponym_instances)\n",
      "Analyzed: Rivesaltes (from .toponym_instances)\n",
      "Analyzed: Les Caillols (from .toponym_instances)\n",
      "Analyzed: La Guespy (from .toponym_instances)\n",
      "Analyzed: Swiss border (from .toponym_instances)\n",
      "Analyzed: Britain (from .toponym_instances)\n",
      "Analyzed: Vichy (from .toponym_instances)\n",
      "Analyzed: Algiers (from .toponym_instances)\n",
      "Analyzed: Camp de Gurs (from .toponym_instances)\n",
      "Analyzed: French (from .toponym_instances)\n",
      "Analyzed: Le Puy (from .toponym_instances)\n",
      "Analyzed: German (from .toponym_instances)\n",
      "Analyzed: Annecy (from .toponym_instances)\n",
      "Analyzed: University of Lyon (from .toponym_instances)\n",
      "Analyzed: Valence (from .toponym_instances)\n",
      "Analyzed: Le Chambon-sur Lignon (from .toponym_instances)\n",
      "Analyzed: Toulouse (from .toponym_instances)\n",
      "Analyzed: England (from .toponym_instances)\n",
      "Analyzed: LePuy-en-Velay (from .toponym_instances)\n",
      "Analyzed: New York (from .toponym_instances)\n",
      "Analyzed: Lyon (from .toponym_instances)\n",
      "Analyzed: Laval (from .toponym_instances)\n",
      "Analyzed: Auschwitz (from .toponym_instances)\n",
      "Analyzed: Washington, DC (from .toponym_instances)\n",
      "Analyzed: Bayside, New York (from .toponym_instances)\n",
      "Analyzed: Alsace-Lorraine (from .toponym_instances)\n",
      "Analyzed: Grenoble (from .toponym_instances)\n",
      "Analyzed: German border (from .toponym_instances)\n",
      "\n",
      "Stage 2 complete: Produced 58 detailed toponym analyses.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Developing function to identify and resolve toponyms, and detect emotions in context \n",
    "on either side of each toponym.  Context length is based on trying different lengths,\n",
    "with the final context length chosen based on which gives the most likely detected emotion\n",
    "with the highest confidence score.\n",
    "\n",
    "\"\"\"\n",
    "# Access libraries\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "import hashlib\n",
    "from difflib import SequenceMatcher\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Set a global variable for my OpenAI API key so that the model can be accessed.\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"\n",
    "client = OpenAI()\n",
    "\n",
    "# Alternative data for testing and to DEBUG:\n",
    "#texts = \"I traveled from Paris to Berlin and saw New York on the way.  It was fantastic.  I was so happy.\"\n",
    "\n",
    "# ========== Robust OpenAI Output Extraction ==========\n",
    "def extract_json_from_arguments(response):\n",
    "    \"\"\"\n",
    "    Robust extraction for OpenAI responses.\n",
    "    Handles both function call and text output scenarios.\n",
    "    Returns dict or list or [].\n",
    "    \"\"\"\n",
    "    # Case 1: Function call pattern\n",
    "    if hasattr(response, \"output\") and response.output:\n",
    "        first = response.output[0]\n",
    "        if hasattr(first, \"arguments\"): # should be a string\n",
    "            arguments_string = first.arguments\n",
    "            if isinstance(arguments_string, (str, bytes)):\n",
    "                try:\n",
    "                    return json.loads(arguments_string)\n",
    "                except Exception as e:\n",
    "                    print(f\"JSON parsing error: {e}\")\n",
    "                    return []\n",
    "            else:\n",
    "                # If already parsed (rare)\n",
    "                return arguments_string\n",
    "        # If it's classic text response\n",
    "        if hasattr(first, \"content\") and first.content:\n",
    "            text_fragment = getattr(first.content[0], \"text\", None)\n",
    "            if text_fragment:\n",
    "                try:\n",
    "                    return json.loads(text_fragment)\n",
    "                except Exception as e:\n",
    "                    print(f\"JSON parsing error (text): {e}\\nTEXT: {text_fragment}\")\n",
    "                    return []\n",
    "    # Case 2: Tool-style .outputs (not present in your current responses)\n",
    "    if hasattr(response, \"outputs\") and response.outputs and hasattr(response.outputs[0], \"arguments\"):\n",
    "        arguments = response.outputs[0].arguments\n",
    "        if arguments is not None:\n",
    "            return arguments\n",
    "    print(\"No recognizable output format found in OpenAI response.\")\n",
    "    return []\n",
    "\n",
    "# 2. Character-based Chunking (if needed).\n",
    "# Reduced number of characters to just 600, with no overlap at all.  \n",
    "\n",
    "def chunk_text_by_chars(text, chunk_size=600, overlap=0):\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    text_len = len(text)\n",
    "    while i < text_len:\n",
    "        start_char = i\n",
    "        end_char = min(i + chunk_size, text_len)\n",
    "        chunk_text = text[start_char:end_char]\n",
    "        chunks.append((chunk_text, start_char))\n",
    "        if end_char == text_len:\n",
    "            break\n",
    "        i += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "# 3. API Call with Retry for Thread Use\n",
    "\n",
    "def call_api_with_retry_chunk(chunk, extraction_instructions, client, max_output_tokens=2048, retries=4):\n",
    "    # print(\"Chunk being sent:\", repr(chunk))     # Can use this to DEBUG chunk issues\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = client.responses.create(\n",
    "                model=\"gpt-4.1-2025-04-14\",\n",
    "                instructions=extraction_instructions,\n",
    "                input=chunk,\n",
    "                text={\"format\": {\"type\": \"text\"}},\n",
    "                reasoning={},\n",
    "                tools=[\n",
    "                    {\n",
    "                        \"type\": \"function\",\n",
    "                        \"name\": \"recognize_toponyms\",\n",
    "                        \"description\": \"Given the user input text, identify all the toponyms in the text.\",\n",
    "                        \"parameters\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"required\": [\"input_text\", \"toponyms\"],\n",
    "                            \"properties\": {\n",
    "                                \"input_text\": {\n",
    "                                    \"type\": \"string\", \n",
    "                                    \"description\": \"The text string from which to recognize and identify toponyms.\"\n",
    "                                },\n",
    "                                \"toponyms\": {\n",
    "                                    \"type\": \"array\",\n",
    "                                    \"description\": \"Array of recognized and identified toponyms.\",\n",
    "                                    \"items\": {\n",
    "                                        \"type\": \"object\",\n",
    "                                        \"properties\": {\n",
    "                                            \"toponym\": {\"type\": \"string\"}\n",
    "                                        },\n",
    "                                        \"required\": [\"toponym\"],\n",
    "                                        \"additionalProperties\": False\n",
    "                                    }\n",
    "                                }\n",
    "                            },\n",
    "                            \"additionalProperties\": False\n",
    "                        },\n",
    "                        \"strict\": True\n",
    "                    }\n",
    "                ],\n",
    "                temperature=0,\n",
    "                tool_choice=\"required\",\n",
    "                max_output_tokens=max_output_tokens,\n",
    "                top_p=1,\n",
    "                store=True\n",
    "            )\n",
    "            \"\"\" These next three lines can be used to DEBUG responses or lack thereof\n",
    "            print(\"Raw response:\", response)\n",
    "            extracted = extract_json_from_arguments(response)\n",
    "            print(\"Extracted:\", extracted)\n",
    "            \"\"\"\n",
    "            return extract_json_from_arguments(response)         \n",
    "        except Exception as e:\n",
    "            wait = 2 ** attempt\n",
    "            print(f\"[API] Error: {e}\\nRetrying in {wait}s...\")\n",
    "            time.sleep(wait)\n",
    "    print(f\"[API] Failed after retries.\")\n",
    "    return []\n",
    "\n",
    "# 4. Stage 1: Parallel Toponym Extraction\n",
    "\n",
    "# ====== Load Extraction Prompt ======\n",
    "with open(\"openai_ToponymExtraction_prompt_complicated_25.txt\", encoding=\"utf-8\") as f:\n",
    "    extraction_instructions = f.read()\n",
    "\n",
    "# ====== Chunk Input ======\n",
    "    # chunk via characters\n",
    "chunks = chunk_text_by_chars(texts, chunk_size=600, overlap=0)\n",
    "print(f\"Text split into {len(chunks)} char-based chunks for extraction.\")\n",
    "\n",
    "# ====== Run Extraction in Parallel ======\n",
    "max_workers = 20   # safe for modern high-tier; can adjust up/down\n",
    "extracted_toponyms = []\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    futures = [\n",
    "        executor.submit(\n",
    "            call_api_with_retry_chunk, chunk, extraction_instructions, client\n",
    "        )\n",
    "        for chunk, _ in chunks\n",
    "    ]\n",
    "    for f in as_completed(futures):\n",
    "        result = f.result()\n",
    "        # print(\"DEBUG:\", result)          # Can use this if need to DEBUG results\n",
    "        if isinstance(result, dict) and \"toponyms\" in result:\n",
    "            extracted_toponyms += result[\"toponyms\"]\n",
    "        elif isinstance(result, list):\n",
    "            extracted_toponyms += result\n",
    "        else:\n",
    "            print(\"Warning: Unexpected result format\", result)\n",
    "\n",
    "print(f\"\\nInitial extraction stage complete: Got {len(extracted_toponyms)} toponym instances (with possible duplicates).\")\n",
    "\n",
    "# ------------------ DEDUPLICATION STEP --------------------\n",
    "def get_local_context(text, name, window=50):\n",
    "    \"\"\"Find the first occurrence of name in text and return local context window.\"\"\"\n",
    "    lowers = text.lower()\n",
    "    name_lower = name.lower()\n",
    "    idx = lowers.find(name_lower)\n",
    "    if idx == -1:\n",
    "        # fallback: just use the first window of the text (may dedupe global substrings, edge case)\n",
    "        return text[:2*window]\n",
    "    start = max(0, idx - window)\n",
    "    end = min(len(text), idx + len(name) + window)\n",
    "    return text[start:end]\n",
    "\n",
    "def deduplicate_by_fuzzy_context_and_longest(toponym_list, texts, window=50, similarity=0.90):\n",
    "    \"\"\"\n",
    "    Group extracted toponyms by fuzzy context similarity.\n",
    "    Keeps only the longest (most specific) name in each group.\n",
    "    Can be more aggressive with wider window & lower similarity threshold!!\n",
    "    \"\"\"\n",
    "    items = [\n",
    "        (t['toponym'].strip(), get_local_context(texts, t['toponym'].strip(), window), t)\n",
    "        for t in toponym_list\n",
    "    ]\n",
    "    groups = []\n",
    "    used = set()\n",
    "    for i, (name_i, ctx_i, obj_i) in enumerate(items):\n",
    "        if i in used: continue\n",
    "        group = [(name_i, ctx_i, obj_i)]\n",
    "        used.add(i)\n",
    "        for j, (name_j, ctx_j, obj_j) in enumerate(items):\n",
    "            if j <= i or j in used: continue\n",
    "            if ctx_i and ctx_j:\n",
    "                score = SequenceMatcher(None, ctx_i, ctx_j).ratio()\n",
    "                if score >= similarity:\n",
    "                    group.append((name_j, ctx_j, obj_j))\n",
    "                    used.add(j)\n",
    "        # Within the group, eliminate all substrings: keep only the longest(s)\n",
    "        group.sort(key=lambda g: len(g[0]), reverse=True)\n",
    "        deduped_names = set()\n",
    "        deduped_objs = []\n",
    "        for name, ctx, obj in group:\n",
    "            if not any(name in longer for longer in deduped_names if len(name) < len(longer)):\n",
    "                deduped_names.add(name)\n",
    "                deduped_objs.append(obj)\n",
    "        # Option 1: Only keep the very longest:\n",
    "        groups.append(deduped_objs[0])\n",
    "        # Option 2: To keep all equally-long max variants, use:\n",
    "        # groups.extend(deduped_objs[:1])  # or groups.extend(deduped_objs)\n",
    "    return groups\n",
    "    \n",
    "before = len(extracted_toponyms)\n",
    "extracted_toponyms = deduplicate_by_fuzzy_context_and_longest(extracted_toponyms, texts, window=50, similarity=0.90)\n",
    "after = len(extracted_toponyms)\n",
    "print(f\"Deduplicated toponyms: {before} → {after}\")\n",
    "\n",
    "# ------------------ END DEDUPLICATION STEP --------------------\n",
    "\n",
    "\n",
    "with open(\"extracted_toponyms.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(extracted_toponyms, f, ensure_ascii=False, indent=2)\n",
    "print(f\"\\nStage 1 complete: Saved {len(extracted_toponyms)} unique toponym instances to file.\")\n",
    "\n",
    "# 5. Stage 2: Parallel Toponym Analysis\n",
    "\n",
    "# ====== Load Analysis Prompt ======\n",
    "with open(\"openai_ToponymEmotionAnalysis_prompt_complicated_25.txt\", encoding=\"utf-8\") as f:\n",
    "    analysis_instructions = f.read()\n",
    "\n",
    "def call_api_with_retry_analysis(\n",
    "    toponym_obj,\n",
    "    texts,\n",
    "    client,\n",
    "    analysis_instructions,\n",
    "    max_output_tokens=32000,\n",
    "    retries=4,\n",
    "):\n",
    "    toponym_str = toponym_obj[\"toponym\"]\n",
    "    user_input = {\n",
    "        \"original_text\": texts,\n",
    "        \"toponym_instances\": [{**toponym_obj}]\n",
    "    }\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = client.responses.create(\n",
    "                model=\"gpt-4.1-2025-04-14\",\n",
    "                instructions=analysis_instructions,\n",
    "                input=json.dumps(user_input),\n",
    "                text={\"format\": {\"type\": \"text\"}},\n",
    "                reasoning={},\n",
    "                tools=[{\n",
    "                    \"type\": \"function\",\n",
    "                    \"name\": \"resolve_toponyms_and_detect_emotions\",\n",
    "                    \"description\": (\n",
    "                        \"Given the user input of the original text and extracted toponyms, determine latitude and longitude of each toponym.\"\n",
    "                        \"If the toponym is in France then proceed and perform emotion detection. If not in France, then do no futher action on that toponym and do not include it in your response.\"\n",
    "                        \"Try multiple possible context window sizes (~different context lengths) for each French toponym and \"\n",
    "                        \"return the window (context) that maximizes the confidence score for the most likely detected emotion.\"\n",
    "                    ),\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"required\": [\"original_text\", \"toponym_instances\"],\n",
    "                        \"properties\": {\n",
    "                            \"original_text\": {\"type\": \"string\", \"description\": \"The text string from which to disambiguate toponyms and utilize their surrounding context.\"},\n",
    "                            \"toponym_instances\": {\n",
    "                                \"type\": \"array\",\n",
    "                \t\t\t\t\"description\": \"Array of identified toponyms, each containing properties of location details and emotional context.\",\n",
    "                                \"items\": {\n",
    "                                    \"type\": \"object\",\n",
    "                                    \"required\": [\n",
    "                                        \"toponym\", \"resolved_name\", \"latitude\",\n",
    "                                        \"longitude\", \"emotion\", \"confidence_score\",\n",
    "                                        \"context\", \"context_length\", \"sub_category_emotion\"\n",
    "                                    ],\n",
    "                                    \"properties\": {\n",
    "                                        \"toponym\": {\"type\": \"string\", \"description\": \"The name of the toponym as found in the previous step.\"},\n",
    "                                        \"resolved_name\": {\"type\": \"string\", \"description\": \"The name of the resolved toponym as identified and disambiguated.\"},\n",
    "                                        \"latitude\": {\"type\": \"number\", \"description\": \"The latitude coordinate of the toponym.\"},\n",
    "                                        \"longitude\": {\"type\": \"number\", \"description\": \"The longitude coordinate of the toponym.\"},\n",
    "                                        \"emotion\": {\"type\": \"string\", \"description\": \"The most likely detected emotion around the toponym.\", \"enum\": [\n",
    "                                            \"anger\", \"disgust\", \"fear\", \"joy\", \"sadness\", \"surprise\", \"neutral\"\n",
    "                                        ]},\n",
    "                                        \"confidence_score\": {\"type\": \"number\", \"description\": \"The confidence score for the detected emotion, on a scale of 0 to 1.\"},\n",
    "                                        \"context\": {\"type\": \"string\", \"description\": \"The text block surrounding the toponym used for emotion detection, whose length is determined based on trying different lengths and seeing which one gives the highest confidence score for the most likely detected emotion.\"},\n",
    "                                        \"context_length\": {\"type\": \"number\", \"description\": \"The length, in characters including spaces, of the final text block surrounding the toponym used for emotion detection.\"},\n",
    "                                        \"sub_category_emotion\": {\"type\": \"string\", \"description\":  \"For each of the emotions that you concluded (anger, disgust, fear, joy, sadness, surprise, neutral), tell us which of the following sub-category of that emotion is most likely, using the Ekman emotion classification system\",\n",
    "                                                                \"enum\": [\"Annoyance\", \"Frustration\", \"Exasperation\", \"Argumentativeness\", \"Bitterness\", \"Vengefulness\", \"Fury\",\n",
    "                                                                         \"Dislike\", \"Aversion\", \"Distaste\", \"Repugnance\", \"Revulsion\", \"Abhorrence\", \"Loathing\",\n",
    "                                                                         \"Trepidation\", \"Nervousness\", \"Anxiety\", \"Dread\", \"Desperation\", \"Panic\", \"Horror\", \"Terror\",\n",
    "                                                                         \"Sensory Pleasure\", \"Rejoicing\", \"Compassion/Joy\", \"Amusement\", \"Schadenfreude\", \"Relief\", \"Peace\", \"Pride\", \"Fiero\", \"Naches\", \"Wonder\", \"Excitement\", \"Ecstasy\",\n",
    "                                                                         \"Disappointment\", \"Discouragement\", \"Distraughtness\", \"Resignation\", \"Helplessness\", \"Hopelessness\", \"Misery\", \"Despair\", \"Grief\", \"Sorrow\", \"Anguish\",\n",
    "                                                                         \"Surprise\",\n",
    "                                                                         \"Neutral\"\n",
    "                                                                        ]}\n",
    "                                    },\n",
    "                                    \"additionalProperties\": False,\n",
    "                                },\n",
    "                            }\n",
    "                        },\n",
    "                        \"additionalProperties\": False,\n",
    "                    },\n",
    "                    \"strict\": True\n",
    "                }],\n",
    "                temperature=1,\n",
    "                tool_choice=\"required\",\n",
    "                max_output_tokens=max_output_tokens,\n",
    "                top_p=1,\n",
    "                store=True\n",
    "            )\n",
    "            return extract_json_from_arguments(response), toponym_str\n",
    "        except Exception as e:\n",
    "            wait = 2 ** attempt\n",
    "            print(f\"[API] Analysis error for '{toponym_str}': {e}\\nRetrying in {wait}s...\")\n",
    "            time.sleep(wait)\n",
    "    print(f\"[API] Analysis failed after retries for '{toponym_str}'.\")\n",
    "    return {\"toponym\": toponym_str, \"error\": \"Failed after retries\"}, toponym_str\n",
    "\n",
    "# Run Stage 2 in Parallel\n",
    "\n",
    "# ---- Load the extracted_toponyms ----\n",
    "with open(\"extracted_toponyms.json\", encoding=\"utf-8\") as f:\n",
    "    extracted_toponyms = json.load(f)\n",
    "\"\"\"    \n",
    "# This sets a context window that is at the maximum of 600 characters to avoid the sometimes random \n",
    "5000-character context windows that the model decides to use when calling the function, even though\n",
    "I told it to not give me more than 600 character windows\n",
    "\"\"\"\n",
    "def get_context(text, toponym, window=600):\n",
    "    idx = text.lower().find(toponym.lower())\n",
    "    if idx == -1:\n",
    "        print(f\"Warning: Toponym {toponym} not found in text.\")\n",
    "        return text\n",
    "    start = max(0, idx - window)\n",
    "    end = min(len(text), idx + len(toponym) + window)\n",
    "    return text[start:end]\n",
    "\n",
    "analysis_results = []\n",
    "\"\"\"\n",
    "Keep max_workers relatively low to prevent truncation of output (which results in \"JSON parsing errors\" \n",
    "due to attempting this on a truncated \"list\" rather than the actual dictionary that it is).  \n",
    "Also keeps it below rate limits.\n",
    "\"\"\"\n",
    "max_workers_analysis = 6\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=max_workers_analysis) as executor:\n",
    "    futures = [\n",
    "        executor.submit(\n",
    "            call_api_with_retry_analysis, t, get_context(texts, t[\"toponym\"]), client, analysis_instructions, 32000\n",
    "        )\n",
    "        for t in extracted_toponyms\n",
    "    ]\n",
    "    for f in as_completed(futures):\n",
    "        batch_result, toponym_str = f.result()\n",
    "        # Handle lists/dicts as before\n",
    "        if isinstance(batch_result, list):\n",
    "            analysis_results += batch_result\n",
    "            print(f\"Analyzed: {toponym_str} (got list)\")\n",
    "        elif isinstance(batch_result, dict) and \"toponym_instances\" in batch_result:\n",
    "            analysis_results += batch_result[\"toponym_instances\"]\n",
    "            print(f\"Analyzed: {toponym_str} (from .toponym_instances)\")\n",
    "        else:\n",
    "            analysis_results.append(batch_result)\n",
    "            print(f\"Analyzed: {toponym_str} (error or unexpected shape)\")\n",
    "\n",
    "with open(\"analysis_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(analysis_results, f, ensure_ascii=False, indent=2)\n",
    "print(f\"\\nStage 2 complete: Produced {len(analysis_results)} detailed toponym analyses.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53d90d8a-b1f8-4507-84aa-0389fe053388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toponym</th>\n",
       "      <th>resolved_name</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>emotion</th>\n",
       "      <th>confidence_score</th>\n",
       "      <th>context</th>\n",
       "      <th>context_length</th>\n",
       "      <th>sub_category_emotion</th>\n",
       "      <th>emotion_numeric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gers</td>\n",
       "      <td>Gers</td>\n",
       "      <td>43.716700</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.50</td>\n",
       "      <td>No relevant context for Gers found in the text.</td>\n",
       "      <td>0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Auch</td>\n",
       "      <td>Auch</td>\n",
       "      <td>43.647800</td>\n",
       "      <td>0.586700</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.73</td>\n",
       "      <td>I was given a \\nFrench identity card with the ...</td>\n",
       "      <td>360</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Le Chambon-sur-Lignon</td>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Eastern Europe</td>\n",
       "      <td>Eastern Europe</td>\n",
       "      <td>54.525960</td>\n",
       "      <td>15.255120</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>France</td>\n",
       "      <td>France</td>\n",
       "      <td>46.603354</td>\n",
       "      <td>1.888334</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.70</td>\n",
       "      <td>At the same time, the thing from the UGIF [Uni...</td>\n",
       "      <td>329</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Germany</td>\n",
       "      <td>Germany</td>\n",
       "      <td>51.165691</td>\n",
       "      <td>10.451526</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Marseilles</td>\n",
       "      <td>Marseille</td>\n",
       "      <td>43.296482</td>\n",
       "      <td>5.369780</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.77</td>\n",
       "      <td>We were told that we'll have to get out in two...</td>\n",
       "      <td>312</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Les Grillons</td>\n",
       "      <td>Les Grillons</td>\n",
       "      <td>45.045850</td>\n",
       "      <td>4.311390</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.81</td>\n",
       "      <td>Mr. Trocmé came on his bicycle to meet me (sti...</td>\n",
       "      <td>545</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Le Chambon sur Lignon</td>\n",
       "      <td>Le Chambon-sur-Lignon</td>\n",
       "      <td>45.060810</td>\n",
       "      <td>4.302941</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.50</td>\n",
       "      <td>Saturday, January 16, 1943 [ Le Chambon sur Li...</td>\n",
       "      <td>523</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Romania</td>\n",
       "      <td>Romania</td>\n",
       "      <td>45.943200</td>\n",
       "      <td>24.966800</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Haute-Loire</td>\n",
       "      <td>Haute-Loire</td>\n",
       "      <td>45.133333</td>\n",
       "      <td>3.916667</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.93</td>\n",
       "      <td>I am sad and weary. During rest hour Mrs. Cava...</td>\n",
       "      <td>324</td>\n",
       "      <td>Discouragement</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Hungary</td>\n",
       "      <td>Hungary</td>\n",
       "      <td>47.162500</td>\n",
       "      <td>19.503300</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Cheylard</td>\n",
       "      <td>Le Cheylard</td>\n",
       "      <td>44.905714</td>\n",
       "      <td>4.420413</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.66</td>\n",
       "      <td>the Marseilles to Paris express (travel, chang...</td>\n",
       "      <td>454</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Massif Central</td>\n",
       "      <td>Massif Central</td>\n",
       "      <td>45.500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.68</td>\n",
       "      <td>I discovered later, especially after the war, ...</td>\n",
       "      <td>465</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>La Rouvière</td>\n",
       "      <td>La Rouvière</td>\n",
       "      <td>44.044444</td>\n",
       "      <td>4.536389</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.91</td>\n",
       "      <td>Monday, January 4, 1943 \\nMr. Brémond came to ...</td>\n",
       "      <td>214</td>\n",
       "      <td>Grief</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>St. Agrève</td>\n",
       "      <td>Saint-Agrève</td>\n",
       "      <td>45.016700</td>\n",
       "      <td>4.416700</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.75</td>\n",
       "      <td>I left aboard the \\nMarseilles to Paris expres...</td>\n",
       "      <td>599</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Paris</td>\n",
       "      <td>Paris, France</td>\n",
       "      <td>48.856600</td>\n",
       "      <td>2.352200</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.65</td>\n",
       "      <td>I took leave, took the first streetcar at 6 a....</td>\n",
       "      <td>278</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>College Cevenol</td>\n",
       "      <td>Collège Cévenol</td>\n",
       "      <td>45.064261</td>\n",
       "      <td>4.304111</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.78</td>\n",
       "      <td>And so I found shelter there. I, I was sent to...</td>\n",
       "      <td>276</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Lavoulte bridge</td>\n",
       "      <td>Pont de La Voulte-sur-Rhône</td>\n",
       "      <td>44.799823</td>\n",
       "      <td>4.780769</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.68</td>\n",
       "      <td>I left aboard the \\nMarseilles to Paris expres...</td>\n",
       "      <td>255</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Saint-Etienne</td>\n",
       "      <td>Saint-Étienne</td>\n",
       "      <td>45.439700</td>\n",
       "      <td>4.387200</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.83</td>\n",
       "      <td>It was a little village up above Saint-Etienne...</td>\n",
       "      <td>310</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Drancy</td>\n",
       "      <td>Drancy</td>\n",
       "      <td>48.924300</td>\n",
       "      <td>2.445600</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.92</td>\n",
       "      <td>This idyll didn't last too long, because alrea...</td>\n",
       "      <td>420</td>\n",
       "      <td>Dread</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Switzerland</td>\n",
       "      <td>Switzerland</td>\n",
       "      <td>46.818200</td>\n",
       "      <td>8.227500</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Alps</td>\n",
       "      <td>Alps</td>\n",
       "      <td>45.999500</td>\n",
       "      <td>7.726600</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Vallorcine</td>\n",
       "      <td>Vallorcine</td>\n",
       "      <td>46.014700</td>\n",
       "      <td>6.934100</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.91</td>\n",
       "      <td>they ordered a truck. And they brought us \\nto...</td>\n",
       "      <td>336</td>\n",
       "      <td>Resignation</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Spain</td>\n",
       "      <td>Spain</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Austria</td>\n",
       "      <td>Austria</td>\n",
       "      <td>47.516200</td>\n",
       "      <td>14.550100</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Poland</td>\n",
       "      <td>Poland</td>\n",
       "      <td>51.919438</td>\n",
       "      <td>19.145136</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Maison des Roches</td>\n",
       "      <td>Maison des Roches</td>\n",
       "      <td>45.067754</td>\n",
       "      <td>4.296655</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.93</td>\n",
       "      <td>I received, through, again, Tracy Strong, infl...</td>\n",
       "      <td>547</td>\n",
       "      <td>Peace</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Gurs</td>\n",
       "      <td>Gurs</td>\n",
       "      <td>43.334722</td>\n",
       "      <td>-0.818056</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.81</td>\n",
       "      <td>I found an Austrian with whom I get along well...</td>\n",
       "      <td>339</td>\n",
       "      <td>Sorrow</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Perpignan</td>\n",
       "      <td>Perpignan</td>\n",
       "      <td>42.698611</td>\n",
       "      <td>2.895833</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.61</td>\n",
       "      <td>But soon, unbelievable gesture of the French V...</td>\n",
       "      <td>415</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Swiss</td>\n",
       "      <td>Switzerland</td>\n",
       "      <td>46.818200</td>\n",
       "      <td>8.227500</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Rivesaltes</td>\n",
       "      <td>Rivesaltes</td>\n",
       "      <td>42.799343</td>\n",
       "      <td>2.879855</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.88</td>\n",
       "      <td>We were put into handcuffs and sent back to Ri...</td>\n",
       "      <td>355</td>\n",
       "      <td>Resignation</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Les Caillols</td>\n",
       "      <td>Les Caillols</td>\n",
       "      <td>43.304720</td>\n",
       "      <td>5.448060</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.82</td>\n",
       "      <td>Tuesday, January 5, 1943 [Les Caillols] \\nMrs....</td>\n",
       "      <td>414</td>\n",
       "      <td>Relief</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>La Guespy</td>\n",
       "      <td>La Guespy</td>\n",
       "      <td>45.061500</td>\n",
       "      <td>4.298700</td>\n",
       "      <td>disgust</td>\n",
       "      <td>0.91</td>\n",
       "      <td>I'm staying at La Guespy, which is a home for ...</td>\n",
       "      <td>336</td>\n",
       "      <td>Dislike</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Swiss border</td>\n",
       "      <td>Swiss-French border</td>\n",
       "      <td>46.204400</td>\n",
       "      <td>6.143200</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Britain</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>55.378100</td>\n",
       "      <td>-3.436000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Vichy</td>\n",
       "      <td>Vichy</td>\n",
       "      <td>46.127800</td>\n",
       "      <td>3.426400</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.87</td>\n",
       "      <td>His father [is] in [the French internment camp...</td>\n",
       "      <td>183</td>\n",
       "      <td>Relief</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Algiers</td>\n",
       "      <td>Algiers</td>\n",
       "      <td>36.753800</td>\n",
       "      <td>3.058800</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Camp de Gurs</td>\n",
       "      <td>Camp de Gurs</td>\n",
       "      <td>43.378709</td>\n",
       "      <td>-0.748064</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.68</td>\n",
       "      <td>So in, sometime in September of 1942, a woman ...</td>\n",
       "      <td>553</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>French</td>\n",
       "      <td>France</td>\n",
       "      <td>46.603354</td>\n",
       "      <td>1.888334</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.60</td>\n",
       "      <td>His father [is] in [the French internment camp...</td>\n",
       "      <td>579</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Le Puy</td>\n",
       "      <td>Le Puy-en-Velay</td>\n",
       "      <td>45.043660</td>\n",
       "      <td>3.885240</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.97</td>\n",
       "      <td>They say they're taking us to Le Puy, which is...</td>\n",
       "      <td>238</td>\n",
       "      <td>Dread</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>German</td>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Annecy</td>\n",
       "      <td>Annecy</td>\n",
       "      <td>45.899247</td>\n",
       "      <td>6.129384</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.75</td>\n",
       "      <td>I'm supposed to be a French Boy Scout just sit...</td>\n",
       "      <td>211</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>University of Lyon</td>\n",
       "      <td>Université de Lyon</td>\n",
       "      <td>45.750000</td>\n",
       "      <td>4.850000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.74</td>\n",
       "      <td>His name was André Philip. He was minister of ...</td>\n",
       "      <td>541</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Valence</td>\n",
       "      <td>Valence</td>\n",
       "      <td>44.933300</td>\n",
       "      <td>4.892360</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.91</td>\n",
       "      <td>Jean-Pierre, Pastor Trocme's son, and Marco Da...</td>\n",
       "      <td>343</td>\n",
       "      <td>Trepidation</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Le Chambon-sur Lignon</td>\n",
       "      <td>Le Chambon-sur-Lignon</td>\n",
       "      <td>45.060810</td>\n",
       "      <td>4.302941</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.93</td>\n",
       "      <td>And then the next day, we went \\nto the Le Cha...</td>\n",
       "      <td>407</td>\n",
       "      <td>Sorrow</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Toulouse</td>\n",
       "      <td>Toulouse</td>\n",
       "      <td>43.604500</td>\n",
       "      <td>1.444000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.75</td>\n",
       "      <td>they, they gathered us together, and we went o...</td>\n",
       "      <td>361</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>England</td>\n",
       "      <td>England</td>\n",
       "      <td>52.355518</td>\n",
       "      <td>-1.174320</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>LePuy-en-Velay</td>\n",
       "      <td>Le Puy-en-Velay</td>\n",
       "      <td>45.043700</td>\n",
       "      <td>3.885400</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.91</td>\n",
       "      <td>They're trying to stop the gendarmes from taki...</td>\n",
       "      <td>481</td>\n",
       "      <td>Nervousness</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>New York</td>\n",
       "      <td>New York, New York, United States</td>\n",
       "      <td>40.712776</td>\n",
       "      <td>-74.005974</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Somebody, he was just last year, he was honore...</td>\n",
       "      <td>125</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Lyon</td>\n",
       "      <td>Lyon</td>\n",
       "      <td>45.750000</td>\n",
       "      <td>4.850000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.81</td>\n",
       "      <td>He was minister of finance under de Gaulle, wh...</td>\n",
       "      <td>488</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Laval</td>\n",
       "      <td>Laval (person, Pierre Laval as Vichy regime pr...</td>\n",
       "      <td>48.077500</td>\n",
       "      <td>-0.763600</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Auschwitz</td>\n",
       "      <td>Auschwitz</td>\n",
       "      <td>50.027400</td>\n",
       "      <td>19.203600</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Washington, DC</td>\n",
       "      <td>Washington, DC</td>\n",
       "      <td>38.895110</td>\n",
       "      <td>-77.036370</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.50</td>\n",
       "      <td>Her name is Feihan now. \\nShe lives in Washing...</td>\n",
       "      <td>115</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Bayside, New York</td>\n",
       "      <td>Bayside, New York</td>\n",
       "      <td>40.763400</td>\n",
       "      <td>-73.774500</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.00</td>\n",
       "      <td>The next one is, the other little girl is Hann...</td>\n",
       "      <td>191</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Alsace-Lorraine</td>\n",
       "      <td>Alsace-Lorraine</td>\n",
       "      <td>48.300000</td>\n",
       "      <td>7.200000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.64</td>\n",
       "      <td>Madame Philip and I sat at his dining-room tab...</td>\n",
       "      <td>492</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Grenoble</td>\n",
       "      <td>Grenoble</td>\n",
       "      <td>45.188529</td>\n",
       "      <td>5.724524</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.79</td>\n",
       "      <td>And she took a, and then, it was a \\nMonday sh...</td>\n",
       "      <td>309</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>German border</td>\n",
       "      <td>German border</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  toponym                                      resolved_name  \\\n",
       "0                    Gers                                               Gers   \n",
       "1                    Auch                                               Auch   \n",
       "2   Le Chambon-sur-Lignon                                                      \n",
       "3          Eastern Europe                                     Eastern Europe   \n",
       "4                  France                                             France   \n",
       "5                 Germany                                            Germany   \n",
       "6              Marseilles                                          Marseille   \n",
       "7            Les Grillons                                       Les Grillons   \n",
       "8   Le Chambon sur Lignon                              Le Chambon-sur-Lignon   \n",
       "9                 Romania                                            Romania   \n",
       "10            Haute-Loire                                        Haute-Loire   \n",
       "11                Hungary                                            Hungary   \n",
       "12               Cheylard                                        Le Cheylard   \n",
       "13         Massif Central                                     Massif Central   \n",
       "14            La Rouvière                                        La Rouvière   \n",
       "15             St. Agrève                                       Saint-Agrève   \n",
       "16                  Paris                                      Paris, France   \n",
       "17        College Cevenol                                    Collège Cévenol   \n",
       "18        Lavoulte bridge                        Pont de La Voulte-sur-Rhône   \n",
       "19          Saint-Etienne                                      Saint-Étienne   \n",
       "20                 Drancy                                             Drancy   \n",
       "21            Switzerland                                        Switzerland   \n",
       "22                   Alps                                               Alps   \n",
       "23             Vallorcine                                         Vallorcine   \n",
       "24                  Spain                                              Spain   \n",
       "25                Austria                                            Austria   \n",
       "26                 Poland                                             Poland   \n",
       "27      Maison des Roches                                  Maison des Roches   \n",
       "28                   Gurs                                               Gurs   \n",
       "29              Perpignan                                          Perpignan   \n",
       "30                  Swiss                                        Switzerland   \n",
       "31             Rivesaltes                                         Rivesaltes   \n",
       "32           Les Caillols                                       Les Caillols   \n",
       "33              La Guespy                                          La Guespy   \n",
       "34           Swiss border                                Swiss-French border   \n",
       "35                Britain                                     United Kingdom   \n",
       "36                  Vichy                                              Vichy   \n",
       "37                Algiers                                            Algiers   \n",
       "38           Camp de Gurs                                       Camp de Gurs   \n",
       "39                 French                                             France   \n",
       "40                 Le Puy                                    Le Puy-en-Velay   \n",
       "41                 German                                                      \n",
       "42                 Annecy                                             Annecy   \n",
       "43     University of Lyon                                 Université de Lyon   \n",
       "44                Valence                                            Valence   \n",
       "45  Le Chambon-sur Lignon                              Le Chambon-sur-Lignon   \n",
       "46               Toulouse                                           Toulouse   \n",
       "47                England                                            England   \n",
       "48         LePuy-en-Velay                                    Le Puy-en-Velay   \n",
       "49               New York                  New York, New York, United States   \n",
       "50                   Lyon                                               Lyon   \n",
       "51                  Laval  Laval (person, Pierre Laval as Vichy regime pr...   \n",
       "52              Auschwitz                                          Auschwitz   \n",
       "53         Washington, DC                                     Washington, DC   \n",
       "54      Bayside, New York                                  Bayside, New York   \n",
       "55        Alsace-Lorraine                                    Alsace-Lorraine   \n",
       "56               Grenoble                                           Grenoble   \n",
       "57          German border                                      German border   \n",
       "\n",
       "     latitude  longitude  emotion  confidence_score  \\\n",
       "0   43.716700   0.600000  neutral              0.50   \n",
       "1   43.647800   0.586700  neutral              0.73   \n",
       "2    0.000000   0.000000  neutral              0.00   \n",
       "3   54.525960  15.255120  neutral              0.00   \n",
       "4   46.603354   1.888334  neutral              0.70   \n",
       "5   51.165691  10.451526  neutral              0.00   \n",
       "6   43.296482   5.369780  neutral              0.77   \n",
       "7   45.045850   4.311390  neutral              0.81   \n",
       "8   45.060810   4.302941  neutral              0.50   \n",
       "9   45.943200  24.966800  neutral              0.00   \n",
       "10  45.133333   3.916667  sadness              0.93   \n",
       "11  47.162500  19.503300  neutral              0.00   \n",
       "12  44.905714   4.420413  neutral              0.66   \n",
       "13  45.500000   3.000000  neutral              0.68   \n",
       "14  44.044444   4.536389  sadness              0.91   \n",
       "15  45.016700   4.416700  neutral              0.75   \n",
       "16  48.856600   2.352200  neutral              0.65   \n",
       "17  45.064261   4.304111  neutral              0.78   \n",
       "18  44.799823   4.780769  neutral              0.68   \n",
       "19  45.439700   4.387200  neutral              0.83   \n",
       "20  48.924300   2.445600     fear              0.92   \n",
       "21  46.818200   8.227500  neutral              0.00   \n",
       "22  45.999500   7.726600  neutral              0.00   \n",
       "23  46.014700   6.934100  sadness              0.91   \n",
       "24  40.000000  -4.000000  neutral              0.00   \n",
       "25  47.516200  14.550100  neutral              0.00   \n",
       "26  51.919438  19.145136  neutral              0.00   \n",
       "27  45.067754   4.296655      joy              0.93   \n",
       "28  43.334722  -0.818056  sadness              0.81   \n",
       "29  42.698611   2.895833  neutral              0.61   \n",
       "30  46.818200   8.227500  neutral              0.00   \n",
       "31  42.799343   2.879855  sadness              0.88   \n",
       "32  43.304720   5.448060      joy              0.82   \n",
       "33  45.061500   4.298700  disgust              0.91   \n",
       "34  46.204400   6.143200  neutral              0.00   \n",
       "35  55.378100  -3.436000  neutral              0.00   \n",
       "36  46.127800   3.426400      joy              0.87   \n",
       "37  36.753800   3.058800  neutral              0.00   \n",
       "38  43.378709  -0.748064  neutral              0.68   \n",
       "39  46.603354   1.888334  neutral              0.60   \n",
       "40  45.043660   3.885240     fear              0.97   \n",
       "41   0.000000   0.000000  neutral              0.00   \n",
       "42  45.899247   6.129384  neutral              0.75   \n",
       "43  45.750000   4.850000  neutral              0.74   \n",
       "44  44.933300   4.892360     fear              0.91   \n",
       "45  45.060810   4.302941  sadness              0.93   \n",
       "46  43.604500   1.444000  neutral              0.75   \n",
       "47  52.355518  -1.174320  neutral              0.00   \n",
       "48  45.043700   3.885400     fear              0.91   \n",
       "49  40.712776 -74.005974  neutral              0.00   \n",
       "50  45.750000   4.850000  neutral              0.81   \n",
       "51  48.077500  -0.763600  neutral              0.00   \n",
       "52  50.027400  19.203600  neutral              0.00   \n",
       "53  38.895110 -77.036370  neutral              0.50   \n",
       "54  40.763400 -73.774500  neutral              0.00   \n",
       "55  48.300000   7.200000  neutral              0.64   \n",
       "56  45.188529   5.724524  neutral              0.79   \n",
       "57   0.000000   0.000000  neutral              0.00   \n",
       "\n",
       "                                              context  context_length  \\\n",
       "0     No relevant context for Gers found in the text.               0   \n",
       "1   I was given a \\nFrench identity card with the ...             360   \n",
       "2                                                                   0   \n",
       "3                                                                   0   \n",
       "4   At the same time, the thing from the UGIF [Uni...             329   \n",
       "5                                                                   0   \n",
       "6   We were told that we'll have to get out in two...             312   \n",
       "7   Mr. Trocmé came on his bicycle to meet me (sti...             545   \n",
       "8   Saturday, January 16, 1943 [ Le Chambon sur Li...             523   \n",
       "9                                                                   0   \n",
       "10  I am sad and weary. During rest hour Mrs. Cava...             324   \n",
       "11                                                                  0   \n",
       "12  the Marseilles to Paris express (travel, chang...             454   \n",
       "13  I discovered later, especially after the war, ...             465   \n",
       "14  Monday, January 4, 1943 \\nMr. Brémond came to ...             214   \n",
       "15  I left aboard the \\nMarseilles to Paris expres...             599   \n",
       "16  I took leave, took the first streetcar at 6 a....             278   \n",
       "17  And so I found shelter there. I, I was sent to...             276   \n",
       "18  I left aboard the \\nMarseilles to Paris expres...             255   \n",
       "19  It was a little village up above Saint-Etienne...             310   \n",
       "20  This idyll didn't last too long, because alrea...             420   \n",
       "21                                                                  0   \n",
       "22                                                                  0   \n",
       "23  they ordered a truck. And they brought us \\nto...             336   \n",
       "24                                                                  0   \n",
       "25                                                                  0   \n",
       "26                                                                  0   \n",
       "27  I received, through, again, Tracy Strong, infl...             547   \n",
       "28  I found an Austrian with whom I get along well...             339   \n",
       "29  But soon, unbelievable gesture of the French V...             415   \n",
       "30                                                                  0   \n",
       "31  We were put into handcuffs and sent back to Ri...             355   \n",
       "32  Tuesday, January 5, 1943 [Les Caillols] \\nMrs....             414   \n",
       "33  I'm staying at La Guespy, which is a home for ...             336   \n",
       "34                                                                  0   \n",
       "35                                                                  0   \n",
       "36  His father [is] in [the French internment camp...             183   \n",
       "37                                                                  0   \n",
       "38  So in, sometime in September of 1942, a woman ...             553   \n",
       "39  His father [is] in [the French internment camp...             579   \n",
       "40  They say they're taking us to Le Puy, which is...             238   \n",
       "41                                                                  0   \n",
       "42  I'm supposed to be a French Boy Scout just sit...             211   \n",
       "43  His name was André Philip. He was minister of ...             541   \n",
       "44  Jean-Pierre, Pastor Trocme's son, and Marco Da...             343   \n",
       "45  And then the next day, we went \\nto the Le Cha...             407   \n",
       "46  they, they gathered us together, and we went o...             361   \n",
       "47                                                                  0   \n",
       "48  They're trying to stop the gendarmes from taki...             481   \n",
       "49  Somebody, he was just last year, he was honore...             125   \n",
       "50  He was minister of finance under de Gaulle, wh...             488   \n",
       "51                                                                  0   \n",
       "52                                                                  0   \n",
       "53  Her name is Feihan now. \\nShe lives in Washing...             115   \n",
       "54  The next one is, the other little girl is Hann...             191   \n",
       "55  Madame Philip and I sat at his dining-room tab...             492   \n",
       "56  And she took a, and then, it was a \\nMonday sh...             309   \n",
       "57                                                                  0   \n",
       "\n",
       "   sub_category_emotion emotion_numeric  \n",
       "0               Neutral               4  \n",
       "1               Neutral               4  \n",
       "2               Neutral               4  \n",
       "3               Neutral               4  \n",
       "4               Neutral               4  \n",
       "5               Neutral               4  \n",
       "6               Neutral               4  \n",
       "7               Neutral               4  \n",
       "8               Neutral               4  \n",
       "9               Neutral               4  \n",
       "10       Discouragement               5  \n",
       "11              Neutral               4  \n",
       "12              Neutral               4  \n",
       "13              Neutral               4  \n",
       "14                Grief               5  \n",
       "15              Neutral               4  \n",
       "16              Neutral               4  \n",
       "17              Neutral               4  \n",
       "18              Neutral               4  \n",
       "19              Neutral               4  \n",
       "20                Dread               2  \n",
       "21              Neutral               4  \n",
       "22              Neutral               4  \n",
       "23          Resignation               5  \n",
       "24              Neutral               4  \n",
       "25              Neutral               4  \n",
       "26              Neutral               4  \n",
       "27                Peace               3  \n",
       "28               Sorrow               5  \n",
       "29              Neutral               4  \n",
       "30              Neutral               4  \n",
       "31          Resignation               5  \n",
       "32               Relief               3  \n",
       "33              Dislike               1  \n",
       "34              Neutral               4  \n",
       "35              Neutral               4  \n",
       "36               Relief               3  \n",
       "37              Neutral               4  \n",
       "38              Neutral               4  \n",
       "39              Neutral               4  \n",
       "40                Dread               2  \n",
       "41              Neutral               4  \n",
       "42              Neutral               4  \n",
       "43              Neutral               4  \n",
       "44          Trepidation               2  \n",
       "45               Sorrow               5  \n",
       "46              Neutral               4  \n",
       "47              Neutral               4  \n",
       "48          Nervousness               2  \n",
       "49              Neutral               4  \n",
       "50              Neutral               4  \n",
       "51              Neutral               4  \n",
       "52              Neutral               4  \n",
       "53              Neutral               4  \n",
       "54              Neutral               4  \n",
       "55              Neutral               4  \n",
       "56              Neutral               4  \n",
       "57              Neutral               4  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take response output in json format, put into a dataframe, then assign numeric values \n",
    "# to the detected emotions.\n",
    "\n",
    "df = pd.DataFrame(analysis_results)\n",
    "\n",
    "conditions = [\n",
    "    df[\"emotion\"] == \"anger\",\n",
    "    df[\"emotion\"] == \"disgust\",\n",
    "    df[\"emotion\"] == \"fear\",\n",
    "    df[\"emotion\"] == \"joy\",\n",
    "    df[\"emotion\"] == \"neutral\",\n",
    "    df[\"emotion\"] == \"sadness\",\n",
    "    df[\"emotion\"] == \"surprise\"\n",
    "]\n",
    "values = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\"]\n",
    "df[\"emotion_numeric\"] = np.select(conditions, values, default=\"Unknown\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2222fa53-54b9-49a9-8fec-2d18892200cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results to csv\n",
    "\n",
    "df.to_csv(\"Results25_ToponymsEmotions_smallSubCorpus.csv\", encoding=\"utf-8-sig\", index=False, header=True, mode=\"w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f13006-7409-41d6-b74e-4a80e4ff5414",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
