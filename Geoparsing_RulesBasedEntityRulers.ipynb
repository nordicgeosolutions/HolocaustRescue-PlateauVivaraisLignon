{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770594ab-623c-4b0e-be15-b74350dd803f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The following set of scripts allows geoparsing using spaCy's NLP capabilities:\n",
    "It begins with loading different datasets which have csv files with names of locations \n",
    "and their coordinates.\n",
    "Then it creates \"Entity Ruler\" pipes and sets them within the spaCy pipeline, notably BEFORE the \n",
    "normal spaCy NER (Named Entity Recognition).\n",
    "Note:  Besides the Entity Rulers created from the loaded datasets, I show here how to directly create\n",
    "additional pipes just using a typed list.  Some of these are not geographical, but are useful for me\n",
    "and I will be using them in future research.\n",
    "Then it brings in the data, and runs it through the spaCy pipeline which now has the Entity Rulers.\n",
    "Then it creates dataframes of all the locations extracted by the Entity Rulers. Of note, here I also\n",
    "included an instance of an error I received when doing this and my work-around.\n",
    "Then it joins the dataframes of the extracted locations to the original dataframes with the coordinates,\n",
    "resulting in a list of resolved toponyms, by dataset.\n",
    "These results are then exported to csv files.\n",
    "I have also included the additional scripts to return the results on the other Entity Rulers \n",
    "(both non-geographic and for unnamed geographic features) as well as the results from the \n",
    "normal spaCy NER categories, and export them to a csv.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99672b6-7804-4b61-ab05-609f690db025",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reading in the csv file with the names and coordinates of the towns, villages, and named features\n",
    "of the Plateau and creating a pandas dataframe with these locations and their coordinates.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"C:/Users/....../PlateauLocationCoord.csv\")\n",
    "df = df.dropna()\n",
    "df = df[[\"Plateau_Location\", \"Lat\", \"Long\", \"Alt\"]]\n",
    "df[\"Lat\"] = pd.to_numeric(df[\"Lat\"], downcast=\"float\")\n",
    "df[\"Long\"] = pd.to_numeric(df[\"Long\"], downcast=\"float\")\n",
    "df[\"Alt\"] = pd.to_numeric(df[\"Alt\"], downcast=\"float\")\n",
    "df.columns = [\"Plateau_Locations\", \"LAT\", \"LON\", \"ALT\"]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e38d5b-f4f1-4a1b-8397-9ebe87e985b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reading in the csv file with the names and coordinates of the childrens homes\n",
    "of the Plateau and creating a pandas dataframe with these locations and their coordinates.\n",
    "\"\"\"\n",
    "\n",
    "df_homes = pd.read_csv(\"C:/Users/....../homesCoord.csv\")\n",
    "df_homes = df_homes.dropna()\n",
    "df_homes = df_homes[[\"homes\", \"Lat\", \"Long\", \"Alt\"]]\n",
    "df_homes[\"Lat\"] = pd.to_numeric(df_homes[\"Lat\"], downcast=\"float\")\n",
    "df_homes[\"Long\"] = pd.to_numeric(df_homes[\"Long\"], downcast=\"float\")\n",
    "df_homes[\"Alt\"] = pd.to_numeric(df_homes[\"Alt\"], downcast=\"float\")\n",
    "df_homes.columns = [\"Childrens_Homes\", \"LAT\", \"LON\", \"ALT\"]\n",
    "df_homes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43649d2e-ed65-4d80-98da-0420274723b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reading in the csv file with the names and coordinates of the schools on the Plateau\n",
    "for refugees and creating a pandas dataframe with these locations and their coordinates.\n",
    "\"\"\"\n",
    "\n",
    "df_schools = pd.read_csv(\"C:/Users/....../schoolsCoord.csv\")\n",
    "df_schools = df_schools.dropna()\n",
    "df_schools = df_schools[[\"schools\", \"Lat\", \"Long\", \"Alt\"]]\n",
    "df_schools[\"Lat\"] = pd.to_numeric(df_schools[\"Lat\"], downcast=\"float\")\n",
    "df_schools[\"Long\"] = pd.to_numeric(df_schools[\"Long\"], downcast=\"float\")\n",
    "df_schools[\"Alt\"] = pd.to_numeric(df_schools[\"Alt\"], downcast=\"float\")\n",
    "df_schools.columns = [\"Schools\", \"LAT\", \"LON\", \"ALT\"]\n",
    "df_schools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec12b8f-5b9d-4dc6-b805-d1f1889a066f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reading in the csv file with the names and coordinates of the farms named in the testimonies\n",
    "and creating a pandas dataframe with these locations and their coordinates.\n",
    "\"\"\"\n",
    "\n",
    "df_farms = pd.read_csv(\"C:/Users/....../farmsCoord.csv\")\n",
    "df_farms = df_farms.dropna()\n",
    "df_farms = df_farms[[\"farms\", \"Lat\", \"Long\", \"Alt\"]]\n",
    "df_farms[\"Lat\"] = pd.to_numeric(df_farms[\"Lat\"], downcast=\"float\")\n",
    "df_farms[\"Long\"] = pd.to_numeric(df_farms[\"Long\"], downcast=\"float\")\n",
    "df_farms[\"Alt\"] = pd.to_numeric(df_farms[\"Alt\"], downcast=\"float\")\n",
    "df_farms.columns = [\"Farms\", \"LAT\", \"LON\", \"ALT\"]\n",
    "df_farms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a246e227-136b-494c-aea4-20af0092e443",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reading in the csv file with the names and coordinates of the hotels used by German soldiers\n",
    "for convalescence and creating a pandas dataframe with these locations and their coordinates.\n",
    "\"\"\"\n",
    "\n",
    "df_hotelsGer = pd.read_csv(\"C:/Users/....../hotelsGermanConvalesCoord.csv\")\n",
    "df_hotelsGer = df_hotelsGer.dropna()\n",
    "df_hotelsGer = df_hotelsGer[[\"hotels\", \"Lat\", \"Long\", \"Alt\"]]\n",
    "df_hotelsGer[\"Lat\"] = pd.to_numeric(df_hotelsGer[\"Lat\"], downcast=\"float\")\n",
    "df_hotelsGer[\"Long\"] = pd.to_numeric(df_hotelsGer[\"Long\"], downcast=\"float\")\n",
    "df_hotelsGer[\"Alt\"] = pd.to_numeric(df_hotelsGer[\"Alt\"], downcast=\"float\")\n",
    "df_hotelsGer.columns = [\"Hotels_GermanConvalescence\", \"LAT\", \"LON\", \"ALT\"]\n",
    "df_hotelsGer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979f244e-4d40-44ae-b819-ead35931d96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reading in the csv file with the names and coordinates of the other French cities, towns, and camps\n",
    "that are frequently mentioned and creating a pandas dataframe with these locations and their coordinates.\n",
    "\"\"\"\n",
    "\n",
    "df_otherFreLoc = pd.read_csv(\"C:/Users/....../otherFrenchLocationsCoord.csv\")\n",
    "df_otherFreLoc = df_otherFreLoc.dropna()\n",
    "df_otherFreLoc = df_otherFreLoc[[\"other_French_Location\", \"Lat\", \"Long\", \"Alt\"]]\n",
    "df_otherFreLoc[\"Lat\"] = pd.to_numeric(df_otherFreLoc[\"Lat\"], downcast=\"float\")\n",
    "df_otherFreLoc[\"Long\"] = pd.to_numeric(df_otherFreLoc[\"Long\"], downcast=\"float\")\n",
    "df_otherFreLoc[\"Alt\"] = pd.to_numeric(df_otherFreLoc[\"Alt\"], downcast=\"float\")\n",
    "df_otherFreLoc.columns = [\"other_French_Locations\", \"LAT\", \"LON\", \"ALT\"]\n",
    "df_otherFreLoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445922c4-1508-4d5a-a0aa-5a53da15fd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Load libraries.\n",
    "\n",
    "import spacy\n",
    "## spacy.require_gpu()    # Could do this if desired, but need to install PyTorch and CuPy.  \n",
    "from spacy.util import filter_spans\n",
    "from spacy.tokens import Span\n",
    "from spacy.language import Language\n",
    "import re\n",
    "import unidecode\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a7c980-1556-4bd0-802a-833639add387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the spaCy model.\n",
    "\n",
    "# The following is for the English trf model.\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "## You would use the following for the French model...\n",
    "# nlp = spacy.load(\"fr_core_news_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d849061c-5a1e-4d14-9dea-4c7daab9d8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an entity ruler to find named locations (communes, hamlets, mountains, rivers, etc.) on the Plateau\n",
    "Plateau_Locations = df[\"Plateau_Locations\"].to_list()\n",
    "Plateau_Location_patterns = []\n",
    "for Plateau_Location in Plateau_Locations:\n",
    "    normalized = unidecode.unidecode(Plateau_Location)\n",
    "    Plateau_Location_patterns.append({\"pattern\": Plateau_Location, \"label\": \"PLATEAU LOCATION\"})\n",
    "    Plateau_Location_patterns.append({\"pattern\": normalized, \"label\": \"PLATEAU LOCATION\"})\n",
    "Plateau_Location_ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\", name=\"Plateau_Location_ruler\")\n",
    "Plateau_Location_ruler.add_patterns(Plateau_Location_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f734a0-fce8-4131-afc2-183645a6b86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an entity ruler to find named children's homes on the Plateau\n",
    "homes = df_homes[\"Childrens_Homes\"].to_list()\n",
    "home_patterns = []\n",
    "for home in homes:\n",
    "    normalized = unidecode.unidecode(home)\n",
    "    home_patterns.append({\"pattern\": home, \"label\": \"CHILDREN'S HOME\"})\n",
    "    home_patterns.append({\"pattern\": normalized, \"label\": \"CHILDREN'S HOME\"})\n",
    "home_ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\", name=\"home_ruler\")\n",
    "home_ruler.add_patterns(home_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92515e2-a969-4d59-b956-4ed73bec481a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an entity ruler to find named schools on the Plateau\n",
    "schools = df_schools[\"Schools\"].to_list()\n",
    "school_patterns = []\n",
    "for school in schools:\n",
    "    normalized = unidecode.unidecode(school)\n",
    "    school_patterns.append({\"pattern\": school, \"label\": \"SCHOOL\"})\n",
    "    school_patterns.append({\"pattern\": normalized, \"label\": \"SCHOOL\"})\n",
    "school_ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\", name=\"school_ruler\")\n",
    "school_ruler.add_patterns(school_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2409431e-8b99-434f-9121-5a2a0a017570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an entity ruler to find named farms on the Plateau\n",
    "farms = df_farms[\"Farms\"].to_list()\n",
    "farm_patterns = []\n",
    "for farm in farms:\n",
    "    normalized = unidecode.unidecode(farm)\n",
    "    farm_patterns.append({\"pattern\": farm, \"label\": \"FARM\"})\n",
    "    farm_patterns.append({\"pattern\": normalized, \"label\": \"FARM\"})\n",
    "farm_ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\", name=\"farm_ruler\")\n",
    "farm_ruler.add_patterns(farm_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ac1376-1b8f-4c0f-a43f-6f3290550645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an entity ruler to find named hotels used for German convalescent soldiers on the Plateau\n",
    "hotels = df_hotelsGer[\"Hotels_GermanConvalescence\"].to_list()\n",
    "hotel_patterns = []\n",
    "for hotel in hotels:\n",
    "    normalized = unidecode.unidecode(hotel)\n",
    "    hotel_patterns.append({\"pattern\": hotel, \"label\": \"HOTEL USED BY GERMANS\"})\n",
    "    hotel_patterns.append({\"pattern\": normalized, \"label\": \"HOTEL USED BY GERMANS\"})\n",
    "hotel_ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\", name=\"hotel_ruler\")\n",
    "hotel_ruler.add_patterns(hotel_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42514f50-c2ec-45ff-98e6-a73bdab90b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an entity ruler to find other named French locations mentioned often in the testimonies\n",
    "other_French_Locations = df_otherFreLoc[\"other_French_Locations\"].to_list()\n",
    "other_French_Location_patterns = []\n",
    "for other_French_Location in other_French_Locations:\n",
    "    normalized = unidecode.unidecode(other_French_Location)\n",
    "    other_French_Location_patterns.append({\"pattern\": other_French_Location, \"label\": \"OTHER FRENCH LOCATION\"})\n",
    "    other_French_Location_patterns.append({\"pattern\": normalized, \"label\": \"OTHER FRENCH LOCATION\"})\n",
    "other_French_Location_ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\", name=\"other_French_Location_ruler\")\n",
    "other_French_Location_ruler.add_patterns(other_French_Location_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1057f7-771a-4a01-b534-0e432d044eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an entity ruler to find Jewish aid organizations active in France during the Holocaust\n",
    "aid_orgs_jewish = ['Oeuvre de Secours aux Enfants', 'OSE', 'OSÉ', \"Children's Aid Society\", 'Mouvement de la Jeunesse Sioniste', \n",
    "                  'MJS', 'Eclaireuses et Eclaireurs Israélites de France', 'Eclaireurs Israélites de France','EIF',\n",
    "                  'Jewish Scouts', 'Boy Scouts', 'Scouts', 'La Sixième', 'Sixième', 'Armée Juive', 'AJ', 'WIZO',\n",
    "                  'Womens International Zionist Organization', 'Union Générale des Israélites de France', 'UGIF',\n",
    "                  'Comité de la rue Amelot', 'rue Amelot', 'Rue Amelot', 'Le Comité Amelot', 'Comité Rue Amelot', \n",
    "                  'Amelot', 'Solidarite', \"Union des Juifs pour la Resistance et l'Entraide\", 'Service André', \n",
    "                  'Groupe Maurice Cachoud', 'Scout', 'Boy Scout', 'Girl Scouts', 'Girl Scout', 'Oeuvre de Secour aux Enfants', \n",
    "                  'Jewish Joint Distribution Committee', 'JDC', 'Joint', 'Circuit Garel', 'Garel Circuit', 'Garel Network',\n",
    "                  \"Children's Aid Rescue Society\", \"Commission Central des Organizations Juives d'Assistance\", 'CCOJA', \n",
    "                  'Central Commission of Jewish Assistance Organizations', 'Camps Commission', 'Jewish Zionist Youth Movement',\n",
    "                  'General Union of French Jews', 'Organization for Rehabilitation and Training of Jews', 'ORT',\n",
    "                  'Zionist Youth Movement', \"Comité d'Assistance aux Refugiés\", 'CAR', 'Committee for Assistance to Refugees',\n",
    "                  'Sixth Division', 'HIAS', 'Hebrew Immigrant Aid Society', 'HICEM', 'Jewish Colonisation Association', \n",
    "                  'Emigdirect', 'Bass Network', 'Bass rescue network', \"Children's Aid\", 'Aid Society']\n",
    "aid_org_jewish_patterns = []\n",
    "for aid_org_jewish in aid_orgs_jewish:\n",
    "    normalized = unidecode.unidecode(aid_org_jewish)\n",
    "    aid_org_jewish_patterns.append({\"pattern\": aid_org_jewish, \"label\": \"JEWISH AID ORGANIZATION\"})\n",
    "    aid_org_jewish_patterns.append({\"pattern\": normalized, \"label\": \"JEWISH AID ORGANIZATION\"})\n",
    "aid_org_jewish_ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\", name=\"aid_org_jewish_ruler\")\n",
    "aid_org_jewish_ruler.add_patterns(aid_org_jewish_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce521b04-0e05-41ec-a802-cc9bc73ee1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an entity ruler to find other, non-Jewish aid organizations active in France during the Holocaust\n",
    "aid_orgs_other = ['Swiss Aid for Children', 'Swiss Aid', 'Cimade', 'Comité inter-mouvements auprès des évacués', \n",
    "                  'Salvation Army', 'Armée du Salut', 'Red Cross', 'Swiss Red Cross', 'Croix-Rouge', \n",
    "                  'Croix-Rouge suisse', 'Quaker', 'Quakers', 'Secours Suisse aux Enfants', 'Secours Suisse',\n",
    "                 'International Protestant Loan Association', 'European Student Relief', 'Nîmes Committee', \n",
    "                 'American Friends Service Committee', 'American Friends', 'Friends Service Committee', 'AFSC',\n",
    "                 'Unitarian Service Committee', 'USC', 'Emergency Rescue Committee', 'ERC', 'Centre Américain de Secours',\n",
    "                 'Fédération Protestante de France', 'French Protestant Federation', 'Amitié Chretienne', 'Rescue Committee',\n",
    "                 'YMCA', 'International Migration Service', \"Service Social d'Aide aux Émigrants\", 'SSAE', \n",
    "                  'Mouvement National Contre le Racisme', 'American Red Cross', 'CIMADE', 'Comité Intermouvement après des Evacuees',\n",
    "                 'Boegner Rescue Network', \"Comite d'Inter Mouvement après des Evacues\", 'American Federation of Labor',\n",
    "                 'Jousselin Rescue Network', 'Nîmes Coordinating Committee', 'Camps Committee', \n",
    "                  \"Young Men's Christian Association\", 'Young Mens Christian Association', 'Unitarians', \n",
    "                 \"American Friends' Service Committee\", 'French Red Cross', 'Croix-Rouge française', \n",
    "                 'International Rescue Committee', 'Fleury Rescue Network', 'Fleury rescue network', \n",
    "                  \"L’Aide Chrétienne aux Israélites\", 'Swiss Aid Society', 'Reconciliation Alliance', 'Swiss Friends',\n",
    "                 'CIMAD', 'Swiss Coalition for Relief to Child War Victims', 'le Comité Inter-Movements Auprès des Evacuées',\n",
    "                 'the Swiss Coalition for Relief to Child War Victims', 'American Quakers',\n",
    "                 'Secours aux Enfants', 'Comité Inter- Movements Auprès des Evacuées', 'The Swiss Coalition for Relief to Child War Victims']\n",
    "aid_org_other_patterns = []\n",
    "for aid_org_other in aid_orgs_other:\n",
    "    normalized = unidecode.unidecode(aid_org_other)\n",
    "    aid_org_other_patterns.append({\"pattern\": aid_org_other, \"label\": \"OTHER AID ORGANIZATION\"})\n",
    "    aid_org_other_patterns.append({\"pattern\": normalized, \"label\": \"OTHER AID ORGANIZATION\"})\n",
    "aid_org_other_ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\", name=\"aid_org_other_ruler\")\n",
    "aid_org_other_ruler.add_patterns(aid_org_other_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbca4db-c64c-402a-8949-8d8b759ad4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an entity ruler to find non-named features\n",
    "features = ['bay', 'bays', 'bog', 'bogs', 'canal', 'canals', 'aqueduct', 'aqueducts', 'cove', 'coves', 'dock',\n",
    "           'docks', 'ditch', 'ditches', 'estuary', 'waterfall', 'mud flat', 'tidal flat', 'glacier', 'glaciers',\n",
    "           'snowfield', 'snowfields', 'gulf', 'harbor', 'harbors', 'inlet', 'inlets', 'lake', 'lake bed', 'lakes',\n",
    "           'marsh', 'ocean', 'pond', 'ponds', 'reservoir', 'reservoirs', 'ravine', 'ravines', 'sea', 'stream',\n",
    "            'streams', 'river', 'rivers', 'swamp', 'swamps', 'tunnel', 'wetland', 'wetlands', 'clearing', \n",
    "            'clearings', 'coast', 'coastline', 'field', 'fields', 'pasture', 'pastures', 'port', 'ports', 'farm', \n",
    "            'farms', 'causeway', 'causeways', 'portage', 'road', 'roads', 'railroad', 'railroads', 'street'\n",
    "            'railroad station', 'railroad stations', 'streets', 'tunnel', 'tunnels', 'trail', 'trails', 'airfield', \n",
    "            'airfields', 'airport', 'airports', 'bridge', 'bridges', 'apartment', 'apartments', \n",
    "            'office', 'offices', 'bank', 'banks', 'barrack', 'barracks', 'boatyard', 'boatyards', 'bus stop', \n",
    "            'bus stops', 'bus station', 'bus stations', 'bus', 'buses', 'train', 'trains', 'cave', 'caves', 'camp',\n",
    "            'camps', 'church', 'churches', 'school', 'schools', 'castle', 'castles', 'synogogue', 'synogogues', \n",
    "            'cemetary', 'cemetaries', 'corral', 'corrals', 'courthouse', 'courthouses', 'town square', 'town squares', \n",
    "            'convent', 'convents', 'monastery', 'monasteries', 'dock', 'docks', 'customs house', 'guard house',\n",
    "            'guard houses', 'guard tower', 'guard towers', 'estate', 'estates', 'factory', 'factories', 'facility', \n",
    "            'facilities', 'farmstead', 'farmsteads', 'fort', 'forts', 'ferry', 'ferries', 'gate', 'gates', 'garden', \n",
    "            'gardens', 'grave', 'graves', 'homestead', 'homesteads', 'home', 'homes', 'house', 'houses', 'hospital', \n",
    "            'hospitals', 'clinic', 'clinics', 'hotel', 'hotels', 'city hall', 'jetty', 'library', 'libraries', \n",
    "            'landfill', 'lighthouse', 'lighthouses', 'marina', 'marinas', 'cannery', 'canneries', 'munitions plant', \n",
    "            'market', 'markets', 'mill', 'mills', 'sawmill', 'sawmills', 'windmill', 'windmills', 'water mill', \n",
    "            'quarry', 'quarries', 'metro station', 'museum', 'museums', 'novitiate', 'palace', 'palaces', 'chateau',\n",
    "            'chateaus', 'pier', 'piers', 'post office', 'police station', 'police post', 'park', 'parks', 'prison',\n",
    "            'prisons', 'reformatory', 'reformatories', 'concentration camp', 'concentration camps', 'customs post',\n",
    "            'border post', 'border house', 'border', 'borders', 'patrol house', 'patrol post', 'restaurant', \n",
    "            'restaurants', 'grocery store', 'store', 'college', 'university', 'universities', 'sheepfold', 'barn', \n",
    "            'barns', 'haystack', 'haystacks', 'sheath', 'sheathes', 'shrine', 'shrines', 'storehouse', 'storehouses',\n",
    "            'sanatorium', 'sanatoriums', 'village square', 'stable', 'stables', 'stadium', 'stadiums', \n",
    "            'military base', 'outpost', 'station', 'stations', 'theater', 'theatre', 'theaters', 'theatres', 'tomb',\n",
    "            'tombs', 'temple', 'temples', 'toll gate', 'barrier', 'barriers', 'fence', 'fences', 'tower', 'towers', \n",
    "            'tram', 'trams', 'trolley', 'trolleys', 'wall', 'walls', 'zoo', 'zoos', 'beach', 'beaaches', 'cliff', \n",
    "            'cliffs', 'canyon', 'canyons', 'corridor', 'corridors', 'cirque', 'crater', 'craters', 'delta', 'dune', \n",
    "            'desert', 'gorge', 'gorges', 'fissure', 'headland', 'headlands', 'hill', 'hills', 'island', 'islands', \n",
    "            'levee', 'levees', 'mound', 'mounds', 'mountain', 'mountains', 'pass', 'peninsula', 'peak', 'peaks',\n",
    "            'plateau', 'ridge', 'ridges', 'escarpment', 'shore', 'shoreline', 'coastline', 'slope', 'terrace', \n",
    "            'upland', 'uplands', 'valley', 'valleys', 'furrow', 'furrows', 'knoll', 'knolls', 'ledge', 'ledges', \n",
    "            'moat', 'shoal', 'shoals', 'bush', 'bushes', 'forest', 'forests', 'woods', 'glade', 'glades', 'tree',\n",
    "            'trees', 'grove', 'groves', 'grass', 'grasses', 'grassland', 'grasslands', 'meadow', 'meadows', \n",
    "            'orchard', 'orchards', 'scrubland', 'scrublands', 'scrub', 'tundra', 'vineyard', 'vineyards', 'village', \n",
    "            'barnyard', 'courtyard', 'crop land', 'crop lands', 'cropland', 'croplands', 'brushland', 'brush land', \n",
    "            'silo', 'railway', 'railway station', 'railway stations', 'railways', 'wagon', 'wagons', 'car', 'cars',\n",
    "            'truck', 'trucks']\n",
    "feature_patterns = []\n",
    "for feature in features:\n",
    "    normalized = unidecode.unidecode(feature)\n",
    "    feature_patterns.append({\"pattern\": feature, \"label\": \"FEATURE\"})\n",
    "    feature_patterns.append({\"pattern\": normalized, \"label\": \"FEATURE\"})\n",
    "feature_ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\", name=\"feature_ruler\")\n",
    "feature_ruler.add_patterns(feature_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197501b4-eebe-4ccd-a857-987500ec7cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load your data...\n",
    "This create a combined string from all .txt files in a directory, which \n",
    "represents your data.\n",
    "You have to use \"f.read\" not \"f.readlines\".\n",
    "\"\"\"\n",
    "\n",
    "# Establish the path to your data\n",
    "import os\n",
    "path = \"C:/Users/....../Data/\"\n",
    "\n",
    "def read_txt_files(directory):\n",
    "    # Reads all .txt files in a directory and returns a combined string of their contents.\n",
    "\n",
    "    file_contents = ''\n",
    "    \n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            with open(filepath, \"r\", encoding=\"utf8\") as f:\n",
    "                file_contents = file_contents + (f.read())\n",
    "    return file_contents\n",
    "\n",
    "texts = read_txt_files(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c92fe8-4020-4d0a-b310-bdd94b40f3fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Runs the combined txt files through the NLP pipeline, looking for specific entities that you set up\n",
    "via the Entity Rulers, and then afterwards the normal entities extracted by the spaCy NER as well,\n",
    "and then visualizing them.\n",
    "\"\"\"\n",
    "\n",
    "doc = nlp(texts)\n",
    "options = {\"ents\": [\"PLATEAU LOCATION\", \"CHILDREN'S HOME\", \"SCHOOL\", \"FARM\", \"HOTEL USED BY GERMANS\",\n",
    "                    \"OTHER FRENCH LOCATION\", \"JEWISH AID ORGANIZATION\", \"OTHER AID ORGANIZATION\", \"PERSON\", \"GPE\", \n",
    "                    \"LOC\", \"ORG\", \"LANGUAGE\", \"NORP\", \"TIME\", \"DATE\", \"QUANTITY\", \"PERCENT\", \"FAC\", \"FEATURE\"],\n",
    "           \"colors\": {\"PLATEAU LOCATION\": \"pink\", \"CHILDREN'S HOME\": \"#bf94e4\", \"SCHOOL\": \"yellow\", \n",
    "                    \"FARM\": \"green\", \"HOTEL USED BY GERMANS\": \"#ff033e\", \"OTHER FRENCH LOCATION\": \"#bfff00\",\n",
    "                    \"JEWISH AID ORGANIZATION\": \"lightgreen\", \"OTHER AID ORGANIZATION\": \"#9966cc\", \"PERSON\": \"coral\", \n",
    "                    \"GPE\": \"#b5651d\", \"LOC\": \"orange\", \"ORG\": \"teal\", \"LANGUAGE\": \"#f56991\", \"NORP\": \"#ace5ee\", \n",
    "                    \"TIME\": \"white\", \"DATE\": \"gray\", \"QUANTITY\": \"tan\", \"PERCENT\": \"gold\", \"FAC\": \"lavender\", \n",
    "                    \"FEATURE\": \"fuchsia\"}}\n",
    "\n",
    "displacy.render(doc, style=\"ent\", jupyter=True, options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115bd441-dc75-46b4-aeb2-44c412641f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tell me now many tokens did spaCy process...\n",
    "\n",
    "len(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c964fcf-b74b-4f23-9a1f-fc5e288fe9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creates dataframe of all named Plateau towns, villages, and features extracted from the testimonies, \n",
    "with datatype as string to avoid commas between words, which is what it does if you use default \n",
    "object datatype..\n",
    "\"\"\"\n",
    "\n",
    "Plateau_Locations = []\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"PLATEAU LOCATION\":\n",
    "        Plateau_Locations.append(ent)\n",
    "\n",
    "df_PlatLocExtract = pd.DataFrame({\"Plateau_Locations\": Plateau_Locations}, dtype=\"string\")\n",
    "\n",
    "df_PlatLocExtract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d230fa26-4444-4ea3-a3b4-ad4491ef0141",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creates dataframe of all named Plateau childrens homes extracted from the testimonies, \n",
    "with datatype as string to avoid commas between words, which is what it does if you use \n",
    "default object datatype..\n",
    "\"\"\"\n",
    "\n",
    "homes = []\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"CHILDREN'S HOME\":\n",
    "        homes.append(ent)\n",
    "\n",
    "df_homesExtract = pd.DataFrame({\"Childrens_Homes\": homes}, dtype=\"string\")\n",
    "\n",
    "df_homesExtract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49e99b5-67d2-4ff6-8241-44f94e244dc5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creates dataframe of all named Plateau schools extracted from the testimonies, \n",
    "with datatype as string to avoid commas between words, which is what it does if you use \n",
    "default object datatype..\n",
    "\"\"\"\n",
    "\n",
    "schools = []\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"SCHOOL\":\n",
    "        schools.append(ent)\n",
    "\n",
    "df_schoolsExtract = pd.DataFrame({\"Schools\": schools}, dtype=\"string\")\n",
    "\n",
    "df_schoolsExtract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726bc79a-1764-4f56-a48a-3b5ed182dab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creates dataframe of all named Plateau farms extracted from the testimonies, \n",
    "with datatype as string to avoid commas between words, which is what it does if you use \n",
    "default object datatype..\n",
    "\n",
    "##  However, different from the others, there was an error that was occuring with this data.  For an unknown\n",
    "## reason, the list from the doc entities was trying to create a dataframe, using two different columns \n",
    "## for \"La\" and \"Bergerie\". When I tried using the method I did above with a dictionary for {\"Farms\": farms},\n",
    "## it was then throwing a \"ValueError: Buffer has wrong number of dimensions (expected 1, got 2)\"\n",
    "## To fix, I create a simple dataframe from the list, then aggregate/join the two columns to combine \"La\" and \"Bergerie\".\n",
    "## Then I dropped the original two columns, leaving just the aggregated column that I want.\n",
    "\"\"\"\n",
    "\n",
    "farms = []\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"FARM\":\n",
    "        farms.append(ent)\n",
    "        \n",
    "df_farmsExtract = pd.DataFrame(farms, dtype=\"string\")\n",
    "df_farmsExtract[\"Farms\"] = df_farmsExtract[[0, 1]].agg(\" \".join, axis=1)\n",
    "df_farmsExtract = df_farmsExtract.drop(columns=[0, 1]) \n",
    "df_farmsExtract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc29cf9-cc16-48cd-9d12-750f1e1c5a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creates dataframe of all named Plateau hotels used by German convalescent soldiers extracted \n",
    "from the testimonies, with datatype as string to avoid commas between words, which is what it \n",
    "does if you use default object datatype..\n",
    "\"\"\"\n",
    "\n",
    "hotels = []\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"HOTEL USED BY GERMANS\":\n",
    "        hotels.append(ent)\n",
    "\n",
    "df_hotelsGerExtract = pd.DataFrame({\"Hotels_GermanConvalescence\": hotels}, dtype=\"string\")\n",
    "\n",
    "df_hotelsGerExtract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae034b2-2e73-453c-b997-251b07f653c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creates dataframe of all other named French locations extracted from the testimonies, \n",
    "with datatype as string to avoid commas between words, which is what it does if you use \n",
    "default object datatype..\n",
    "\"\"\"\n",
    "\n",
    "other_French_Locations = []\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"OTHER FRENCH LOCATION\":\n",
    "        other_French_Locations.append(ent)\n",
    "\n",
    "df_othFreLocExtract = pd.DataFrame({\"other_French_Locations\": other_French_Locations}, dtype=\"string\")\n",
    "\n",
    "df_othFreLocExtract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f3616f-5da0-42e7-b6c4-7dbb02c758b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This merges two dataframes: the first is the dataframe with the named Plateau locations and their coordinates \n",
    "and the second is the dataframe which contains the extracted Plateau locations from the spaCy entity recognition.\n",
    "Then it exports the results to a csv file.\n",
    "\"\"\"\n",
    "\n",
    "df_merged = pd.merge(df, df_PlatLocExtract, on=\"Plateau_Locations\", how=\"right\")\n",
    "\n",
    "df_merged.to_csv(\"C:/Users/....../Results_ExtractedPlateauLocations.csv\", encoding=\"utf-8-sig\", index=False, header=True, mode=\"w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b850b2a9-bac9-4376-ad98-c5a5a7e5be6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This merges two dataframes: the first is the dataframe with the named children's homes and their coordinates \n",
    "and the second is the dataframe which contains the extracted children's homes from the spaCy entity recognition.\n",
    "Then it exports the results to a csv file.\n",
    "\"\"\"\n",
    "\n",
    "df_mergedHomes = pd.merge(df_homes, df_homesExtract, on=\"Childrens_Homes\", how=\"right\")\n",
    "\n",
    "df_mergedHomes.to_csv(\"C:/Users/....../Results_ExtractedChildrensHomes.csv\", encoding=\"utf-8-sig\", index=False, header=True, mode=\"w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be00799-25c1-4b2b-9107-8241e3a0b294",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This merges two dataframes: the first is the dataframe with the named schools and their coordinates \n",
    "and the second is the dataframe which contains the extracted schools from the spaCy entity recognition.\n",
    "Then it exports the results to a csv file.\n",
    "\"\"\"\n",
    "\n",
    "df_mergedSchools = pd.merge(df_schools, df_schoolsExtract, on=\"Schools\", how=\"right\")\n",
    "\n",
    "df_mergedSchools.to_csv(\"C:/Users/....../Results_ExtractedSchools.csv\", encoding=\"utf-8-sig\", index=False, header=True, mode=\"w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfd0c4d-f39d-41a8-82a5-1a8fc21578c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This merges two dataframes: the first is the dataframe with the named farms and their coordinates \n",
    "and the second is the dataframe which contains the extracted farms from the spaCy entity recognition.\n",
    "Then it exports the results to a csv file.\n",
    "\"\"\"\n",
    "\n",
    "df_mergedFarms = pd.merge(df_farms, df_farmsExtract, on=\"Farms\", how=\"right\")\n",
    "\n",
    "df_mergedFarms.to_csv(\"C:/Users/....../Results_ExtractedFarms.csv\", encoding=\"utf-8-sig\", index=False, header=True, mode=\"w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21db32be-7b73-4fd9-a868-6fef08e7717c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This merges two dataframes: the first is the dataframe with the named hotels used by the Germans and their coordinates \n",
    "and the second is the dataframe which contains the extracted hotels from the spaCy entity recognition.\n",
    "Then it exports the results to a csv file.\n",
    "\"\"\"\n",
    "\n",
    "df_mergedHotelsGer = pd.merge(df_hotelsGer, df_hotelsGerExtract, on=\"Hotels_GermanConvalescence\", how=\"right\")\n",
    "\n",
    "df_mergedHotelsGer.to_csv(\"C:/Users/....../Results_ExtractedHotelsGermans.csv\", encoding=\"utf-8-sig\", index=False, header=True, mode=\"w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2501d5c5-7690-47ce-b1fb-32213979f110",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This merges two dataframes: the first is the dataframe with the named Other French locations and their coordinates \n",
    "and the second is the dataframe which contains the extracted Other French locations from the spaCy entity recognition.\n",
    "Then it exports the results to a csv file.\n",
    "\"\"\"\n",
    "\n",
    "df_mergedOthFreLoc = pd.merge(df_otherFreLoc, df_othFreLocExtract, on=\"other_French_Locations\", how=\"right\")\n",
    "\n",
    "df_mergedOthFreLoc.to_csv(\"C:/Users/....../Results_ExtractedOtherFrenchLocations.csv\", encoding=\"utf-8-sig\", index=False, header=True, mode=\"w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700e57c1-f2f6-4d58-93ec-faae5f82b644",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creates dataframe of all non-named features found on the Plateau, with datatype as string to avoid commas between\n",
    "words, which is what it does if you use default \"Object\" datatype..\n",
    "\"\"\"\n",
    "\n",
    "features = []\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"FEATURE\":\n",
    "        features.append(ent)\n",
    "\n",
    "df_feat = pd.DataFrame({\"Features\": features}, dtype=\"string\")\n",
    "\n",
    "df_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c445d4f5-7bf8-456d-aa57-e03aaab7cbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Take output from \"Feature\" dataframe, give count of each feature name, then export to a csv.\n",
    "Note: you need the \"Index = true\" to get the feature name to go to the csv file.  \n",
    "Also note, you need the \"encoding = utf-8-sig\" to get the French words correct.\n",
    "\"\"\"\n",
    "\n",
    "df_feat_summary = df_feat[\"Features\"].value_counts()\n",
    "\n",
    "df_feat_summary.to_csv(\"C:/Users/....../Results_ExtractedFeatures.csv\", encoding=\"utf-8-sig\", index=True, header=True, mode=\"w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7181827-3874-4aad-936f-d454883cc359",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creates dataframe of all GPEs extracted from testimonies, with datatype as string to avoid commas between\n",
    "words, which is what it does if you use default \"Object\" datatype..\n",
    "\"\"\"\n",
    "\n",
    "gpes = []\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"GPE\":\n",
    "        gpes.append(ent)\n",
    "\n",
    "df_gpe = pd.DataFrame({\"GPEs\": gpes}, dtype=\"string\")\n",
    "df_gpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359630fd-5cb3-4c13-b9a9-53fbcd75cb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Take output from \"GPE\" dataframe, give count of each GPE name, then export to a csv.\n",
    "Note: you need the \"Index = true\" to get the GPE name to go to the csv file.  \n",
    "Also note, you need the \"encoding = utf-8-sig\" to get the French words correct.\n",
    "\"\"\"\n",
    "\n",
    "df_gpe_summary = df_gpe[\"GPEs\"].value_counts()\n",
    "\n",
    "df_gpe_summary.to_csv(\"C:/Users/....../Results_ExtractedGPEs.csv\", encoding=\"utf-8-sig\", index=True, header=True, mode=\"w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275693d3-aaf5-4108-a2cd-cf9350b860c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creates dataframe of all locations extracted from testimonies, with datatype as string to avoid commas between\n",
    "words, which is what it does if you use default \"Object\" datatype..\n",
    "\"\"\"\n",
    "\n",
    "locs = []\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"LOC\":\n",
    "        locs.append(ent)\n",
    "\n",
    "df_loc = pd.DataFrame({\"LOCs\": locs}, dtype=\"string\")\n",
    "df_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fabbc40-c6c5-494c-a82d-7d3a4639549b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Take output from \"Location\" dataframe, give count of each location name, then export to a csv.\n",
    "Note: you need the \"Index = true\" to get the location name to go to the csv file.  \n",
    "Also note, you need the \"encoding = utf-8-sig\" to get the French words correct.\n",
    "\"\"\"\n",
    "\n",
    "df_loc_summary = df_loc[\"LOCs\"].value_counts()\n",
    "\n",
    "df_loc_summary.to_csv(\"C:/Users/....../Results_ExtractedLOCs.csv\", encoding=\"utf-8-sig\", index=True, header=True, mode=\"w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7af8104-c926-412f-aefe-1609ed0ccd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creates dataframe of all facilities extracted from testimonies, with datatype as string to avoid commas between\n",
    "words, which is what it does if you use default \"Object\" datatype..\n",
    "\"\"\"\n",
    "\n",
    "facs = []\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"FAC\":\n",
    "        facs.append(ent)\n",
    "\n",
    "df_fac = pd.DataFrame({\"FACs\": facs}, dtype=\"string\")\n",
    "df_fac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46818a9d-4ff8-4823-b432-0fd0386ecbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Take output from \"Facilities\" dataframe, give count of each facilities name, then export to a csv.\n",
    "Note: you need the \"Index = true\" to get the facilities name to go to the csv file.  \n",
    "Also note, you need the \"encoding = utf-8-sig\" to get the French words correct.\n",
    "\"\"\"\n",
    "\n",
    "df_fac_summary = df_fac[\"FACs\"].value_counts()\n",
    "\n",
    "df_fac_summary.to_csv(\"C:/Users/....../Results_ExtractedFACs.csv\", encoding=\"utf-8-sig\", index=True, header=True, mode=\"w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9756e4d-b224-4a38-9097-788abc2a02d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creates dataframe of all Jewish Aid Organizations extracted from testimonies, with datatype as string to avoid commas between\n",
    "words, which is what it does if you use default \"Object\" datatype..\n",
    "\"\"\"\n",
    "\n",
    "jewishAidOrgans = []\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"JEWISH AID ORGANIZATION\":\n",
    "        jewishAidOrgans.append(ent)\n",
    "\n",
    "df_jewishAidOrgan = pd.DataFrame({\"Jewish Aid Organizations\": jewishAidOrgans}, dtype=\"string\")\n",
    "df_jewishAidOrgan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bdd07a-f042-4673-b378-62e36b63b49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Take output from \"Jewish Aid Organizations\" dataframe, give count of each organizations name, then export to a csv.\n",
    "Note: you need the \"Index = true\" to get the organizations name to go to the csv file.  \n",
    "Also note, you need the \"encoding = utf-8-sig\" to get the French words correct.\n",
    "\"\"\"\n",
    "\n",
    "df_jewishAidOrgan_summary = df_jewishAidOrgan[\"Jewish Aid Organizations\"].value_counts()\n",
    "\n",
    "df_jewishAidOrgan_summary.to_csv(\"C:/Users/....../Results_ExtractedJewishAidOrgans.csv\", encoding=\"utf-8-sig\", index=True, header=True, mode=\"w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f75efa1-3600-4ad6-b9d8-bbf48133fc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creates dataframe of all Other Aid Organizations extracted from testimonies, with datatype as string to avoid commas between\n",
    "words, which is what it does if you use default \"Object\" datatype..\n",
    "\"\"\"\n",
    "\n",
    "otherAidOrgans = []\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"OTHER AID ORGANIZATION\":\n",
    "        otherAidOrgans.append(ent)\n",
    "\n",
    "df_otherAidOrgan = pd.DataFrame({\"Other Aid Organizations\": otherAidOrgans}, dtype=\"string\")\n",
    "df_otherAidOrgan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d65f63-82e0-4dd5-ac8b-4610e2ce6515",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Take output from \"Other Aid Organizations\" dataframe, give count of each organizations name, then export to a csv.\n",
    "Note: you need the \"Index = true\" to get the organizations name to go to the csv file.  \n",
    "Also note, you need the \"encoding = utf-8-sig\" to get the French words correct.\n",
    "\"\"\"\n",
    "\n",
    "df_otherAidOrgan_summary = df_otherAidOrgan[\"Other Aid Organizations\"].value_counts()\n",
    "\n",
    "df_otherAidOrgan_summary.to_csv(\"C:/Users/....../Results_ExtractedOtherAidOrgans.csv\", encoding=\"utf-8-sig\", index=True, header=True, mode=\"w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8308ea29-90b2-4bfc-b47b-f3b082020451",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creates dataframe of all Persons extracted from testimonies, with datatype as string to avoid commas between\n",
    "words, which is what it does if you use default \"Object\" datatype..\n",
    "\"\"\"\n",
    "\n",
    "persons = []\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"PERSON\":\n",
    "        persons.append(ent)\n",
    "\n",
    "df_person = pd.DataFrame({\"Persons\": persons}, dtype=\"string\")\n",
    "df_person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f3a036-499c-42e4-b136-1b28239969cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Take output from \"Persons\" dataframe, give count of each Persons name, then export to a csv.\n",
    "Note: you need the \"Index = true\" to get the Persons name to go to the csv file.  \n",
    "Also note, you need the \"encoding = utf-8-sig\" to get the French words correct.\n",
    "\"\"\"\n",
    "\n",
    "df_person_summary = df_person[\"Persons\"].value_counts()\n",
    "\n",
    "df_person_summary.to_csv(\"C:/Users/....../Results_ExtractedPersons.csv\", encoding=\"utf-8-sig\", index=True, header=True, mode=\"w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ded92bf-bc0a-4236-af3f-72b5fdb938d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
